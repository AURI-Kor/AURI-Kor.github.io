[
  
    {
      "title"       : "Machine Learning - Simple Regression",
      "category"    : "",
      "tags"        : "machine learning, study_model, linear regression",
      "url"         : "./MachineLearning_Simple.html",
      "date"        : "2021-03-24 00:00:00 +0900",
      "description" : "ML Basic",
      "content"     : "Simple Regression &#49892;&#49845; 1&#52264;&#49884; - ZeroR, OneR, Naive Bayes Classifier&#182;&#49892;&#49845; &#45236;&#50857;:&#182;ZeroROneRNaive Bayes classifierIn&nbsp;[&nbsp;]: import numpy as npimport pandas as pd In&nbsp;[&nbsp;]: import sklearnprint(sklearn.__version__) 1.0.2&#49892;&#49845; &#45936;&#51060;&#53552;&#182;In&nbsp;[&nbsp;]: # 데이터 받기url = &quot;https://raw.githubusercontent.com/inikoreaackr/ml_datasets/main/playgolf.csv&quot;df = pd.read_csv(url) In&nbsp;[&nbsp;]: # 데이터 첫 다섯 instance 확인df.head() Out[&nbsp;]: OUTLOOK TEMPERATURE HUMIDITY WINDY PLAY GOLF 0 Rainy Hot High False No 1 Rainy Hot High True No 2 Overcast Hot High False Yes 3 Sunny Mild High False Yes 4 Sunny Cool Normal False Yes &lt;svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\" width=\"24px\"&gt; &lt;/svg&gt; In&nbsp;[&nbsp;]: # 데이터 타입 확인df.dtypes Out[&nbsp;]:OUTLOOK objectTEMPERATURE objectHUMIDITY objectWINDY boolPLAY GOLF objectdtype: objectIn&nbsp;[&nbsp;]: # object 타입을 category로 변경for col in df.columns: df[col] = df[col].astype(&#39;category&#39;) In&nbsp;[&nbsp;]: # 변경이 되었는지 확인df.dtypes Out[&nbsp;]:OUTLOOK categoryTEMPERATURE categoryHUMIDITY categoryWINDY categoryPLAY GOLF categorydtype: object1. ZeroR&#182;ZeroR은 가장 간단한 분류 방법이며, 다른 모든 feature들을 무시하고 label에만 의존합니다.ZeroR 분류기는 단순히 데이터의 class를 다수 카테고리로 예측합니다.ZeroR에는 예측 능력이 없지만, 이것은 표준 성능을 가늠하여 다른 분류 방법 성능의 기준점이 됩니다.In&nbsp;[&nbsp;]: # PLAY GOLF feature 출력df[&#39;PLAY GOLF&#39;] Out[&nbsp;]:0 No1 No2 Yes3 Yes4 Yes5 No6 Yes7 No8 Yes9 Yes10 Yes11 Yes12 Yes13 NoName: PLAY GOLF, dtype: categoryCategories (2, object): [&#39;No&#39;, &#39;Yes&#39;]In&nbsp;[&nbsp;]: # PLAY GOLF는 binary 변수입니다. 각 카테고리의 갯수를 세어봅니다.df[&#39;PLAY GOLF&#39;].value_counts(sort = True) Out[&nbsp;]:Yes 9No 5Name: PLAY GOLF, dtype: int64In&nbsp;[&nbsp;]: # 이 데이터셋에서 &quot;Play Golf = Yes&quot;로 예측하는 ZeroR 모델의 정확도를 계산해봅니다.9 / (9 + 5) Out[&nbsp;]:0.6428571428571429 &lt;img src=https://www.saedsayad.com/images/ZeroR_3.png&gt;위의 데이터셋에서 \"Play Golf = Yes\"로 예측하는 ZeroR모델의 정확도는 0.64가 됩니다.OneR&#182;OneR은 One Rule의 약자이며, 간단하고 정확한 분류 알고리즘입니다.OneR은 데이터의 각 feature 마다 하나의 룰 셋(Rule Set)을 생성합니다. 그리고 생성된 룰 셋 중에서, 전체데이터에 대해 오차가 가장 작은 룰 셋을 One Rule로 결정합니다.각 feature당 룰 셋은 frequency table을 이용하여 만들 수 있습니다.OneR Algorithm각 feature 마다, 각 feature의 value 마다, 룰을 아래와 같이 만듭니다. 그 feature의 value에 해당되는 instance중에 target class가 몇개인지 셉니다. 가장 갯수가 많은 class를 찾습니다. 그 feature의 value가 해당되면 그 갯수가 많은 class로 예측되도록 룰을 하나 만듭니다. 각 feature의 룰들의 전체 에러를 계산합니다. (반대로 정확도를 계산할 수도 있습니다.)가장 작은 에러를 보이는 feature을 선택합니다.아래 그림에서는 outlook과 humidity feature 모두 에러의 갯수가 4이므로 제일 작습니다. 하지만 활동에서는 첫번째 feature인 outlook만 고려할 것입니다.For example:&lt;img src=https://www.saedsayad.com/images/OneR_1.png&gt; In&nbsp;[&nbsp;]: # 수도코드 구현from collections import Countertotal_errors = []for col in df.columns[:-1]: error = 0 for val in df[col].unique(): length = len(df[df[col] == val]) print(f&quot;{col} : {val}, length : {length}&quot;) print(Counter(df[df[col] == val][&#39;PLAY GOLF&#39;]).most_common()) error += (length - Counter(df[df[col] == val][&#39;PLAY GOLF&#39;]).most_common()[0][1]) print(f&quot;\\nerror of {col}: [{error}] \\n&quot;) total_errors.append(error) OUTLOOK : Rainy, length : 5[(&#39;No&#39;, 3), (&#39;Yes&#39;, 2)]OUTLOOK : Overcast, length : 4[(&#39;Yes&#39;, 4)]OUTLOOK : Sunny, length : 5[(&#39;Yes&#39;, 3), (&#39;No&#39;, 2)]error of OUTLOOK: [4] TEMPERATURE : Hot, length : 4[(&#39;No&#39;, 2), (&#39;Yes&#39;, 2)]TEMPERATURE : Mild, length : 6[(&#39;Yes&#39;, 4), (&#39;No&#39;, 2)]TEMPERATURE : Cool, length : 4[(&#39;Yes&#39;, 3), (&#39;No&#39;, 1)]error of TEMPERATURE: [5] HUMIDITY : High, length : 7[(&#39;No&#39;, 4), (&#39;Yes&#39;, 3)]HUMIDITY : Normal, length : 7[(&#39;Yes&#39;, 6), (&#39;No&#39;, 1)]error of HUMIDITY: [4] WINDY : False, length : 8[(&#39;Yes&#39;, 6), (&#39;No&#39;, 2)]WINDY : True, length : 6[(&#39;No&#39;, 3), (&#39;Yes&#39;, 3)]error of WINDY: [5] In&nbsp;[&nbsp;]: # 오류가 가장 작은 feature를 고릅니다.best_feature = df.columns[np.argmin(total_errors)]print(best_feature) OUTLOOKIn&nbsp;[&nbsp;]: # best feature에 대해 룰셋을 생성합니다.oneRules = []for val in df[best_feature].unique(): print(f&quot;{best_feature} : {val}&quot;, &quot;-&gt; &quot;, end = &#39; &#39;) print(Counter(df[df[best_feature] == val][&#39;PLAY GOLF&#39;]).most_common()[0][0]) oneRules.append((best_feature, val, Counter(df[df[best_feature] == val][&#39;PLAY GOLF&#39;]).most_common()[0][0])) OUTLOOK : Rainy -&gt; NoOUTLOOK : Overcast -&gt; YesOUTLOOK : Sunny -&gt; YesThe best feature is:&lt;img src=https://www.saedsayad.com/images/OneR_3.png&gt;Naive Bayes Classifier with scikit-learn&#182;scikit-learn의 Naive Bayes classifier 다큐멘테이션: https://scikit-learn.org/stable/modules/naive_bayes.htmlIn&nbsp;[&nbsp;]: df.head() Out[&nbsp;]: OUTLOOK TEMPERATURE HUMIDITY WINDY PLAY GOLF 0 Rainy Hot High False No 1 Rainy Hot High True No 2 Overcast Hot High False Yes 3 Sunny Mild High False Yes 4 Sunny Cool Normal False Yes &lt;svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\" width=\"24px\"&gt; &lt;/svg&gt; In&nbsp;[&nbsp;]: df.describe() Out[&nbsp;]: OUTLOOK TEMPERATURE HUMIDITY WINDY PLAY GOLF count 14 14 14 14 14 unique 3 3 2 2 2 top Rainy Mild High False Yes freq 5 6 7 8 9 &lt;svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\" width=\"24px\"&gt; &lt;/svg&gt; In&nbsp;[&nbsp;]: # 카테고리 데이터를 정수로 인코딩df_enc = pd.DataFrame()df_enc[&#39;OUTLOOK&#39;] = df[&#39;OUTLOOK&#39;].cat.codesdf_enc[&#39;TEMPERATURE&#39;] = df[&#39;TEMPERATURE&#39;].cat.codesdf_enc[&#39;HUMIDITY&#39;] = df[&#39;HUMIDITY&#39;].cat.codesdf_enc[&#39;WINDY&#39;] = df[&#39;WINDY&#39;].cat.codesdf_enc[&#39;PLAY GOLF&#39;] = df[&#39;PLAY GOLF&#39;].cat.codes In&nbsp;[&nbsp;]: df_enc.head() Out[&nbsp;]: OUTLOOK TEMPERATURE HUMIDITY WINDY PLAY GOLF 0 1 1 0 0 0 1 1 1 0 1 0 2 0 1 0 0 1 3 2 2 0 0 1 4 2 0 1 0 1 &lt;svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\" width=\"24px\"&gt; &lt;/svg&gt; In&nbsp;[&nbsp;]: # 인코딩된 데이터의 타입을 프린트해봅니다.df_enc.dtypes Out[&nbsp;]:OUTLOOK int8TEMPERATURE int8HUMIDITY int8WINDY int8PLAY GOLF int8dtype: objectIn&nbsp;[&nbsp;]: # 분류기에 넣을 feature과 해당 label을 구분합니다.features = df_enc.drop(columns=[&#39;PLAY GOLF&#39;])label = df_enc[&#39;PLAY GOLF&#39;] In&nbsp;[&nbsp;]: from sklearn.naive_bayes import CategoricalNBmodel = CategoricalNB() In&nbsp;[&nbsp;]: model.fit(features.values, label) Out[&nbsp;]:CategoricalNB()In&nbsp;[&nbsp;]: score = model.score(features.values, label)score Out[&nbsp;]:0.9285714285714286In&nbsp;[&nbsp;]: # p(x_i|y_i) 출력from pprint import pprintfeature_log_prior = model.feature_log_prob_for feature_prior in feature_log_prior: pprint(np.exp(feature_prior)) array([[0.125 , 0.5 , 0.375 ], [0.41666667, 0.25 , 0.33333333]])array([[0.25 , 0.375 , 0.375 ], [0.33333333, 0.25 , 0.41666667]])array([[0.71428571, 0.28571429], [0.36363636, 0.63636364]])array([[0.42857143, 0.57142857], [0.63636364, 0.36363636]])In&nbsp;[&nbsp;]: # p(y_j) 출력np.exp(model.class_log_prior_) Out[&nbsp;]:array([0.35714286, 0.64285714])In&nbsp;[&nbsp;]: # instances에 대해서 예측을 해봅니다. # (&quot;Sunny&quot;, &quot;Hot&quot;, &quot;Normal&quot;, False) [2, 1, 1, 0]# (&quot;Rainy&quot;, &quot;Mild&quot;, &quot;High&quot;, False) [1, 2, 0, 0]print(model.predict_proba([[2, 1, 1, 0]]), model.predict([[2, 1, 1, 0]]))print(model.predict_proba([[1, 2, 0, 0]]), model.predict([[1, 2, 0, 0]])) [[0.22086561 0.77913439]] [1][[0.5695011 0.4304989]] [0]1. &#44592;&#44228;&#54617;&#49845; &#44592;&#48376; &#50857;&#50612; &#51221;&#51032;/&#51032;&#48120;&#182;기계학습: 인공지능의 한 분야로 사람의 학습과 같은 능력을 컴퓨터를 통해 실현하고자 하는 기술로, 데이터로부터 모델을 만들어내는 과정을 의미한다.Label (=target): label은 model이 예측하려는 값으로 training을 한 후의 output이다. 데이터를 차별화 할 수 있는 범주이다.class: 데이터를 분류하는 범주Features: feature은 training data의 분석 대상이 되는 속성들로 input set에 있는 column으로 표현된다.Input: input은 일반적으로 model에 전당되는 데이터 집합(X)를 나타낸다. 예를 들어 (X,Y(label)) 형태의 데이터 세트에서 X는 입력이며 레이블인 Y는 대상 또는 출력이 된다.Numerical data (=Quantitative data): Numerical data는 숫자로 표현되는 것을 의미한다.Categorical data (=Qualitative data): Categorical data는 일반적으로 숫자로 표현되지 않는 것을 의미하며 이산적인 형태의 데이터를 표현하기 위해 사용된다.Unlabeled example (=instance): Unlabeled example은 주로 unsupervised learning에 사용되는 데이터로 의미 있는 label이 존재하지 않는다. training data의 한 예시로 예상되는 결과에 대해 정보를 내포하지 않은 데이터이다.Labeled example:labeled example은 주로 supervised learning에 사용되는 데이터로 의미 있는 label이나 class를 가지고 있다.Training (=learning): 주로 성능의 향상을 의미한다. 경험(data)에 따라 예측하려는 값에 대해 배우는 것으로 명시적인 지시가 아닌 경험에 의해 작업의 성능이 향상되는 것을 의미한다.Predict (=inference): prediction이란 과거의 data set에 대한 training을 거쳐 특정 결과의 가능성을 예측하는 model의 출력을 의미한다.Train Set: Train set은 기계 학습 model을 훈련하는데 사용되는 데이터이다.Test Set: Test set은 학습된 모델을 테스트하기 위한 data의 하위 set이다.Regression: feature와 outcome 간의 관계를 이해하기 위한 방법으로 이를 통해 관계가 추정되면 결과를 예측할 수 있게 된다. 기계학습에서는 일반적으로 best fit을 그리는 작업을 의미하며 각 point 사이의 거리를 최소화하는 방법을 통해 best fit을 찾는다.Error (=loss): model의 오류 합계를 나타내는 값으로 model이 얼마나 잘 훈련되었는지를 판단한다. loss가 크면 model이 제대로 작동하지 않는다는 것을 의미한다.Classification: classification은 input의 주어진 data에 대하여 특정 class label을 예측하는 model을 의미한다.Accuracy: Accuracy는 model이 얼마나 잘 예측하고 있는지를 측정한다.2. &#51201;&#50857; &#44284;&#51228;&#182;Data documentation: Car Evaluation, BalloonsCar Evaluation의 feature: buying, maint, doors, persons, lung_boot, satefyCar Evalaution의 label: acceptBalloons의 feature: color, size, act, ageBalloons의 label: inflated2.1 pandas&#182;아래 data url을 통해 각 데이터마다 dataframe을 생성합니다.Car Evalaution의 모든 column name을 출력하시오.Car Evaluation의 buying feature에 어떤 카테고리가 있는지 출력하시오.Car Evaluation의 accept label의 각 class와 해당 class의 instance 개수를 메소드 하나로 출력하시오.Balloons에서 color feature이 yellow인 instance을 모두 출력하시오.In&nbsp;[&nbsp;]: # load datacar_url = &#39;https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data&#39;balloons_url = &#39;https://archive.ics.uci.edu/ml/machine-learning-databases/balloons/adult+stretch.data&#39; In&nbsp;[&nbsp;]: import numpy as npimport pandas as pdimport sklearn In&nbsp;[&nbsp;]: # 데이터 프레임 생성car_df = pd.read_csv(car_url, header = None)car_df.columns = [&quot;buying&quot;, &quot;maint&quot;, &quot;doors&quot;, &quot;persons&quot;, &quot;lung_boot&quot;, &quot;satefy&quot;, &quot;accept&quot;]bal_df = pd.read_csv(balloons_url, header = None)bal_df.columns = [&quot;color&quot;, &quot;size&quot;, &quot;act&quot;, &quot;age&quot;, &quot;inflated&quot;] In&nbsp;[&nbsp;]: # 데이터의 column name 출력print(car_df.columns.to_list()[:-1])print(bal_df.columns.to_list()[:-1]) [&#39;buying&#39;, &#39;maint&#39;, &#39;doors&#39;, &#39;persons&#39;, &#39;lung_boot&#39;, &#39;satefy&#39;][&#39;color&#39;, &#39;size&#39;, &#39;act&#39;, &#39;age&#39;]In&nbsp;[&nbsp;]: # Car evaluation &#39;buying&#39; feature 의 카테고리 출력print(car_df[&#39;buying&#39;].unique()) [&#39;vhigh&#39; &#39;high&#39; &#39;med&#39; &#39;low&#39;]In&nbsp;[&nbsp;]: # Car evaluation &#39;accept&#39; label 의 각 class 와 instance 개수car_df[&#39;accept&#39;].value_counts(sort = True) Out[&nbsp;]:unacc 1210acc 384good 69vgood 65Name: accept, dtype: int64In&nbsp;[&nbsp;]: # Balloons에서 &#39;color&#39; feature이 yellow인 instance 출력bal_df[&#39;color&#39;].value_counts(sort = True)print(bal_df.loc[(bal_df[&#39;color&#39;] == &#39;YELLOW&#39;)]) color size act age inflated0 YELLOW SMALL STRETCH ADULT T1 YELLOW SMALL STRETCH ADULT T2 YELLOW SMALL STRETCH CHILD F3 YELLOW SMALL DIP ADULT F4 YELLOW SMALL DIP CHILD F5 YELLOW LARGE STRETCH ADULT T6 YELLOW LARGE STRETCH ADULT T7 YELLOW LARGE STRETCH CHILD F8 YELLOW LARGE DIP ADULT F9 YELLOW LARGE DIP CHILD F2.2 &#45936;&#51060;&#53552; &#51060;&#54644; &#48143; &#51204;&#52376;&#47532;&#182;데이터 정보를 읽고 각 feature이 무슨 의미인지 파악하여 서술합니다. 실습했던 내용을 바탕으로 Car Evaluation 데이터와 Balloons 데이터를 scikit-learn의 Categorical Naive Bayesian Classifier에 적합하도록, Object 타입을 정수형으로 전처리합니다.2.3 &#47784;&#45944; &#49373;&#49457;, &#54984;&#47144; &#48143; &#44208;&#44284; &#54644;&#49437;&#182;scikit-learn 패키지를 사용하여 car와 balloons 두 데이터에:Categorical Naive Bayesian Classifier을 fit합니다.두 데이터에 대하여 score를 출력합니다.각 class probability와 각 feature probability을 출력합니다.본인이 임의로 만든 두개의 각기 다른 instances에 대하여 예측을 출력합니다. (car 두 개, balloons 두 개, 총 네 개)모델 예측 결과를 데이터의 맥락으로 해설합니다. (ex. 자동차1의 가격이 높고 보수비용이 낮으며... 할 때, 모델은 자동차1의 평가를 매우 좋음으로 예측하였다.)Car Evaluation의 feature: buying, maint, doors, persons, lung_boot, satefybuying :차량의 구매 가격 maint : 차량을 유지보수하기 위한 가격doors : 문의 개수persons : 차량이 운반할 수 있는 사람의 수 (탑승인원)lung_boot : 차량 트렁크의 크기safety : 안전 측정에서 평가된 차량 안전Car Evalaution의 label: accept** accept 차량의 용인 가능성(구매 가능성)Balloons의 feature: color, size, act, agecolor : 풍선의 색상size : 풍선의 크기act : 풍선을 늘어나게 했는지 줄어들게 했는지 여부age : 나이 / 어른 아이 여부 Balloons의 label: inflated** inflated 풍선이 부풀려진 여부In&nbsp;[&nbsp;]: # Object 타입을 정수형으로 전처리car_df_enc = pd.DataFrame()bal_df_enc = pd.DataFrame()# Car evaluationfor col in car_df.columns: car_df[col] = car_df[col].astype(&#39;category&#39;)car_df_enc[&#39;buying&#39;] = car_df[&#39;buying&#39;].cat.codescar_df_enc[&#39;maint&#39;] = car_df[&#39;maint&#39;].cat.codescar_df_enc[&#39;doors&#39;] = car_df[&#39;doors&#39;].cat.codescar_df_enc[&#39;persons&#39;] = car_df[&#39;persons&#39;].cat.codescar_df_enc[&#39;lung_boot&#39;] = car_df[&#39;lung_boot&#39;].cat.codescar_df_enc[&#39;satefy&#39;] = car_df[&#39;satefy&#39;].cat.codescar_df_enc[&#39;accept&#39;] = car_df[&#39;accept&#39;].cat.codes# Balloonsfor col in bal_df.columns: bal_df[col] = bal_df[col].astype(&#39;category&#39;)bal_df_enc[&#39;color&#39;] = bal_df[&#39;color&#39;].cat.codesbal_df_enc[&#39;size&#39;] = bal_df[&#39;size&#39;].cat.codesbal_df_enc[&#39;act&#39;] = bal_df[&#39;act&#39;].cat.codesbal_df_enc[&#39;age&#39;] = bal_df[&#39;age&#39;].cat.codesbal_df_enc[&#39;inflated&#39;] = bal_df[&#39;inflated&#39;].cat.codes In&nbsp;[&nbsp;]: from sklearn.naive_bayes import CategoricalNBcar_model = CategoricalNB()bal_model = CategoricalNB() In&nbsp;[&nbsp;]: # Car evaluation fit &amp; score 출력car_features = car_df_enc.drop(columns=[&#39;accept&#39;])car_label = car_df_enc[&#39;accept&#39;]car_model.fit(car_features.values, car_label)car_score = car_model.score(car_features.values, car_label)car_score Out[&nbsp;]:0.8715277777777778In&nbsp;[&nbsp;]: # Balloons fit &amp; score 출력bal_features = bal_df_enc.drop(columns=[&#39;inflated&#39;])bal_label = bal_df_enc[&#39;inflated&#39;]bal_model.fit(bal_features.values, bal_label)bal_score = bal_model.score(bal_features, bal_label)bal_score /usr/local/lib/python3.8/dist-packages/sklearn/base.py:443: UserWarning: X has feature names, but CategoricalNB was fitted without feature names warnings.warn( Out[&nbsp;]:1.0In&nbsp;[&nbsp;]: # class probability&amp; feature probability 출력# Car evaluationfrom pprint import pprintcar_feature_log_prior = car_model.feature_log_prob_for featue_prior in car_feature_log_prior: pprint(np.exp(featue_prior))print(np.exp(car_model.class_log_prior_)) array([[0.28092784, 0.23195876, 0.29896907, 0.18814433], [0.01369863, 0.64383562, 0.32876712, 0.01369863], [0.26771005, 0.21334432, 0.22158155, 0.29736409], [0.01449275, 0.57971014, 0.39130435, 0.01449275]])array([[0.27319588, 0.23969072, 0.29896907, 0.18814433], [0.01369863, 0.64383562, 0.32876712, 0.01369863], [0.25947282, 0.22158155, 0.22158155, 0.29736409], [0.20289855, 0.39130435, 0.39130435, 0.01449275]])array([[0.21134021, 0.25773196, 0.26546392, 0.26546392], [0.21917808, 0.26027397, 0.26027397, 0.26027397], [0.2693575 , 0.24794069, 0.24135091, 0.24135091], [0.15942029, 0.23188406, 0.30434783, 0.30434783]])array([[0.00258398, 0.51421189, 0.48320413], [0.01388889, 0.51388889, 0.47222222], [0.47568013, 0.25803792, 0.26628195], [0.01470588, 0.45588235, 0.52941176]])array([[0.374677 , 0.35142119, 0.27390181], [0.34722222, 0.34722222, 0.30555556], [0.30420445, 0.32399011, 0.37180544], [0.60294118, 0.38235294, 0.01470588]])array([[0.52971576, 0.00258398, 0.46770026], [0.43055556, 0.01388889, 0.55555556], [0.22918384, 0.47568013, 0.29513603], [0.97058824, 0.01470588, 0.01470588]])[0.22222222 0.03993056 0.70023148 0.03761574]In&nbsp;[&nbsp;]: # Balloonsbal_feature_log_prior = bal_model.feature_log_prob_for featue_prior in bal_feature_log_prior: pprint(np.exp(featue_prior))print(np.exp(bal_model.class_log_prior_)) In&nbsp;[&nbsp;]: # car evaluation instances 예측# (&quot;vhigh&quot;, &quot;vhigh&quot;, 2, 2, &quot;small&quot;, &quot;high&quot;) [3, 3, 0, 0, 2, 0]# (&quot;low&quot;, &quot;low&quot;, &quot;5more&quot;, &quot;more&quot;, &quot;big, &quot;med) [1, 1, 3, 2, 0, 0]print(car_model.predict_proba([[3, 3, 0, 0, 2, 0]]), car_model.predict([[3, 3, 0, 0, 2, 0]]))print(car_model.predict_proba([[1, 1, 3, 2, 0, 0]]), car_model.predict([[1, 1, 3, 2, 0, 0]])) [[9.21115137e-04 4.43484909e-06 9.99074059e-01 3.90721613e-07]] [2][[0.20014669 0.19352277 0.09437552 0.51195502]] [3]In&nbsp;[&nbsp;]: # balloons instances 예측#(&quot;YELLOW&quot;,&quot;LARGE&quot;, &quot;STRETCH&quot;, &quot;ADULT&quot;) [1, 0, 1, 0]#(&quot;PULPLE&quot;, &quot;SMALL&quot;, &quot;DIP&quot;, &quot;ADULT&quot;) [0, 1, 0, 0]print(bal_model.predict_proba([[1, 0, 1, 0]]), bal_model.predict([[1, 0, 1, 0]]))print(bal_model.predict_proba([[0, 1, 0, 0]]), bal_model.predict([[0, 1, 0, 0]])) [[0.19107307 0.80892693]] [1][[0.79281184 0.20718816]] [0]In&nbsp;[&nbsp;]: bal_df_enc.head(15) Out[&nbsp;]: color size act age inflated 0 1 1 1 0 1 1 1 1 1 0 1 2 1 1 1 1 0 3 1 1 0 0 0 4 1 1 0 1 0 5 1 0 1 0 1 6 1 0 1 0 1 7 1 0 1 1 0 8 1 0 0 0 0 9 1 0 0 1 0 10 0 1 1 0 1 11 0 1 1 0 1 12 0 1 1 1 0 13 0 1 0 0 0 14 0 1 0 1 0 &lt;svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\" width=\"24px\"&gt; &lt;/svg&gt; &#47784;&#45944; &#50696;&#52769; &#44208;&#44284; &#54644;&#49444;&#182;car evaluation&#182;자동차 3(index 2)는 가격이 매우 높고 보수 비용도 매우 높으며 차량의 문이 2개이고 탑승 가능 인원이 2명이다. 트렁크 크기는 작고, 안전평가에서 매우 높은 평가를 받았는데 모델은 자동차 3을 수용불가로 예측했다.자동차 1728(index 1727) 가격이 낮고 보수 비용도 매우 낮으며 차량의 문은 5개 이상이고 인원도 이상이며 트렁크 크기가 크고 안전평가에서는 중간 단계의 평가를 받았다. 모델은 자동차 1728을 매우 좋음으로 예측하였다.balloons&#182;풍선 6(index5)는 노란색이고 크기가 크며 풍선을 늘리는 행동을 했고 어른이 불었다. 모델은 풍선6이 부풀려졌을 것으로 예측했다풍선14(index13)은 보라색이고 크기가 작으며 풍선을 줄이는 행위를 했고 어른이 불었다 모델은 풍선14이 부풀려지지 않았을 것으로 예측했다.3. Naive Bayes Classifier &#44396;&#54788;&#182;Naive Bayes Classifier을 코드로 구현하는 것의 문제는 feature dimension이 매우 커질 경우에, 0과 1사이의 확률을 곱하기 때문에 전체 곱이 0와 매우 가까워지며 가끔은 long double으로도 표현하기 어려울정도로 매우 작은 확률이 계산될 수 있습니다.따라서 log probability을 사용하며, 이에 대한 이점은log probability range가 $[-∞, 0]$ 으로 넓어집니다.if $a &lt; b$, then $ log(a) &lt; log(b)$$log(a \\cdot b) = log(a) + log(b)$ 와 같은 규칙을 적용할 수 있습니다.$P(y|x_1, ..., x_n) = argmax_y \\left[ \\prod_{i=1}^{n} P(x_i|y) \\right] P(y)$$P(x_i|y)$: likelihood probability$P(y)$: class prior probability위 식에 로그를 씌우면$\\log(P(y|x_1, ..., x_n)) = argmax_y \\left[ \\sum_{i=1}^{n} \\log(P(x_i|y)) \\right] + log(P(y))$식이 합으로 바뀌어 다룰 수 있는 숫자로 계산됩니다.log likelihood probability 계산 함수 작성log class prior probability 계산 함수 작성log posterior probability을 사용하여 예측하는 Naive Bayes Classifier 함수 작성car data instance, balloon data instance에 대해 예측 출력In&nbsp;[&nbsp;]: import math In&nbsp;[&nbsp;]: import pandas as pd In&nbsp;[&nbsp;]: cdf = pd.read_csv(car_url, header = None)bdf = pd.read_csv(balloons_url, header = None)cdf.columns = [&quot;buying&quot;, &quot;maint&quot;, &quot;doors&quot;, &quot;persons&quot;, &quot;lung_boot&quot;, &quot;satefy&quot;, &quot;accept&quot;]bdf.columns = [&quot;color&quot;, &quot;size&quot;, &quot;act&quot;, &quot;age&quot;, &quot;inflated&quot;] In&nbsp;[&nbsp;]: # log likelihood probabilitydef calculate_likelihood(df): likelihood = dict() y = df[df.columns[-1]] sz = df.size/df.columns.size for feature in df.columns[:-1]: likelihood[feature] ={} for categ in y.unique(): class_count = y.value_counts()[categ] feature_count = df[df.columns[:-1]][feature][y[y == categ].index.values.tolist()].value_counts().to_dict() for feat_cat, feat_count in feature_count.items(): likelihood[feature][feat_cat + &quot;_&quot; + categ] = feat_count/class_count return likelihooddef calc_prior(df): prior={} for feat in df.columns.to_list()[:-1]: values = df[feat].value_counts().to_dict() prior[feat] = {} for value, count in values.items(): prior[feat][value] = count/df[df.columns[:-1]].size return prior In&nbsp;[&nbsp;]: # log class prior probability def calculate_class_prob(y): class_prior = {} for categ in y.unique(): class_prior[categ] = math.log(y.value_counts(normalize = True)[categ]) return class_priorcalculate_class_prob(cdf[cdf.columns[-1]])[&#39;unacc&#39;] Out[&nbsp;]:-0.3563443107732141In&nbsp;[&nbsp;]: def naive_bayes_classifier(df, inst): likelihood = calculate_likelihood(df) prior = calc_prior(df) prob_out = dict() for categ in df[df.columns[-1]].unique(): calculate_class_prob(df[df.columns[-1]]) likesum = 0 for feature, feature_value in zip (df.columns[:-1], inst): if feature_value + &#39;_&#39; + categ not in likelihood[feature]: continue else: likesum += math.log(likelihood[feature][feature_value + &#39;_&#39; + categ]) class_prior = calculate_class_prob(df[df.columns[-1]]) if categ in class_prior: prob_out[categ] = likesum + class_prior[categ] else: continue result = min(prob_out, key = lambda x :prob_out[x]) print(prob_out) In&nbsp;[&nbsp;]: naive_bayes_classifier(cdf, [&quot;vhigh&quot;, &quot;vhigh&quot;, &quot;2&quot;, &quot;2&quot;, &quot;small&quot;, &quot;high&quot;]) {&#39;unacc&#39;: -7.298119948403321, &#39;acc&#39;: -8.33742842300862, &#39;vgood&#39;: -5.152134856369955, &#39;good&#39;: -6.769162938070731}In&nbsp;[&nbsp;]: naive_bayes_classifier(bdf, [&quot;PULPLE&quot;, &quot;SMALL&quot;, &quot;DIP&quot;, &quot;CHILD&quot;]) {&#39;T&#39;: -1.6094379124341003, &#39;F&#39;: -2.014903020542265}참조https://medium.com/@rangavamsi5/na%C3%AFve-bayes-algorithm-implementation-from-scratch-in-python-7b2cc39268b9https://machinelearningmastery.com/naive-bayes-classifier-scratch-python/"
    } ,
  
    {
      "title"       : "Machine Learning - Decision Tree",
      "category"    : "",
      "tags"        : "machine learning, study_model, Decision Tree",
      "url"         : "./MachineLearning_DT.html",
      "date"        : "2021-03-24 00:00:00 +0900",
      "description" : "ML Basic",
      "content"     : "Decision_Tree 결정 트리(Decision Tree)In&nbsp;[&nbsp;]: !pip install mglearn!pip install --upgrade joblib==1.1.0 Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/Requirement already satisfied: mglearn in /usr/local/lib/python3.7/dist-packages (0.1.9)Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from mglearn) (1.0.2)Requirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (from mglearn) (2.9.0)Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from mglearn) (1.3.5)Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from mglearn) (7.1.2)Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mglearn) (1.21.6)Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from mglearn) (1.1.0)Requirement already satisfied: cycler in /usr/local/lib/python3.7/dist-packages (from mglearn) (0.11.0)Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from mglearn) (3.2.2)Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;mglearn) (3.0.9)Requirement already satisfied: python-dateutil&gt;=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;mglearn) (2.8.2)Requirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;mglearn) (1.4.4)Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver&gt;=1.0.1-&gt;matplotlib-&gt;mglearn) (4.1.1)Requirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil&gt;=2.1-&gt;matplotlib-&gt;mglearn) (1.15.0)Requirement already satisfied: pytz&gt;=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas-&gt;mglearn) (2022.4)Requirement already satisfied: threadpoolctl&gt;=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn-&gt;mglearn) (3.1.0)Requirement already satisfied: scipy&gt;=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn-&gt;mglearn) (1.7.3)Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/Requirement already satisfied: joblib==1.1.0 in /usr/local/lib/python3.7/dist-packages (1.1.0)&#51648;&#45768; &#48520;&#49692;&#46020; (Gini Impurity)&#182;지니 불순도는 결정 트리의 분할기준 중 하나입니다.&lt;img src=https://s3.amazonaws.com/codecademy-content/programs/data-science-path/decision-trees/comparison_1.svg width=300px&gt;&lt;img src=https://s3.amazonaws.com/codecademy-content/programs/data-science-path/decision-trees/comparison_2.svg width=300px&gt;지니 불순도를 찾기 위해서는 1에서 시작해서 세트의 각 class 비율의 제곱을 빼면 됩니다.$$\\text{Gini Impurity} = 1 - \\text{Gini Index} \\\\ = 1 - \\sum_{i=1}^{K}p_{i}^{2}$$위 식에서 $K$은 class label의 개수이며, $p_i$은 $i$번째 class label의 비율입니다.예를 들어, A class인 instance가 3개 있고 B class인 instance가 1개 있는 데이터의 경우에는 지니 불순도는 아래와 같이 계산됩니다.$$1 - (3/4)^2 - (1/4)^2 = 0.375$$만약 데이터가 하나의 class만 있다면, 지니 불순도는 0이 됩니다. 불순도가 낮으면 낮을수록 결정 트리의 성능은 더 좋아집니다.&#49892;&#49845; 1&#182;위 정리에서 주어진 Tree의 불순도 계산In&nbsp;[&nbsp;]: 1 - (4/6)**2 - (2/6)**2 Out[&nbsp;]:0.4444444444444445sample_labels 리스트의 지니 불순도 계산In&nbsp;[&nbsp;]: sample_labels = [&quot;unacc&quot;, &quot;unacc&quot;, &quot;acc&quot;, &quot;acc&quot;, &quot;good&quot;, &quot;good&quot;]impurity = 1 sample labels에 포함되어있는 class의 개수In&nbsp;[&nbsp;]: from collections import Counterlabel_counts = Counter(sample_labels)print(label_counts) Counter({&#39;unacc&#39;: 2, &#39;acc&#39;: 2, &#39;good&#39;: 2})데이터셋에서 각 label의 확률 계산In&nbsp;[&nbsp;]: for label in label_counts: print(label) prob = label_counts[label]/len(sample_labels) print(prob) unacc0.3333333333333333acc0.3333333333333333good0.3333333333333333확률을 이용하여 sample_labels의 불순도 계산In&nbsp;[&nbsp;]: for label in label_counts: prob = label_counts[label]/len(sample_labels) impurity -= prob ** 2print(impurity) 0.6666666666666665지니 불순도를 계산하는 코드 함수 제작In&nbsp;[&nbsp;]: def gini(dataset): impurity = 1 label_counts =Counter(dataset) for label in label_counts: prob_of_label = label_counts[label] / len(dataset) impurity -= prob_of_label ** 2 return impurity &#51221;&#48372;&#51613;&#44032;&#47049; (Information Gain)&#182;이제 지니 불순도가 낮은 끝마디(leaf node)를 만들기 위해서 어떠한 feature에 따라 데이터를 나누어야하는지 결정해야 합니다.예를 들어, 학생들의 수면 시간 또는 학생들의 공부 시간 둘 중 어느 feature을 기준으로 학생들을 나누어야 더 좋은 tree를 만들 수 있을까요?위 질문에 답하기 위해 어떠한 feature에 대하여 데이터를 나누었을 때의 정보증가량을 계산해야 합니다.정보증가량은 데이터 분할 전과 후의 불순도 차이를 측정합니다.예를 들어, 불순도가 0.5인 데이터를 어떠한 feature에 대해 나누었을 때, 불순도가 각각 0, 0.375, 0 인 끝마디가 생긴다고 가정해봅니다.&lt;img src=https://s3.amazonaws.com/codecademy-content/programs/data-science-path/decision-trees/info.svg width=300px&gt;이 경우에 데이터를 나누는 정보증가량은 0.5 - 0 - 0.375 - 0 = 0.125 입니다.데이터를 나누었을때의 정보 증가량은 양수입니다. 따라서, 위처럼 결정 지점을 나눈 것은 결과적으로 불순도를 낮추었기 때문에 좋은 결정 지점입니다.정보증가량은 크면 클수록 좋습니다.&#49892;&#49845; 2&#182;unsplit_labels라는 임의의 데이터를 두가지 다른 분할 지점으로 나누었습니다. 이는 split_labels_1와 split_labels_2 입니다.각 분할에 대해 information gain 계산In&nbsp;[&nbsp;]: unsplit_labels = [&quot;unacc&quot;, &quot;unacc&quot;, &quot;unacc&quot;, &quot;unacc&quot;, &quot;unacc&quot;, &quot;unacc&quot;, &quot;good&quot;, &quot;good&quot;, &quot;good&quot;, &quot;good&quot;, &quot;vgood&quot;, &quot;vgood&quot;, &quot;vgood&quot;]split_labels_1 = [[&quot;unacc&quot;, &quot;unacc&quot;, &quot;unacc&quot;, &quot;unacc&quot;, &quot;unacc&quot;, &quot;unacc&quot;, &quot;good&quot;, &quot;good&quot;, &quot;vgood&quot;], [ &quot;good&quot;, &quot;good&quot;], [&quot;vgood&quot;, &quot;vgood&quot;]]split_labels_2 = [[&quot;unacc&quot;, &quot;unacc&quot;, &quot;unacc&quot;, &quot;unacc&quot;,&quot;unacc&quot;, &quot;unacc&quot;, &quot;good&quot;, &quot;good&quot;, &quot;good&quot;, &quot;good&quot;], [&quot;vgood&quot;, &quot;vgood&quot;, &quot;vgood&quot;]] In&nbsp;[&nbsp;]: # unsplit_labels의 지니 불순도를 계산해봅니다.info_gain_1 = gini(unsplit_labels)info_gain_1 Out[&nbsp;]:0.6390532544378698split_labels_1의 각 부분집합에 대하여 지니 불순도을 계산하여 정보 증가량 계산In&nbsp;[&nbsp;]: for subset in split_labels_1: info_gain_1 -= gini(subset)print(info_gain_1) 0.14522609394404257split_labels_2에 대해 정보증가량을 계산In&nbsp;[&nbsp;]: info_gain_2 = gini(unsplit_labels)for subset in split_labels_2: info_gain_2 -= gini(subset)print(info_gain_2) 0.15905325443786977정보증가량을 계산하는 함수In&nbsp;[&nbsp;]: def information_gain(starting_labels, split_labels): info_gain = gini(starting_labels) for subset in split_labels: info_gain -= gini(subset) return info_gain &#44032;&#51473; &#51221;&#48372;&#51613;&#44032;&#47049; (Weighted Information Gain)&#182;만약 정보증가량이 0이라면 그 feature에 대해 데이터를 나누는 것은 소용이 없습니다. 때에 따라서 데이터를 나누었을 때 정보증가량이 음수가 될 수 있습니다. 이 문제를 해결하기 위해서 가중 정보증가량 (weighted information gain)을 사용합니다.분할 후에 생성되는 데이터의 부분집합의 크기 또한 중요합니다. 예를 들어서, 아래 이미지에서는 불순도가 같은 두 부분집합이 있습니다.&lt;img src=https://s3.amazonaws.com/codecademy-content/programs/data-science-path/decision-trees/impurity-0.svg width=300px&gt;어느 부분집합을 결정 트리의 끝마디로 정하는게 좋은 결정트리를 만들 수 있을까요?두 부분집합은 모두 불순도가 0으로써 완전하지만, 두 번째 부분집합이 더욱 의미있습니다. 두 번째 부분집합에는 많은 개수의 instance들이 있기 때문에 이 부분집합이 구성된 것이 우연이 아님을 알수 있습니다.그 반대를 생각해보는 것도 도움이 됩니다. 아래 그림에서 같은 값의 불순도를 가지고 있는 두 부분집합이 있습니다.&lt;img src=https://s3.amazonaws.com/codecademy-content/programs/data-science-path/decision-trees/impurity-5.svg width=300px&gt;이 두 부분집합의 불순도는 굉장히 높습니다. 그렇지만 어느 부분집합의 불순도가 더 큰 의미를 가질까요? 왼쪽의 부분집합을 분할하는 것보다는 오른쪽 부분집합을 분할하여 불순도가 없는 집합을 만드는 것이 정보증가량이 더 클 것입니다. 따라서, 집합의 instance 개수를 고려하여 정보증가량을 계산해야 합니다.집합의 크기까지 고려하도록 정보증가량 함수를 수정할 것 입니다. 단순히 불순도를 빼는 것에서 더 나아가 분할된 부분집합의 가중 불순도를 뺄 것입니다. 만약 분할 전의 데이터가 10개의 instance을 가지고 있고 하나의 부분집합이 2개의 instance가 있다면, 그 부분집합의 가중 불순도는 2/10 * impurity가 되어 instance 숫자가 적은 세트의 중요도를 낮춥니다.가중 정보증가량 계산의 예시는 아래와 같습니다.&lt;img src=https://s3.amazonaws.com/codecademy-content/programs/data-science-path/decision-trees/weighted_info.svg&gt;&#49892;&#49845; 3&#182;아래는 데이터셋의 각 feature와 class label에 대한 설명입니다. Car dataset은 class에 해당하는 4가지 label과 각 차량의 특징을 나타내는 6개의 feature을 갖고 있습니다.Label은 4개의 class, unacc(unacceptable), acc(acceptable), good, vgood로 이루어져 있으며, 각 class는 차량 구매시의 만족도(acceptability)를 나타냅니다.각 차량은 6개의 feature을 가지고 있고, 아래와 같습니다.buying (차량의 가격): \"vhigh\",\"high\",\"med\", or \"low\".maint (차량 유지 비용): \"vhigh\",\"high\",\"med\", or \"low\".doors (차의 문 갯수): \"2\",\"3\",\"4\",\"5more\".persons (차량의 최대 탑승 인원): \"2\",\"4\", or \"more\".lug_boot (차량 트렁크의 사이즈): \"small\",\"med\", or \"big\"safety (차량의 안전성 등급): \"low\",\"med\", or \"high\".In&nbsp;[&nbsp;]: # 샘플 데이터cars = [[&#39;med&#39;, &#39;low&#39;, &#39;3&#39;, &#39;4&#39;, &#39;med&#39;, &#39;med&#39;], [&#39;med&#39;, &#39;vhigh&#39;, &#39;4&#39;, &#39;more&#39;, &#39;small&#39;, &#39;high&#39;], [&#39;high&#39;, &#39;med&#39;, &#39;3&#39;, &#39;2&#39;, &#39;med&#39;, &#39;low&#39;], [&#39;med&#39;, &#39;low&#39;, &#39;4&#39;, &#39;4&#39;, &#39;med&#39;, &#39;low&#39;], [&#39;med&#39;, &#39;low&#39;, &#39;5more&#39;, &#39;2&#39;, &#39;big&#39;, &#39;med&#39;], [&#39;med&#39;, &#39;med&#39;, &#39;2&#39;, &#39;more&#39;, &#39;big&#39;, &#39;high&#39;], [&#39;med&#39;, &#39;med&#39;, &#39;2&#39;, &#39;more&#39;, &#39;med&#39;, &#39;med&#39;], [&#39;vhigh&#39;, &#39;vhigh&#39;, &#39;2&#39;, &#39;2&#39;, &#39;med&#39;, &#39;low&#39;], [&#39;high&#39;, &#39;med&#39;, &#39;4&#39;, &#39;2&#39;, &#39;big&#39;, &#39;low&#39;], [&#39;low&#39;, &#39;low&#39;, &#39;2&#39;, &#39;4&#39;, &#39;big&#39;, &#39;med&#39;]]car_labels = [&#39;acc&#39;, &#39;acc&#39;, &#39;unacc&#39;, &#39;unacc&#39;, &#39;unacc&#39;, &#39;vgood&#39;, &#39;acc&#39;, &#39;unacc&#39;, &#39;unacc&#39;, &#39;good&#39;] information_gain 함수를 수정하여 가중 정보증가량을 계산가중치: 부분집합의 label 갯수 `len(subset)` / 분할 전의 집합의 label 갯수 `len(starting_labels)`In&nbsp;[&nbsp;]: def weighted_information_gain(starting_labels, split_labels): info_gain = gini(starting_labels) for subset in split_labels: info_gain -= gini(subset) * (len(subset) / len(starting_labels)) return info_gain 아래 split() 함수를 살펴보겠습니다.In&nbsp;[&nbsp;]: def split(dataset, labels, column): data_subsets = [] label_subsets = [] # empty list counts = list(set([data[column] for data in dataset])) # list 의 중복 항목 제거를 위한 set 변환 for k in counts: # k=counts element [&#39;2&#39;, &#39;4&#39;, &#39;more&#39;] new_data_subset = [] new_label_subset = [] for i in range(len(dataset)): # data set len -&gt; all looping if dataset[i][column] == k: new_data_subset.append(dataset[i]) new_label_subset.append(labels[i]) data_subsets.append(new_data_subset) label_subsets.append(new_label_subset) return data_subsets, label_subsets In&nbsp;[&nbsp;]: # split 함수 호출split_data, split_labels = split(cars, car_labels, 3) In&nbsp;[&nbsp;]: split_data Out[&nbsp;]:[[[&#39;med&#39;, &#39;vhigh&#39;, &#39;4&#39;, &#39;more&#39;, &#39;small&#39;, &#39;high&#39;], [&#39;med&#39;, &#39;med&#39;, &#39;2&#39;, &#39;more&#39;, &#39;big&#39;, &#39;high&#39;], [&#39;med&#39;, &#39;med&#39;, &#39;2&#39;, &#39;more&#39;, &#39;med&#39;, &#39;med&#39;]], [[&#39;med&#39;, &#39;low&#39;, &#39;3&#39;, &#39;4&#39;, &#39;med&#39;, &#39;med&#39;], [&#39;med&#39;, &#39;low&#39;, &#39;4&#39;, &#39;4&#39;, &#39;med&#39;, &#39;low&#39;], [&#39;low&#39;, &#39;low&#39;, &#39;2&#39;, &#39;4&#39;, &#39;big&#39;, &#39;med&#39;]], [[&#39;high&#39;, &#39;med&#39;, &#39;3&#39;, &#39;2&#39;, &#39;med&#39;, &#39;low&#39;], [&#39;med&#39;, &#39;low&#39;, &#39;5more&#39;, &#39;2&#39;, &#39;big&#39;, &#39;med&#39;], [&#39;vhigh&#39;, &#39;vhigh&#39;, &#39;2&#39;, &#39;2&#39;, &#39;med&#39;, &#39;low&#39;], [&#39;high&#39;, &#39;med&#39;, &#39;4&#39;, &#39;2&#39;, &#39;big&#39;, &#39;low&#39;]]]In&nbsp;[&nbsp;]: len(split_data) Out[&nbsp;]:3split_labels를 사용, index 3에 대해 스플릿한 information gainIn&nbsp;[&nbsp;]: # index 3으로 데이터를 분할하였을 때 정보증가량을 출력weighted_information_gain(car_labels, split_labels) Out[&nbsp;]:0.30666666666666675정보증가량을 찾는 과정을 모든 feature에 대해서 적용In&nbsp;[&nbsp;]: # 데이터에 있는 모든 feature들에 대하여 `split()` 함수와 `information_gain()` 함수를 호출 # 4th feature(persons feature)가 가장 큰 영향을 미친다.for i in range(0,6): split_data, split_labels = split(cars, car_labels, i) print(weighted_information_gain(car_labels, split_labels)) 0.27333333333333340.0400000000000000360.106666666666666660.306666666666666750.150000000000000020.29000000000000004&#49892;&#49845; 4: &#51116;&#44480; &#53944;&#47532; &#47564;&#46308;&#44592; (Recursive Tree Building)&#182;데이터를 분할하였을 때 정보증가량이 가장 높은 feature을 찾을 수 있습니다. 이 방법을 반복하는 재귀 알고리즘을 통하여 트리를 구성할 수 있습니다. 데이터의 모든 instance에서 시작하여 데이터를 분할할 가장 좋은 feature을 찾고, 그 feature에 대해서 데이터를 나눈 후에 생성된 부분집합에 대해서 재귀적으로 위의 순서를 되풀이합니다.정보증가량이 일어나지 않는 feature을 찾을 때까지 재귀를 반복합니다. 다른 말로, 우리는 더이상 불순도가 없는 부분집합을 만드는 분할이 존재하지 않을 때 결정 트리의 끝마디를 생성합니다. 이 끝마디는 전체 데이터에서 분류된 instance의 class을 담고 있습니다.In&nbsp;[&nbsp;]: # 위의 함수들을 종합하여 가장 적합한 분할 feature을 찾는 함수 작성def find_best_split(dataset, labels): best_gain = 0 best_feature = 0 for feature in range(len(dataset[0])): data_subset, label_subset = split(dataset, labels, feature) gain = weighted_information_gain(labels, label_subset) if gain &gt; best_gain: best_gain, best_feature = gain, feature return best_gain, best_feature 위 함수를 cars와 car_labels에 대해 호출In&nbsp;[&nbsp;]: best_gain, best_feature = find_best_split(cars, car_labels) In&nbsp;[&nbsp;]: best_feature Out[&nbsp;]:3In&nbsp;[&nbsp;]: best_gain Out[&nbsp;]:0.30666666666666675data와 labels를 파라미터로 받는 build_tree()라는 함수를 선언이 함수는 재귀적으로 트리를 구성합니다.In&nbsp;[&nbsp;]: def build_tree(data, labels): best_gain, best_feature = find_best_split(data, labels) if best_gain == 0: return Counter(labels) data_subsets, label_subsets = split(data, labels, best_feature) branches = [] for i in range(len(data_subsets)): branch = build_tree(data_subsets[i], label_subsets[i]) branches.append(branch) return branches 만들어진 build_tree 함수 테스트In&nbsp;[&nbsp;]: def print_tree(node, spacing=&quot;&quot;): question_dict = {0: &quot;Buying Price&quot;, 1:&quot;Price of maintenance&quot;, 2:&quot;Number of doors&quot;, 3:&quot;Person Capacity&quot;, 4:&quot;Size of luggage boot&quot;, 5:&quot;Estimated Saftey&quot;} # Base case: 끝노드에 도달함 if isinstance(node, Counter): print (spacing + str(node)) return print (spacing + &quot;Splitting&quot;) # 분할 지점에서 각 브랜치에 대해 재귀적으로 print_tree 함수를 호출 for i in range(len(node)): print (spacing + &#39;--&gt; Branch &#39; + str(i)+&#39;:&#39;) print_tree(node[i], spacing + &quot; &quot;) In&nbsp;[&nbsp;]: # `build_tree` 함수와 `print_tree` 함수를 출력해봅니다.tree = build_tree(cars, car_labels)print_tree(tree) Splitting--&gt; Branch 0: Splitting --&gt; Branch 0: Counter({&#39;acc&#39;: 1}) --&gt; Branch 1: Counter({&#39;acc&#39;: 1}) --&gt; Branch 2: Counter({&#39;vgood&#39;: 1})--&gt; Branch 1: Splitting --&gt; Branch 0: Counter({&#39;unacc&#39;: 1}) --&gt; Branch 1: Counter({&#39;good&#39;: 1}) --&gt; Branch 2: Counter({&#39;acc&#39;: 1})--&gt; Branch 2: Counter({&#39;unacc&#39;: 4})&#49892;&#49845; 5: scikit-learn&#51004;&#47196; &#44396;&#54788;&#54616;&#45716; &#44208;&#51221;&#53944;&#47532;&#182;scikit-learn에서 제공되는 make_moons 데이터를 사용합니다.In&nbsp;[&nbsp;]: import matplotlib.pyplot as pltimport seaborn as snsfrom sklearn.datasets import make_moonsX, y = make_moons(noise=0.32, random_state=42, n_samples=250)sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y, marker=&quot;o&quot;, s=25, edgecolor=&quot;k&quot;, legend=False).set_title(&quot;Moon Data&quot;)plt.show() scikit-learn 패키지로 결정트리 구현DecisionTreeClassifier` 분류기를 사용합니다.[scikit-learn DecisionTreeClassifier documentation] (https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html?highlight=decisiontreeclassifier#sklearn.tree.DecisionTreeClassifier)In&nbsp;[&nbsp;]: from sklearn.tree import DecisionTreeClassifier In&nbsp;[&nbsp;]: dt = DecisionTreeClassifier() .fit() 메소드를 통해 데이터를 Tree에 훈련In&nbsp;[&nbsp;]: dt.fit(X,y) Out[&nbsp;]:DecisionTreeClassifier()정확도 확인In&nbsp;[&nbsp;]: dt.score(X,y) Out[&nbsp;]:1.0완성된 결정트리 시각화In&nbsp;[&nbsp;]: # classifier 결정트리를 시각화from sklearn.tree import export_graphviz # drawing graphs specified in DOT language scriptsfrom six import StringIOfrom IPython.display import Image import pydotplusdot_data = StringIO()export_graphviz(dt, out_file=dot_data)graph = pydotplus.graph_from_dot_data(dot_data.getvalue())graph.write_png(&#39;/tree.png&#39;)Image(graph.create_png()) Out[&nbsp;]:결정 경계 시각화In&nbsp;[&nbsp;]: from mglearn import plot_interactive_treeax = plot_interactive_tree.plot_tree_partition(X, y, dt)ax.set_title(&quot;first decision tree&quot;) Out[&nbsp;]:Text(0.5, 1.0, &#39;first decision tree&#39;) &#49892;&#49845; 6: &#44208;&#51221; &#53944;&#47532; &#44032;&#51648;&#52824;&#44592; (pruning)&#182;가지치기란 최대트리로 형성된 결정트리의 특정 노드 밑의 하부 트리를 제거하여 일반화 성능을 높히는 것을 의미합니다. 모든 끝노드의 불순도가 0인 트리를 full tree라고 하는데, 이 경우에는 분할이 너무 많이 과적합의 위험이 발생합니다. 과적합은 학습 데이터에 과하게 학습하여 실제 데이터에 오차가 증가하는 현상입니다. 이를 방지하기 위해서 적절한 수준에서 끝노드를 결합해주는 기법을 가지치기(pruning)이라고 합니다.scikit-learn DecisionTreeClassifier documentation새로운 결정트리를 생성 (깊이를 지정)In&nbsp;[&nbsp;]: pruned_dt = DecisionTreeClassifier(max_depth = 3)pruned_dt.fit(X, y)print(pruned_dt.score(X,y)) 0.888가지치기된 트리를 시각화이렇게 끝노드의 개수를 지정해주면 트리가 데이터에 더욱 잘 일반화됩니다.In&nbsp;[&nbsp;]: dot_data = StringIO()export_graphviz( pruned_dt, out_file=dot_data)graph = pydotplus.graph_from_dot_data(dot_data.getvalue())graph.write_png(&#39;/tree.png&#39;)Image(graph.create_png()) Out[&nbsp;]:결정 경계 시각화를 통한 비교In&nbsp;[&nbsp;]: ax = plot_interactive_tree.plot_tree_partition(X, y, pruned_dt)ax.set_title(&quot;first decision tree&quot;) Out[&nbsp;]:Text(0.5, 1.0, &#39;first decision tree&#39;) &#44284;&#51228;&#182;Kaggle의 Titanic 데이터를 사용타이타닉 데이터의 feature:Pclass: 승객 등급. 1등급=1, 2등급=2, 3등급=3Sex: 성별Age: 나이SibSp: 함께 탑승한 형제 또는 배우자 수Parch: 함께 탑승한 부모 또는 자녀 수Fare: 여객 운임Label: Survived 생존=1, 죽음=0데이터 파악 및 전처리In&nbsp;[&nbsp;]: import pandas as pddata_url = &quot;https://raw.githubusercontent.com/inikoreaackr/ml_datasets/main/titanic.csv&quot;data = pd.read_csv(data_url)data Out[&nbsp;]: PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked 0 1 0 3 Braund, Mr. Owen Harris male 22.0 1 0 A/5 21171 7.2500 NaN S 1 2 1 1 Cumings, Mrs. John Bradley (Florence Briggs Th... female 38.0 1 0 PC 17599 71.2833 C85 C 2 3 1 3 Heikkinen, Miss. Laina female 26.0 0 0 STON/O2. 3101282 7.9250 NaN S 3 4 1 1 Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35.0 1 0 113803 53.1000 C123 S 4 5 0 3 Allen, Mr. William Henry male 35.0 0 0 373450 8.0500 NaN S ... ... ... ... ... ... ... ... ... ... ... ... ... 886 887 0 2 Montvila, Rev. Juozas male 27.0 0 0 211536 13.0000 NaN S 887 888 1 1 Graham, Miss. Margaret Edith female 19.0 0 0 112053 30.0000 B42 S 888 889 0 3 Johnston, Miss. Catherine Helen \"Carrie\" female NaN 1 2 W./C. 6607 23.4500 NaN S 889 890 1 1 Behr, Mr. Karl Howell male 26.0 0 0 111369 30.0000 C148 C 890 891 0 3 Dooley, Mr. Patrick male 32.0 0 0 370376 7.7500 NaN Q 891 rows × 12 columns &lt;svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\" width=\"24px\"&gt; &lt;/svg&gt; In&nbsp;[&nbsp;]: data.columnsdata = data[[&#39;Survived&#39;, &#39;Pclass&#39;, &#39;Sex&#39;, &#39;Age&#39;, &#39;SibSp&#39;, &#39;Parch&#39;, &#39;Fare&#39;]]data.describe() Out[&nbsp;]: Survived Pclass Age SibSp Parch Fare count 891.000000 891.000000 714.000000 891.000000 891.000000 891.000000 mean 0.383838 2.308642 29.699118 0.523008 0.381594 32.204208 std 0.486592 0.836071 14.526497 1.102743 0.806057 49.693429 min 0.000000 1.000000 0.420000 0.000000 0.000000 0.000000 25% 0.000000 2.000000 20.125000 0.000000 0.000000 7.910400 50% 0.000000 3.000000 28.000000 0.000000 0.000000 14.454200 75% 1.000000 3.000000 38.000000 1.000000 0.000000 31.000000 max 1.000000 3.000000 80.000000 8.000000 6.000000 512.329200 &lt;svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\" width=\"24px\"&gt; &lt;/svg&gt; In&nbsp;[&nbsp;]: data = data.dropna() In&nbsp;[&nbsp;]: data.shape Out[&nbsp;]:(714, 7)In&nbsp;[&nbsp;]: data.dtypes Out[&nbsp;]:Survived int64Pclass int64Sex objectAge float64SibSp int64Parch int64Fare float64dtype: objectIn&nbsp;[&nbsp;]: from sklearn.preprocessing import LabelEncoderle = LabelEncoder()data.Sex = le.fit_transform(data.Sex) /usr/local/lib/python3.7/dist-packages/pandas/core/generic.py:5516: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.Try using .loc[row_indexer,col_indexer] = value insteadSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy self[name] = valueIn&nbsp;[&nbsp;]: data.dtypes Out[&nbsp;]:Survived int64Pclass int64Sex int64Age float64SibSp int64Parch int64Fare float64dtype: objectIn&nbsp;[&nbsp;]: y = data[&#39;Survived&#39;]X = data.drop(columns = [&#39;Survived&#39;]) 기계학습 모델을 훈련시키고 성능을 파악하기 위해서는 데이터를 훈련 데이터와 테스트 데이터로 나누어야 합니다.&lt;img src=https://upload.wikimedia.org/wikipedia/commons/b/bb/ML_dataset_training_validation_test_sets.png width=500px&gt;scikit-learn에서 지원하는 train_test_split 을 사용합니다. scikit-learn train_test_split documentationIn&nbsp;[&nbsp;]: from sklearn.model_selection import train_test_splitX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3) DecisionTreeClassifier 분류기를 사용해 결정트리를 생성In&nbsp;[&nbsp;]: dt = DecisionTreeClassifier() 트레이팅 데이터에 .fit() 메소드를 호출함으로써 트리를 데이터에 훈련, .fit()메소드는 training_points와 training_labels을 파라미터로 받음.In&nbsp;[&nbsp;]: dt.fit(X_train, y_train) Out[&nbsp;]:DecisionTreeClassifier()testing_points와 testing_labels에 대한 결정 트리의 정확도(.score())를 출력In&nbsp;[&nbsp;]: print(dt.score(X_test, y_test)) 0.8046511627906977훈련된 트리 시각화In&nbsp;[&nbsp;]: dot_data = StringIO()export_graphviz(dt, out_file=dot_data)graph = pydotplus.graph_from_dot_data(dot_data.getvalue())graph.write_png(&#39;/tree.png&#39;)Image(graph.create_png()) Out[&nbsp;]:scikit-learn DecisionTreeClassifier documentation과제 1max_leaf_nodes : 트리가 가질 수 있는 최대 leaf node의 수를 지정하는 parameter입니다. 해당 parameter가 지정되지 않은 경우 leaf node 수의 제한을 두지 않습니다.max_depth : 트리의 최대 깊이 parameter가 None일 때는 node가 모든 잎들이 pure해질 때까지 혹은, min_samples_split 이 지정하는 값 이하의 수를 포함하도록 확장됩니다.min_sample_split : node를 분할하기 위해 필요한 최소 샘플 수를 지정하는 parameter로 정수일 때는 최소 숫자로 간주되고 float일 때는 전체 샘플에 대한 min_sample_split의 비율만큼을 최소 샘플로 합니다.min_sample_leaf : leaf nodes에 필요한 최소 샘플의 수르 지정하는 parameter로 양쪽 가지에 필요한 최소 training sample을 의미합니다. 특히, regression에서는 모형을 smoothing하는데 효과가 있습니다.min_impurity_decrease : impurity가 감소하는 경우 node를 분할하도록 합니다.In&nbsp;[&nbsp;]: from sklearn.tree import DecisionTreeClassifierdf = DecisionTreeClassifier() In&nbsp;[&nbsp;]: dt_mxlf = DecisionTreeClassifier(max_leaf_nodes = 1000)dt_mxdth = DecisionTreeClassifier(max_depth = 6)dt_mss = DecisionTreeClassifier(min_samples_split = 8)dt_msl = DecisionTreeClassifier(min_samples_leaf = 32) dt_mid = DecisionTreeClassifier(min_impurity_decrease = 0.02) In&nbsp;[&nbsp;]: # prunig parameter를 조절하지 않은 Decision treedt.fit(X_train, y_train)dt.score(X_test, y_test) Out[&nbsp;]:0.8232558139534883In&nbsp;[&nbsp;]: # `max_leaf_nodes` parameter를 조절한 Decision treedt_mxlf.fit(X_train, y_train)dt_mxlf.score(X_test, y_test) Out[&nbsp;]:0.8186046511627907In&nbsp;[&nbsp;]: # `max_depth` parameter를 조절한 Decision treedt_mxdth.fit(X_train, y_train)dt_mxdth.score(X_test, y_test) Out[&nbsp;]:0.8372093023255814In&nbsp;[&nbsp;]: #`min_sample_split` parameter를 조절한 Decision treedt_mss.fit(X_train, y_train)dt_mss.score(X_test, y_test) Out[&nbsp;]:0.8In&nbsp;[&nbsp;]: #`min_sample_leaf` parameter를 조절한 Decision treedt_msl.fit(X_train, y_train)dt_msl.score(X_test, y_test) Out[&nbsp;]:0.827906976744186In&nbsp;[&nbsp;]: #`min_impurity_decrease` parameter를 조절한 Decision treedt_mid.fit(X_train, y_train)dt_mid.score(X_test, y_test) Out[&nbsp;]:0.8과제3prunig 파라미터를 조정하지 않은 Decision tree의 경우 분류 정확도가 [0.7488372093023256]로 pruning 파라미터를 조정한 Decision tree의 분류 정확도가 각각 [0.7813953488372093, 0.8, 0.7813953488372093, 0.813953488372093, 0.7813953488372093] 인 것과 비교해 볼 때 정확도가 떨어집니다.In&nbsp;[&nbsp;]: # `max_leaf_nodes` parameter를 조절한 Decision treedot_data = StringIO()export_graphviz(dt_mxlf, out_file=dot_data)graph = pydotplus.graph_from_dot_data(dot_data.getvalue())graph.write_png(&#39;/tree.png&#39;)Image(graph.create_png()) Out[&nbsp;]:In&nbsp;[&nbsp;]: # `max_depth` parameter를 조절한 Decision treedot_data = StringIO()export_graphviz(dt_mxdth, out_file=dot_data)graph = pydotplus.graph_from_dot_data(dot_data.getvalue())graph.write_png(&#39;/tree.png&#39;)Image(graph.create_png()) Out[&nbsp;]:In&nbsp;[&nbsp;]: #`min_sample_split` parameter를 조절한 Decision treedot_data = StringIO()export_graphviz(dt_mss, out_file=dot_data)graph = pydotplus.graph_from_dot_data(dot_data.getvalue())graph.write_png(&#39;/tree.png&#39;)Image(graph.create_png()) Out[&nbsp;]:In&nbsp;[&nbsp;]: #`min_sample_leaf` parameter를 조절한 Decision treedot_data = StringIO()export_graphviz(dt_msl, out_file=dot_data)graph = pydotplus.graph_from_dot_data(dot_data.getvalue())graph.write_png(&#39;/tree.png&#39;)Image(graph.create_png()) Out[&nbsp;]:In&nbsp;[&nbsp;]: #`min_impurity_decrease` parameter를 조절한 Decision treedot_data = StringIO()export_graphviz(dt_mid, out_file=dot_data)graph = pydotplus.graph_from_dot_data(dot_data.getvalue())graph.write_png(&#39;/tree.png&#39;)Image(graph.create_png()) Out[&nbsp;]:과제 5번max_leaf_nodes, max_depth, min_samples_splitparameter를 조정한 모델은 모두 트리 모델의 끝마디에서 Gini index는 높으나 node가 가지고 있는 sample의 수가 1 또는 2로 overfitting 되었을 확률이 높은 노드들을 다수 가지고 있으므로 적합하게 학습된 모델이라 할 수 없습니다. 한편 min_impurity_decrease parameter를 조절한 Decision tree 모델의 경우에는 tree 의 끝마디 noder가 가진 gini index가 다소 높게 측정되었기 때문에 적합되었다고 보기 어렵습니다.반면 'min_samples_leaf` parameter를 조절한 Decision tree는 하나의 끝마디에서 gini index가 다소 높게 측정되기는 했지만 각각의 끝마디 gini index가 낮게 나왔으며 각 끝마디의 샘플 수도 적정하기 때문에 다른 parameter를 조절한 Decision tree에 비교해 가장 적합하게, 가장 일반화를 잘 실행한 모델이라 볼 수 있을 것입니다."
    } ,
  
    {
      "title"       : "Starting the adventure",
      "category"    : "",
      "tags"        : "general blogging, thoughts, life",
      "url"         : "./starting-the-adventure.html",
      "date"        : "2021-03-24 00:00:00 +0900",
      "description" : "Midlife career change: a disaster or an opportunity?",
      "content"     : "In the midst of a global pandemic caused by the SARS-COV2 coronavirus; I decided to start blogging. I wanted to blog since a long time, I have always enjoyed writing, but many unknowns and having “no time” for it prevented me from taking it up. Things like: “I don’t really know who my target audience is”, “what would my topic or topics be?”, “I don’t think I am a world-class expert in anything”, and many more kept stopping me from setting up my own blog. Now seemed like a good time as any so with those and tons of other questions in my mind I decided it was time to start.Funnily, this is not my first post. The birth of the blog came very natural as a way to “document” my newly established pursuit for getting myself into Machine Learning. This new adventure of mine comprises several things, and if I want to succeed I need to be serious about them all: I want to start coding again! I used to code a long time ago, starting when I was 8 years old in a Tandy Color Computer hooked up to my parent’s TV. Machine Learning is a vast, wide subject, I want to learn the generals, but also to select a few areas to focus on. Setting up a blog to document my journey and share it: Establish a learning and blogging routine. If I don’t do this, I am sure this endeavour will die off soon.As for the focus areas I will start with: Neural Networks fundamentals: history, basic architecture and math behind them Deep Neural Networks Reinforcement Learning Current state of the art: what is at the cutting edge now in terms of Deep Neural Networks and Reinforcement Learning?I selected the above areas to focus on based on my personal interests, I have been fascinated by the developments in reinforcement learning for a long time, in particular Deep Mind’s awesome Go, Chess and Starcraft playing agents. Therefore, I started reading a lot about it and even started a personal project for coding a tic-tac-toe learning agent.With my limited knowledge I have drafted the following learning path: Youtube: Three Blue One Brown’s videos on Neural Networks, Calculus and Linear Algebra. I cannot recommend them enough, they are of sufficient depth and use animation superbly to facilitate the understanding of the subjects. Coursera: Andrew Ng’s Machine Learning course Book: Deep Learning with Python by Francois Chollet Book: Reinforcement Learning: An Introduction, by Richard S. Sutton and Andrew G. BartoAs for practical work I decided to start by coding my first models from scratch (without using libraries such as Tensorflow), to be able to deeply understand the math and logic behind the models, so far it has proven to be priceless.For my next project I think I will start to do the basic hand-written digits recognition, which is the Machine Learning Hello World, for this I think I will start to use Tensorflow already.I will continue to write about my learning road, what I find interesting and relevant, and to document all my practical exercises, as well as news and the state of the art in the world of AI.So far, all I have learned has been so engaging that I am seriously thinking of a career change. I have 17 years of international experience in multinational corporations across various functions, such as Information Services, Sales, Customer Care and New Products Introduction, and sincerely, I am finding more joy in artificial intelligence than anything else I have worked on before. Let’s see where the winds take us.Thanks for reading!P.S. For the geeks like me, here is a snippet on the technical side of the blog.Static Website GeneratorI researched a lot on this, when I started I didn’t even know I needed a static website generator. I was just sure of one thing, I wanted my blog site to look modern, be easy to update and not to have anything extra or additional content or functionality I did not need.There is a myriad of website generators nowadays, after a lengthy search the ones I ended up considering are: wordpress wix squarespace ghost webflow netlify hugo gatsby jekyllI started with the web interfaced generators with included hosting in their offerings:wordpress is the old standard, it is the one CMS I knew from before, and I thought I needed a fully fledged CMS, so I blindly ran towards it. Turns out, it has grown a lot since I remembered, it is now a fully fledged platform for complex websites and ecommerce development, even so I decided to give it a try, I picked a template and created a site. Even with the most simplistic and basic template I could find, there is a lot going on in the site. Setting it up was not as difficult or cumbersome as others claim, it took me about one hour to have it up and running, it looks good, but a bit crowded for my personal taste, and I found out it serves ads in your site for the readers, that is a big no for me.I have tried wix and squarespace before, they are fantastic for quick and easy website generation, but their free offering has ads, so again, a big no for me.I discovered ghost as the platform used by one of the bloggers I follow (Sebastian Ruder), turns out is a fantastic evolution over wordpress. It runs on the latest technologies, its interface is quite modern, and it is focused on one thing only: publishing. They have a paid hosting service, but the software is open sourced, therefore free to use in any hosting.I also tested webflow and even created a mockup there, the learning curve was quite smooth, and its CMS seems quite robust, but a bit too much for the functionalities I required.Next were the generators that don’t have a web interface, but can be easily set up:The first I tried was netlify, I also set up a test site in it. Netlify provides free hosting, and to keep your source files it uses GitHub (a repository keeps the source files where it publishes from). It has its own CMS, Netlify CMS, and you have a choice of site generators: Hugo, Gatsby, MiddleMan, Preact CLI, Next.js, Elevently and Nuxt.js, and once you choose there are some templates for each. I did not find the variety of templates enticing enough, and the set up process was much more cumbersome than with wordpress (at least for my knowledge level). I choose Hugo for my test site.I also tested gatsby with it’s own Gatsby Cloud hosting service, here is my test site. They also use GitHub as a base to host the source files to build the website, so you create a repository, and it is connected to it. I found the free template offerings quite limited for what I was looking for.Finally it came the turn for jekyll, although an older, and slower generator (compared to Hugo and Gatsby), it was created by one of the founders of GitHub, so it’s integration with GitHub Pages is quite natural and painless, so much so, that to use them together you don’t even have to install Jekyll in your machine! You have two choices: keep it all online, by having one repository in Github keep all the source files, modify or add them online, and having Jekyll build and publish your site to the special gh-pages repository everytime you change or add a new file to the source repository. Have a synchronized local copy of the source files for the website, this way you can edit your blog and customize it in your choice of IDE (Integrated Development Environment). Then, when you update any file on your computer, you just “push” the changes to GitHub, and GitHub Pages automatically uses Jekyll to build and publish your site.I chose the second option, specially because I can manipulate files, like images, in my laptop, and everytime I sync my local repository with GitHub, they are updated and published automatically. Quite convenient.After testing with several templates to get the feel for it, I decided to keep Jekyll for my blog for several reasons: the convenience of not having to install anything extra on my computer to build my blog, the integration with GitHub Pages, the ease of use, the future proofing via integration with modern technologies such as react or vue and the vast online community that has produced tons of templates and useful information for issue resolution, customization and added functionality.I picked up a template, just forked the repository and started modifying the files to customize it, it was fast and easy, I even took it upon myself to add some functionality to the template (it served as a coding little project) like: SEO meta tags Dark mode (configurable in _config.yml file) automatic sitemap.xml automatic archive page with infinite scrolling capability new page of posts filtered by a single tag (without needing autopages from paginator V2), also with infinite scrolling click to tweet functionality (just add a &lt;tweet&gt; &lt;/tweet&gt; tag in your markdown. custom and responsive 404 page responsive and automatic Table of Contents (optional per post) read time per post automatically calculated responsive post tags and social share icons (sticky or inline) included linkedin, reddit and bandcamp icons copy link to clipboard sharing option (and icon) view on github link button (optional per post) MathJax support (optional per post) tag cloud in the home page ‘back to top’ button comments ‘courtain’ to mask the disqus interface until the user clicks on it (configurable in _config.yml) CSS variables to make it easy to customize all colors and fonts added several pygments themes for code syntax highlight configurable from the _config.yml file. See the highlighter directory for reference on the options. responsive footer menu and footer logo (if setup in the config file) smoother menu animationsAs a summary, Hugo and Gatsby might be much faster than Jekyll to build the sites, but their complexity I think makes them useful for a big site with plenty of posts. For a small site like mine, Jekyll provides sufficient functionality and power without the hassle.You can use the modified template yourself by forking my repository. Let me know in the comments or feel free to contact me if you are interested in a detailed walkthrough on how to set it all up.HostingSince I decided on Jekyll to generate my site, the choice for hosting was quite obvious, Github Pages is very nicely integrated with it, it is free, and it has no ads! Plus the domain name isn’t too terrible (the-mvm.github.io).Interplanetary File SystemTo contribute to and test IPFS I also set up a mirror in IPFS by using fleek.co. I must confess that it was more troublesome than I imagined, it was definetively not plug and play because of the paths used to fetch resources. The nature of IPFS makes short absolute paths for website resources (like images, css and javascript files) inoperative; the easiest fix for this is to use relative paths, however the same relative path that works for the root directory (i.e. /index.html) does not work for links inside directories (i.e. /tags/), and since the site is static, while generating it, one must make the distinction between the different directory levels for the page to be rendered correctly.At first I tried a simple (but brute force solution):# determine the level of the current file{% assign lvl = page.url | append:'X' | split:'/' | size %}# create the relative base (i.e. \"../\"){% capture relativebase %}{% for i in (3..lvl) %}../{% endfor %}{% endcapture %}{% if relativebase == '' %} {% assign relativebase = './' %}{% endif %}...# Eliminate unecesary double backslashes{% capture post_url %}{{ relativebase }}{{ post.url }}{% endcapture %}{% assign post_url = post_url | replace: \"//\", \"/\" %}This jekyll/liquid code was executed in every page (or include) that needed to reference a resource hosted in the same server.But this fix did not work for the search function, because it relies on a search.json file (also generated programmatically to be served as a static file), therefore when generating this file one either use the relative path for the root directory or for a nested directory, thus the search results will only link correctly the corresponding pages if the page where the user searched for something is in the corresponding scope.So the final solution was to make the whole site flat, meaning to live in a single directory. All pages and posts will live under the root directory, and by doing so, I can control how to address the relative paths for resources."
    } 
  
]
