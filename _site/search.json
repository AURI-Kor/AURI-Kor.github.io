[
  
    {
      "title"       : "Machine Learning - Perceptron",
      "category"    : "",
      "tags"        : "machine learning, study_model, perceptron",
      "url"         : "./Machine-Learning-Perceptron.html",
      "date"        : "2023-01-11 00:00:00 +0900",
      "description" : "ML summary - perceptron",
      "content"     : "Perceptron Perceptron&#182;&#54140;&#49481;&#53944;&#47200;&#51060;&#46976;?&#182;퍼셉트론은 신경망의 가장 기초가 되는 기본 단위입니다. 생물학적 뉴런의 작업을 시뮬레이션하는 인공 뉴런이라고 할 수 있습니다.퍼셉트론은 3가지 구성요소로 이루어져 있습니다.입력(Inputs)가중치(Weights)출력(Output)Perceptron Class&#182;Perceptron 클래스 작성하기In&nbsp;[&nbsp;]: lines = []class Perceptron: def __init__(self, num_inputs = 3, weights = [1,1,1]): self.num_inputs = num_inputs self.weights = weights def weighted_sum(self, inputs): weighted_sum = 0 for i in range(self.num_inputs): weighted_sum += inputs[i] * self.weights[i] return weighted_sum def activation(self, weighted_sum): if weighted_sum &gt;= 0: return 1 else: return -1 def training(self, training_set): foundLine = False while not foundLine: total_error = 0 for inputs in training_set: prediction = self.activation(self.weighted_sum(inputs)) actual = training_set[inputs] error = actual - prediction total_error += abs(error) for i in range(self.num_inputs): self.weights[i] += error * inputs[i] slope = -self.weights[0] / self.weights[1] intercept = -self.weights[2] / self.weights[1] y1 = (slope * 0) + intercept y2 = (slope * 50) + intercept lines.append([[0, 50], [y1, y2]]) if total_error == 0: foundLine = True &#44032;&#51473;&#54633;(Weighted Sum) &#44396;&#54616;&#44592;&#182;입력과 가중치를 출력으로 변환하기 위해서는, 우선 각 입력에 대한 가중치를 더해서 가중합을 계산해야합니다.weighted sum = x_1 * w_1 + x_2 * w_2 + ... + x_n * w_n위 수식을 구현해봅니다.&#54876;&#49457;&#54868; &#54632;&#49688;(Activation Function)&#182;퍼셉트론의 출력은 앞에서 계산한 가중합으로부터 결과값을 생성함으로써 결정되게 됩니다. 결과값을 생성해주는 함수를 활성화 함수라고 합니다.다양한 활성화 함수들이 존재하는데, 이번 실습에서는 Step Activation Function을 사용합니다.입력에 대한 퍼셉트론의 가중합이 양수라면 1을 반환합니다.입력에 대한 퍼셉트론의 가중합이 음수라면 -1을 반환합니다.활성화 함수를 perceptron class에 구현해봅니다.&#50724;&#52264;(Error) &#44396;&#54616;&#44592;&#182;퍼셉트론의 출력값과 학습 데이터의 타겟값이 일치하지 않을 때, 오차를 구할 수 있습니다. 오차는 해당 모델과 실제 데이터간의 차이를 나타냅니다.우리의 목표는 퍼셉트론을 오차가 작아지는 방향으로 학습시키는 것입니다. 훈련 오차는 실제 레이블 값에서 예측된 레이블 값을 빼서 계산됩니다.training error = actual label - predicted labelStep activation function을 사용하는 퍼셉트론이 만드는 오차는 다음의 네 가지 경우를 띕니다.&#44032;&#51473;&#52824; &#49688;&#51221;&#54616;&#44592;&#182;퍼셉트론 학습의 목표는 가중치를 올바른 방향으로 수정해서, 정답을 출력하는 최적의 가중치를 찾는 것입니다.최적의 가중치를 찾는 방법 중, 가장 널리 사용되고 있는 것이 바로 경사하강법(Gradient Descent)입니다. 이번에 우리는 Step Activation Function을 사용하기 때문에, 경사하강법 중 Delta Rule을 통해 가중치를 학습시킬 것입니다.Delta rule은 다음의 간단한 규칙을 통해 가중치를 수정하게 됩니다.weight = weight + learning rate * (error * input)퍼셉트론은 모든 결과값을 정확하게 예측할 때까지 가중치를 계속 조정합니다. 그렇기 때문에 학습이 완료되기 전에 데이터셋을 여러번 반복하여 학습시켜야 할 수도 있습니다. 위 perceptron class에 학습시키는 코드를 수정해봅니다.&#54200;&#54693;&#52824;(Bias)&#182;그러나 퍼셉트론의 정확도를 높이기 위해 약간의 조정이 필요한 경우가 있습니다. 이를 위해 우리는 퍼셉트론에 편향치를 추가합니다.편향이 추가된 가중합의 식은 다음과 같습니다.weighted sum = x_1 * w_1 + x_2 * w_2 + ... + x_n * w_n​ + w_b다음 두 가지 변경사항만 고려하면 앞서 구현한 퍼셉트론에 편향을 적용할 수 있을 것입니다.입력 데이터의 마지막에 1을 추가합니다 (이제 2 대신 3 개의 입력이 있음).가중치 목록에 편향 가중치를 추가합니다 (이제 2 개 대신 3 개의 가중치가 있음).위 perceptron class에 해당 코드를 추가해봅니다.&#54140;&#49481;&#53944;&#47200;&#51012; &#53685;&#54620; &#49440;&#54805; &#48516;&#47448;&#182;퍼셉트론의 훈련 과정을 더 잘 이해하기 위해 퍼셉트론의 훈련 과정을 시각화 해봅니다.가중치는 훈련 과정 전반에 걸쳐 변경되므로 해당 가중치를 의미있게 시각화 할 수만 있다면 기울기-절편 형식을 사용하여 선을 나타낼 수 있다는 것을 알고있을 것입니다. 퍼셉트론의 가중치는 퍼셉트론이 나타내는 선의 기울기와 절편을 찾는 데 사용할 수 있습니다.기울기 = -self.weights[0] /self.weights[1]절편 = -self.weights[2] /self.weights[1]위 perceptron class에 해당 코드를 추가해봅니다.&#54140;&#49481;&#53944;&#47200; &#54617;&#49845;&#49884;&#53412;&#44592;&#182;실제 대부분의 기계학습에서 사용하는 데이터는 tabular 형태를 띄고 있지만, 이번 실습에서는 입력값과 출력값을 dictionary 형태로 표현한 데이터셋을 사용하도록 하겠습니다. 우리가 사용할 데이터셋은 다음과 같은 형태를 지닙니다.training_set = {(18, 49): -1, (2, 17): 1, (24, 35): -1, (14, 26): 1, (17, 34): -1}In&nbsp;[&nbsp;]: import randomdef generate_training_set(num_points): x_coordinates = [random.randint(0, 50) for i in range(num_points)] y_coordinates = [random.randint(0, 50) for i in range(num_points)] training_set = dict() for x, y in zip(x_coordinates, y_coordinates): if x &lt;= 45-y: training_set[(x,y,1)] = 1 elif x &gt; 45-y: training_set[(x,y,1)] = -1 return training_set In&nbsp;[&nbsp;]: #training 데이터셋을 generate_training_set 함수를 사용하여 30개의 데이터를 생성합니다.training_set = generate_training_set(30) In&nbsp;[&nbsp;]: import seabornimport matplotlib.pyplot as pltx_plus = []y_plus = []x_minus = []y_minus = []for data in training_set: if training_set[data] == 1: x_plus.append(data[0]) y_plus.append(data[1]) elif training_set[data] == -1: x_minus.append(data[0]) y_minus.append(data[1]) fig = plt.figure()ax = plt.axes(xlim=(-25, 75), ylim=(-25, 75))plt.scatter(x_plus, y_plus, marker = &#39;+&#39;, c = &#39;green&#39;, s = 128, linewidth = 2)plt.scatter(x_minus, y_minus, marker = &#39;_&#39;, c = &#39;red&#39;, s = 128, linewidth = 2)plt.title(&quot;Training Set&quot;)plt.show() In&nbsp;[&nbsp;]: # 퍼셉트론을 만들고 데이터에 훈련시켜 봅니다.perceptron = Perceptron()perceptron.training(training_set) 아래 코드는 구현한 퍼셉트론이 가중치를 변화시켜가며 최적의 선을 그리는것을 시각화 한 것입니다.In&nbsp;[&nbsp;]: import seabornimport matplotlib.pyplot as pltimport matplotlib.animation as animationfrom matplotlib import rcfrom IPython.display import HTMLimport randomx_plus = []y_plus = []x_minus = []y_minus = []for data in training_set: if training_set[data] == 1: x_plus.append(data[0]) y_plus.append(data[1]) elif training_set[data] == -1: x_minus.append(data[0]) y_minus.append(data[1])fig = plt.figure()ax = plt.axes(xlim=(-25, 75), ylim=(-25, 75))line, = ax.plot([], [], lw=2)fig.patch.set_facecolor(&#39;#ffc107&#39;)plt.scatter(x_plus, y_plus, marker = &#39;+&#39;, c = &#39;green&#39;, s = 128, linewidth = 2)plt.scatter(x_minus, y_minus, marker = &#39;_&#39;, c = &#39;red&#39;, s = 128, linewidth = 2)plt.title(&#39;Iteration: 0&#39;)plt.close()def animate(i): line.set_xdata(lines[i][0]) # update the data line.set_ydata(lines[i][1]) # update the data return line,def init(): line.set_data([], []) return line,ani = animation.FuncAnimation(fig, animate, frames=len(lines), init_func=init, interval=100, blit=True, repeat=False)rc(&#39;animation&#39;, html=&#39;jshtml&#39;)ani Out[&nbsp;]: &lt;/input&gt; Once Loop Reflect In&nbsp;[&nbsp;]: def plot_data(x, y): plt.scatter([point[0] for point in x], [point[1] for point in x], c=y, cmap=ListedColormap([&#39;#FF0000&#39;, &#39;#0000FF&#39;])) plt.show() In&nbsp;[&nbsp;]: from matplotlib.colors import ListedColormap# Perceptron의 decision boundary를 그려주는 함수입니다.# 함수 출처: http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.htmldef plot_decision_boundary(classifier, X, y, title): xmin, xmax = np.min(X[:, 0]) - 0.05, np.max(X[:, 0]) + 0.05 ymin, ymax = np.min(X[:, 1]) - 0.05, np.max(X[:, 1]) + 0.05 step = 0.01 cm = plt.cm.coolwarm_r thr = 0.0 xx, yy = np.meshgrid(np.arange(xmin - thr, xmax + thr, step), np.arange(ymin - thr, ymax + thr, step)) if hasattr(classifier, &#39;decision_function&#39;): Z = classifier.decision_function(np.hstack((xx.ravel()[:, np.newaxis], yy.ravel()[:, np.newaxis]))) else: Z = classifier.predict_proba(np.hstack((xx.ravel()[:, np.newaxis], yy.ravel()[:, np.newaxis])))[:, 1] Z = Z.reshape(xx.shape) plt.contourf(xx, yy, Z, cmap=cm, alpha=0.8) plt.colorbar() plt.scatter(X[:, 0], X[:, 1], c=y, cmap=ListedColormap([&#39;#FF0000&#39;, &#39;#0000FF&#39;]), alpha=0.6) plt.xlim(xmin, xmax) plt.ylim(ymin, ymax) plt.xticks((0.0, 1.0)) plt.yticks((0.0, 1.0)) plt.title(title) AND 데이터에 대하여 아래와 같은 단계로 Perceptron을 훈련시키고 데이터를 분류하였습니다.데이터를 생성합니다. (and_data, and_labels)plot_data 함수를 사용하여 데이터 시각화해봅니다.scikit-learn의 Perceptron 모델을 생성하고 (모델의 이름은 perceptron_and) 데이터에 .fit()해봅니다. 다큐멘테이션.score()로 데이터에 대해 정확도를 출력합니다.plot_decision_boundary() 함수를 사용하여 훈련된 perceptron의 decision boundary를 시각화합니다.In&nbsp;[&nbsp;]: import numpy as np In&nbsp;[&nbsp;]: and_data = np.array([[0, 0], [1, 0], [0, 1], [1, 1]])and_labels = np.array([0, 0, 0, 1]) In&nbsp;[&nbsp;]: plot_data(and_data, and_labels) In&nbsp;[&nbsp;]: from sklearn.linear_model import Perceptronperceptron_and = Perceptron()perceptron_and.fit(and_data, and_labels) Out[&nbsp;]:Perceptron()In&nbsp;[&nbsp;]: perceptron_and.score(and_data, and_labels) Out[&nbsp;]:1.0In&nbsp;[&nbsp;]: plot_decision_boundary(perceptron_and, and_data, and_labels, &quot;PERCEPTRON AND&quot;) AND data에 대해 perceptron의 decision boundary는 데이터를 구분하고 있으므로 올바르게 분류되었다.OR 데이터에 대하여 아래 다섯 단계를 수행합니다.OR 데이터를 생성합니다. (or_data, or_labels) plot_data 함수를 사용하여 데이터 시각화해봅니다.scikit-learn의 Perceptron 모델을 생성하고 (모델의 이름은 perceptron_or) 데이터에 .fit()해봅니다. 다큐멘테이션.score()로 데이터에 대해 정확도를 출력합니다.plot_decision_boundary() 함수를 사용하여 훈련된 perceptron의 decision boundary를 시각화합니다.In&nbsp;[&nbsp;]: or_data = np.array([[0, 0], [1, 0], [0, 1], [1, 1]])or_labels = np.array([0, 1, 1, 1]) In&nbsp;[&nbsp;]: plot_data(or_data, or_labels) In&nbsp;[&nbsp;]: perceptron_or = Perceptron()perceptron_or.fit(or_data, or_labels) Out[&nbsp;]:Perceptron()In&nbsp;[&nbsp;]: perceptron_or.score(or_data, or_labels) Out[&nbsp;]:1.0In&nbsp;[&nbsp;]: plot_decision_boundary(perceptron_or, or_data, or_labels, &quot;PERCEPTRON OR&quot;) OR data에 대해 perceptron의 decision boundary는 데이터를 구분하고 있으므로 올바르게 분류되었다.XOR 데이터에 대하여 아래 다섯 단계를 수행합니다.데이터를 생성합니다. (xor_data, xor_labels)plot_data 함수를 사용하여 데이터 시각화해봅니다.scikit-learn의 Perceptron 모델을 생성하고 (모델의 이름은 perceptron_xor) 데이터에 .fit()해봅니다. 다큐멘테이션.score()로 데이터에 대해 정확도를 출력합니다.plot_decision_boundary() 함수를 사용하여 훈련된 perceptron의 decision boundary를 시각화합니다.In&nbsp;[&nbsp;]: xor_data = np.array([[0,0],[1,0],[0,1],[1,1]])xor_labels = np.array([0,1,1,0]) In&nbsp;[&nbsp;]: plot_data(xor_data, xor_labels) In&nbsp;[&nbsp;]: plot_data(xor_data, xor_labels) In&nbsp;[&nbsp;]: perceptron_xor = Perceptron()perceptron_xor.fit(xor_data, xor_labels) Out[&nbsp;]:Perceptron()In&nbsp;[&nbsp;]: perceptron_xor.score(xor_data, xor_labels) Out[&nbsp;]:0.5In&nbsp;[&nbsp;]: plot_decision_boundary(perceptron_xor, xor_data, xor_labels, &quot;PERCEPTRON XOR&quot;) 클래스가 다른 데이터가 같은 boundary로 분류되고 있으므로 제대로 분류되지 않았다.XOR 데이터에 대해 scikit-learn의 MLPClassifier 모델을 여러개 훈련시키고 (모델의 이름은 mlp_xor_n, 여기서 n은 숫자입니다) 그 차이점을 분석해봅니다. MLPClassifier 다큐멘테이션MLPClassifier의 다큐멘테이션을 읽고 아래 질문에 대한 답변을 서술합니다.1-1. MLPClassifier 모델의 기본 hidden layer size는 무엇입니까?1-2. 기본 learning rate은 무엇입니까?1-3. 기본 maximum iteration 횟수는 무엇입니까?1-4. 해당 다큐멘테이션에서 random_state 파라미터가 존재하는 이유는 무엇입니까?모델을 여러개 만들어 hidden_layer_sizes 파라미터를 다양하게 조정한 모델을 여러개 만들어 XOR 데이터에 .fit()해봅니다. (random_state=42 필수 지정)2-1. hidden_layer_sizes 파라미터를 조정하지 않은 모델의 정확도, decision boundary을 시각화합니다.2-2. hidden_layer_sizes 파라미터의 hidden layer 개수와 neuron의 개수를 적절히 많게 설정하고 그 모델의 정확도, decision boundary을 시각화합니다.2-3. hidden_layer_sizes 파라미터의 hidden layer 개수와 neuron의 개수를 적절히 적게 설정하고 그 모델의 정확도, decision boundary을 시각화합니다.2-4. 과제 1-3의 결과와 어떤 차이점이 있는지 서술합니다.2-5. 2-1, 2-2, 2-3에서 나온 decision boundary의 차이를 hidden layer의 dimension과 연관지어 비교해보고, 차이가 나는 이유를 서술해봅니다.1-1.MLPClassifier 모델의 기본 hidden layer size는 100이다.1-2. MLPClassifier 모델의 기본 learning rate은 CONSTANT learning rate이다.1-3. MLPClassifier 모델의 기본 maximum iteration 횟수는 200이다1-4. random_state 파라미터는 데이터를 추출할 때 같은 데이터가 추출될 수 있게 해준다. 이는 여러 번의 수행시 마다 동일한 결과를 얻기 위해 존재한다.In&nbsp;[&nbsp;]: from sklearn.neural_network import MLPClassifier In&nbsp;[&nbsp;]: # default값으로 훈련한 xor MLPmlp_xor_1 = MLPClassifier()mlp_xor_1.fit(xor_data, xor_labels)print(mlp_xor_1.score(xor_data, xor_labels)) 1.0 /usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet. ConvergenceWarning,In&nbsp;[&nbsp;]: plot_decision_boundary(mlp_xor_1, xor_data, xor_labels, &quot;MLP_HIDDEN_LAYER_SIZES=100&quot;) In&nbsp;[&nbsp;]: # hidden_layer_sizes를 많게 설정한 모델mlp_xor_2 = MLPClassifier(hidden_layer_sizes=400)mlp_xor_2.fit(xor_data, xor_labels)print(mlp_xor_2.score(xor_data, xor_labels)) 1.0 /usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet. ConvergenceWarning,In&nbsp;[&nbsp;]: plot_decision_boundary(mlp_xor_2, xor_data, xor_labels, &quot;MLP_HIDDEN_LAYER_SIZES=200&quot;) In&nbsp;[&nbsp;]: # hidden_layer_sizes를 적게 설정한 모델mlp_xor_3 = MLPClassifier(hidden_layer_sizes=50)mlp_xor_3.fit(xor_data, xor_labels)print(mlp_xor_3.score(xor_data, xor_labels)) 1.0 /usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet. ConvergenceWarning,In&nbsp;[&nbsp;]: plot_decision_boundary(mlp_xor_3, xor_data, xor_labels, &quot;MLP_HIDDEN_LAYER_SIZES=50&quot;) 1-3의 경우에는 decision boundary가 선형적으로 나타나는데 이 때문에 같은 클래스가 동일한 decision boundary내에 위치하게 된다.이를 통해 적절한 classification이 이루어지지 못했음을 알 수 있다. 하지만 MLP를 이용한 과제에서는 좀 더 복잡한 decision boundary를 통해 절절하게 class를 분류하는 모습을 보여준다.hidden layer는 MLP학습을 위해 거치는 노드들이 위치한 층으로 hidden layer의 수가 많아질 수록 학습에 사용되는 parameter(weight)의 값들이 많아지게 된다. 따라서 hidden layer층이 적은 MLP 모델에서는 hidden layer층이 많은 MLP모델과 비교하여 학습에 사용되는 가중치 값의 수가 적어지게 되고 decision boundary와 양 class를 구분하기 위한 확률(y=1) 값이 차이도 간격이 줄어들게 된다. 반면 hidden layer층이 많은 MLP모델의 경우에는 확률(y=1) 값의 간격이 더 크고 decision boundary 또한 구분이 더 명확해지기 때문에 더 잘 구분되는 모델이 만들어진다.titanic 데이터를 전처리합니다.사용 column: Pclass, Sex, Age, SibSp, Parch, Fare, Embarked, SurvivedNA가 있는 열 drop 하기Sex feature 정수형으로 인코딩Embarked feature one-hot으로 인코딩x 와 y 를 분리 (y은 Survived feature)데이터를 train와 test 데이터로 split (random_state=42)scikit-learn의 MLPClassifier을 사용하여 classification을 합니다. 다큐멘테이션어떠한 모델 파라미터도 조정하지 않은 baseline MLPClassifier 모델을 생성하고 훈련시켜 테스트 데이터에 대한 정확도를 출력합니다.두번째 MLPClassifier 모델을 생성합니다. 이번에는 다큐멘테이션을 읽고 모델 파라미터를 조정합니다. 조정하는 파라미터는 hidden_layer_sizes, activation, learning_rate_init, max_iter 입니다. 네가지 파라미터를 모두 조정하여도 되고 이 중 선택하여 조정하여도 됩니다. 이 모델이 baseline 모델보다 테스트 데이터에 대해 정확도가 높게 나오도록 파라미터를 조정해야 합니다. 두번째 모델에 대한 정확도 또한 출력합니다.모델 파라미터를 어떻게 조정하여 정확도를 높일 수 있었는지 서술합니다.In&nbsp;[&nbsp;]: import pandas as pddata_url = &quot;https://raw.githubusercontent.com/inikoreaackr/ml_datasets/main/titanic.csv&quot; In&nbsp;[&nbsp;]: df = pd.read_csv(data_url) In&nbsp;[&nbsp;]: import pandas as pddef onehot(data, feature): &#39;&#39;&#39; data의 feature column을 one hot으로 변환해줍니다. data: pandas DataFrame feature: string, 데이터 프레임의 column 이름 &#39;&#39;&#39; return pd.concat([data, pd.get_dummies(data[feature], prefix=feature)], axis=1).drop([feature], axis=1) In&nbsp;[&nbsp;]: data = df [[&#39;Pclass&#39;, &#39;Sex&#39;, &#39;Age&#39;, &#39;SibSp&#39;, &#39;Parch&#39;, &#39;Fare&#39;, &#39;Embarked&#39;, &#39;Survived&#39;]]data = data.dropna() In&nbsp;[&nbsp;]: from sklearn.preprocessing import LabelEncoderle = LabelEncoder()data.Sex = le.fit_transform(data.Sex)data = onehot(data, &#39;Embarked&#39;) In&nbsp;[&nbsp;]: y = data[[&#39;Survived&#39;]].to_numpy().ravel()x = data.drop(columns=[&#39;Survived&#39;])from sklearn.model_selection import train_test_splitx_train, x_test, y_train, y_test = train_test_split(x,y,test_size= .3, random_state=42) In&nbsp;[&nbsp;]: from sklearn.neural_network import MLPClassifier In&nbsp;[&nbsp;]: mlp = MLPClassifier()mlp.fit(x_train, y_train)print(mlp.score(x_test, y_test)) 0.7383177570093458 /usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet. ConvergenceWarning,In&nbsp;[&nbsp;]: mlp_parameter = MLPClassifier(activation = &#39;logistic&#39;, max_iter=400)mlp_parameter.fit(x_train, y_train)print(mlp_parameter.score(x_test, y_test)) 0.8084112149532711 /usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (400) reached and the optimization hasn&#39;t converged yet. ConvergenceWarning,activation의 default 값은 relu(the rectified linear unit function)이며 더 적합한 방법이 logistic이라고 판단하였다.max_iter의 default 값은 200인데 횟수가 최적의 weight 값을 찾기에 부족한 반복횟수라고 판단하여 400으로 늘려 충분한 학습이 일어날 수 있도록 했다."
    } ,
  
    {
      "title"       : "Machine Learning - Model Evaluation",
      "category"    : "",
      "tags"        : "machine learning, study_model, model evaluation",
      "url"         : "./Machine-Learning-Model-Evaluation.html",
      "date"        : "2023-01-11 00:00:00 +0900",
      "description" : "Machine Learning - Model Evaluation",
      "content"     : "&lt;!DOCTYPE html&gt;ModelEvaluation Model Evaluation (&#47784;&#45944; &#49457;&#45733; &#54217;&#44032;)&#182;1. Accuracy(&#51221;&#54869;&#46020;)&#182;Confusion matrix:&lt;img src=https://miro.medium.com/max/712/1*Z54JgbS4DUwWSknhDCvNTQ.png width=300px&gt;True Positive(TP): 참양성False Positive(FP): 위양성True Negative(TN): 참음성False Negative(FN): 위음성분류 모델을 만든 후 다음 단계는 모델의 예측 능력을 평가해야합니다.모델의 성능을 평가하는 가장 간단한 방법은 모델의 정확도를 계산하는 것입니다.$$\\frac{\\text{True Positives + True Negatives}}{\\text{True Positives + True Negatives + False Positives + False Negatives}}$$In&nbsp;[&nbsp;]: labels_A = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1]guesses_A =[0, 0, 0, 0, 0, 0, 0, 1, 0, 0]labels_B = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1]guesses_B =[1, 1, 1, 1, 1, 1, 1, 0, 1, 1] In&nbsp;[&nbsp;]: # 위의 예제로 TP, FP, TN, FN을 직접 계산해보는 함수입니다def cal_metrics(labels, guesses): true_positives = 0 true_negatives = 0 false_positives = 0 false_negatives = 0 for i in range(len(guesses)): #True Positives if labels[i] == 1 and guesses[i] == 1: true_positives += 1 #True Negatives if labels[i] == 0 and guesses[i] == 0: true_negatives += 1 #False Positives if labels[i] == 0 and guesses[i] == 1: false_positives += 1 #False Negatives if labels[i] == 1 and guesses[i] == 0: false_negatives += 1 return true_positives, true_negatives, false_positives, false_negatives In&nbsp;[&nbsp;]: # A에 대하여 accuracy를 계산해봅니다.TP_A, TN_A, FP_A, FN_A = cal_metrics(labels_A, guesses_A)accuracy_A = (TP_A + TN_A) / len(guesses_A)print(accuracy_A) 0.8In&nbsp;[&nbsp;]: # B에 대하여 accuracy를 계산해봅니다.TP_B, TN_B, FP_B, FN_B = cal_metrics(labels_B, guesses_B)accuracy_B = (TP_B + TN_B) / len(guesses_B)print(accuracy_B) 0.22. Precision(&#51221;&#48128;&#46020;)&#182;양성으로 식별된 사례 중 실제로 양성이었던 사례의 비율Precision은 양성으로 예측된 것 (TP + FP) 중 얼마나 많은 샘플이 진짜 양성 (TP) 인지 측정합니다.직관적으로 Precision은 negative sample을 positive sample로 예측하지 않는 분류기의 성능을 나타냅니다.Precision은 거짓 양성(FP)의 수를 줄이는 것이 목표일 때 성능 지표로 사용합니다.위양성이 생성되지 않는 모델의 정밀도는 1.0입니다.예시) 모델의 정밀도는 0.5일때, 어떠한 종양이 악성일 가능성이 있다고 예측하면 예측 정확도가 50% 입니다.$$\\frac{\\text{True Positives}}{\\text{True Positives + False Positives}}$$In&nbsp;[&nbsp;]: # precision_A을 계산해봅니다.TP_A / (TP_A + FP_A) Out[&nbsp;]:1.0In&nbsp;[&nbsp;]: # precision_B을 계산해봅니다.TP_B / (TP_B + FP_B) Out[&nbsp;]:0.22222222222222223. Recall(&#51116;&#54788;&#50984;)&#182;실제 양성 중 정확히 양성이라고 식별된 사례의 비율Recall은 전체 양성 샘플 (TP + FN) 중에서 얼마나 많은 샘플이 양성 클래스(TP)로 분류되는지를 측정합니다.직관적으로, Recall은 분류기가 positive sample들을 올바르게 찾는 성능을 평가합니다.Recall은 모든 양성 샘플을 식별해야 할 때 성능 지표로 사용됩니다. 즉, 거짓 음성(FN)을 피하는 것이 중요할 때 입니다.위음성을 생성하지 않는 모델의 재현율은 1.0입니다.예시) 모델의 재현율은 0.11일때, 모든 악성 종양의 11% 를 올바르게 식별합니다..$$\\frac{\\text{True Positives}}{\\text{True Positives + False Negatives}}$$Precision과 Recall은 서로 상충하는 metric입니다. 하나가 내려가면 다른 하나가 올라갑니다.In&nbsp;[&nbsp;]: # Recall_A을 계산해봅니다.recall_A = TP_A / (TP_A + FN_A)print(recall_A) 0.3333333333333333In&nbsp;[&nbsp;]: # Recall_B을 계산해봅니다.recall_B = TP_B / (TP_B + FN_B)print(recall_B) 0.6666666666666666Side Note:&#182;&lt;img src=https://miro.medium.com/max/712/1*Z54JgbS4DUwWSknhDCvNTQ.png width=300px&gt;Precision(정밀도): 양성으로 식별된 사례 중 실제로 양성이었던 사례의 비율$$\\text{Precision} = \\frac{TP}{TP + FP}$$Recall(재현율): 실제 양성 중 정확히 양성이라고 식별된 사례의 비율$$\\text{Recall} = \\frac{TP}{TP + FN}$$Sensitivity(민감도) = recall: 실제로 양성인 사람이 검사에서 양성으로 판정될 확률$$\\text{Sensitivity} = \\frac{TP}{TP + FN}$$Specificity(특이도): 실제로 음성인 사람이 검사에서 음성으로 판정될 확률$$\\text{Specificity} = \\frac{TN}{FP + TN}$$4. F1 Score&#182;모델의 성능을 완전히 평가하려면 정밀도와 재현율을 모두 검사해야 합니다. F1 score을 통해 Precision과 Recall 모두를 고려하는 하나의 metric을 제시할 수 있습니다. F1 score은 Precision과 Recall의 조화평균(harmonic mean)입니다. 조화평균은 n개의 양수에 대하여 그 역수들을 산술평균한 것의 역수를 말합니다.$$2*\\frac{\\text{Precision} * \\text{Recall}}{\\text{Precision} + \\text{Recall}}$$F1 score는 Precision과 Recall을 하나의 metric으로 결합합니다. 산술 평균보다 조화 평균을 사용하여 F1 score을 계산하는 이유는 Precision이나 Recall이 0일 때 F1 score이 낮아지기를 원하기 때문입니다.예를 들어, Recall = 1이고 Precision = 0.01인 모델이 있다고 가정합니다. Precision이 너무 낮기 때문에 이 모델에 문제가 있을 가능성이 높습니다. 따라서 F1 score은 Precision과 Recall을 같이 고려하므로 불균형한 이진 분류 데이터셋에서는 정확도보다 더 나은 지표가 될 수 있습니다.산술평균(arithmetic mean)을 사용한 F1 score은 다음과 같습니다.$$\\frac{1 + 0.01}{2} = 0.505$$너무 높습니다. 따라서 조화 평균을 사용한 F1 score은 다음과 같습니다.$$2*\\frac{1 * 0.01}{1 + 0.01} = 0.019$$산술 평균을 사용한 F1 score은 분류기의 성능을 더욱 정확하게 파악하는데 도움이 됩니다.In&nbsp;[&nbsp;]: # F1 score을 계산해봅니다.precision_A = TP_A / (TP_A + FP_A)F1_A = 2 *(precision_A * recall_A) / (precision_A + recall_A)print(F1_A) 0.5In&nbsp;[&nbsp;]: # F1 score을 계산해봅니다.precision_B = TP_B / (TP_B + FP_B)F1_B = 2 *(precision_B * recall_B) / (precision_B + recall_B)print(F1_B) 0.33333333333333335. Scikit-Learn&#182;Scikit_learn 라이브러리에는 위의 metric들을 간단하게 계산해주는 함수들이 있습니다.In&nbsp;[&nbsp;]: from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_scorelabels = [1, 0, 0, 1, 1, 1, 0, 1, 1, 1]guesses = [0, 1, 1, 1, 1, 0, 1, 0, 1, 0] In&nbsp;[&nbsp;]: # 정확도, recall, precision, F1 score을 계산해봅니다.print(accuracy_score(labels, guesses))print(recall_score(labels, guesses))print(precision_score(labels, guesses))print(f1_score(labels, guesses)) 0.30.428571428571428550.50.46153846153846156. Binary Classification and Model Evaluation&#182;6-1. Dataset&#182;실습은 California Housing Dataset를 사용합니다. 이 데이터는 미국의 1990년 인구 조사 데이터를 기반으로 캘리포니아 특정 지역의 주택에 관한 것입니다.longitude: 집이 서쪽으로 얼마나 멀리 떨어져 있는지 나타내는 척도이며, 값이 더 높은 것은 서쪽으로 더 멀리 떨어져 있습니다.latitude: 집이 북쪽으로 얼마나 멀리 떨어져 있는지 나타내는 척도이며, 더 높은 값이 더 북쪽입니다.housing_median_age: 블록 내 주택의 연식입니다. 숫자가 낮으면 새 건물입니다.total_rooms: 블록 내의 방 개수입니다.total_bedrooms: 한 블록 내의 총 침실 개수입니다.population: 한 블록 내의 총 인구 수입니다.households: 한 블록에 대한 총 가구 수입니다.median_income: 주택 블록 내 가구의 중위소득입니다.medium_house_value: 블록 내 가구의 평균 주택 값(미국 달러로 측정)입니다.In&nbsp;[&nbsp;]: import pandas as pd train_df = pd.read_csv(&quot;https://download.mlcc.google.com/mledu-datasets/california_housing_train.csv&quot;)test_df = pd.read_csv(&quot;https://download.mlcc.google.com/mledu-datasets/california_housing_test.csv&quot;) In&nbsp;[&nbsp;]: train_df.describe() Out[&nbsp;]: longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_value count 17000.000000 17000.000000 17000.000000 17000.000000 17000.000000 17000.000000 17000.000000 17000.000000 17000.000000 mean -119.562108 35.625225 28.589353 2643.664412 539.410824 1429.573941 501.221941 3.883578 207300.912353 std 2.005166 2.137340 12.586937 2179.947071 421.499452 1147.852959 384.520841 1.908157 115983.764387 min -124.350000 32.540000 1.000000 2.000000 1.000000 3.000000 1.000000 0.499900 14999.000000 25% -121.790000 33.930000 18.000000 1462.000000 297.000000 790.000000 282.000000 2.566375 119400.000000 50% -118.490000 34.250000 29.000000 2127.000000 434.000000 1167.000000 409.000000 3.544600 180400.000000 75% -118.000000 37.720000 37.000000 3151.250000 648.250000 1721.000000 605.250000 4.767000 265000.000000 max -114.310000 41.950000 52.000000 37937.000000 6445.000000 35682.000000 6082.000000 15.000100 500001.000000 &lt;svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\" width=\"24px\"&gt; &lt;/svg&gt; 6-2. Z-score Normalization&#182;여러 feature이 존재하는 데이터로 모델을 훈련시키는 경우 각 피쳐의 값은 거의 동일한 범위를 가져야합니다. 예를 들어 한 feature의 범위가 500 ~ 100,000이고 다른 feature의 범위가 2 ~ 12인 경우 모델을 훈련하기가 어렵거나 불가능합니다.이번 실습에서 다룰 정규화 방법은 각 raw value(레이블 포함)을 Z-score으로 변환하여 정규화하는 방식입니다. Z-score는 특정 raw value에 대한 평균으로부터의 표준 편차의 개수입니다. 예를 들어, 다음과 같은 특성을 가진 feature이 있다고 가정합니다.평균은 60 입니다.표준편차는 10 입니다.75인 raw value는 Z-score이 +1.5이 될 것입니다: Z-score = (75 - 60) / 10 = +1.538인 raw value는 Z-score이 -2.2가 될 것입니다: Z-score = (38 - 60) / 10 = -2.2Z-score 정규화 공식은 다음과 같습니다.$$\\frac{value - \\mu}{\\sigma}$$여기서 $\\mu$는 평균값이고 $\\sigma$는 표준 편차입니다. raw value가 feature의 모든 값의 평균과 정확히 같으면 0으로 정규화됩니다. 평균 아래에 있으면 음수가 되고 평균 위에 있으면 양수가 됩니다. Z-score의 크기는 표준 편차에 의해 결정됩니다. 정규화되지 않은 데이터에 큰 표준 편차가 있으면 정규화 값이 0에 가깝습니다.In&nbsp;[&nbsp;]: train_df.columns Out[&nbsp;]:Index([&#39;longitude&#39;, &#39;latitude&#39;, &#39;housing_median_age&#39;, &#39;total_rooms&#39;, &#39;total_bedrooms&#39;, &#39;population&#39;, &#39;households&#39;, &#39;median_income&#39;, &#39;median_house_value&#39;], dtype=&#39;object&#39;)In&nbsp;[&nbsp;]: import seaborn as snssns.scatterplot(x=train_df.total_rooms, y=train_df.housing_median_age) Out[&nbsp;]:&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f4a74795a90&gt; In&nbsp;[&nbsp;]: train_df_mean = train_df.mean()train_df_std = train_df.std()train_df_norm = (train_df - train_df_mean) / train_df_std In&nbsp;[&nbsp;]: # training set에 대하여 Z-Score 정규화를 합니다. In&nbsp;[&nbsp;]: # 정규화된 데이터를 확인합니다.train_df_norm.head() Out[&nbsp;]: longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_value 0 2.619288 -0.671500 -1.079639 1.361655 1.764152 -0.361173 -0.075996 -1.252506 -1.210522 1 2.539494 -0.573248 -0.761850 2.296540 3.230346 -0.261858 -0.099401 -1.081451 -1.096713 2 2.494610 -0.905436 -0.920744 -0.882436 -0.866931 -0.955326 -0.999223 -1.170071 -1.048430 3 2.489623 -0.928830 -1.159087 -0.524171 -0.480216 -0.796769 -0.715753 -0.362590 -1.154480 4 2.489623 -0.961581 -0.682402 -0.545731 -0.506313 -0.701809 -0.622130 -1.026424 -1.222593 &lt;svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\" width=\"24px\"&gt; &lt;/svg&gt; In&nbsp;[&nbsp;]: # Z-score 전후 데이터의 범위를 비교해봅니다.import seaborn as snsimport matplotlib.pyplot as pltsns.set(rc={&#39;figure.figsize&#39;:(14,5)})fig, ax = plt.subplots(1,2)sns.scatterplot(x=train_df.total_rooms, y=train_df.housing_median_age, ax=ax[0]).set(title=&#39;Before Normalization&#39;)sns.scatterplot(x=train_df_norm.total_rooms, y=train_df_norm.housing_median_age, ax=ax[1]).set(title=&#39;Z-score Normalization&#39;)fig.show() In&nbsp;[&nbsp;]: # 그래프 사이즈 재설정sns.set(rc={&#39;figure.figsize&#39;:(11.7,8.27)}) In&nbsp;[&nbsp;]: # test set에 대해서도 동일하게 Z-Score 정규화를 합니다.test_df_mean = test_df.mean()test_df_std = test_df.std()test_df_norm = (test_df - test_df_mean) / test_df_std 6-3. Create a binary label&#182;분류 문제에서 모든 예제의 레이블은 0 또는 1이어야 합니다. 하지만 California Housing Dataset의 label인 median_house_value는 0과 1이 아닌 80,100 또는 85,700과 같은 float 값을 포함하고 있는 반면, 정규화된 데이터의 median_house_values는 주로 -3과 +3 사이의 float 값을 포함하고 있습니다.Training set과 test set에 median_house_value_is_high의 새 column을 만듭니다. median_house_value가 특정 임계값(threshold)보다 높은 경우 median_house_value_is_high를 1로 설정합니다. 그렇지 않으면 median_house_value_is_high를 0으로 설정합니다.In&nbsp;[&nbsp;]: # median_house_value를 임계값에 따라 0과 1로 인코딩합니다.threshold_in_z = 1.0train_df_norm[&quot;median_house_value_is_high&quot;] = (train_df_norm[&quot;median_house_value&quot;] &gt; threshold_in_z).astype(float)test_df_norm[&quot;median_house_value_is_high&quot;] = (test_df_norm[&quot;median_house_value&quot;] &gt; threshold_in_z).astype(float) In&nbsp;[&nbsp;]: # 변환이 잘 되었는지 확인해봅니다.train_df_norm[&quot;median_house_value_is_high&quot;].unique() Out[&nbsp;]:array([0., 1.])In&nbsp;[&nbsp;]: # California housing dataset은 label이 불균형한 데이터입니다. 불균형한 정도를 확인해봅니다train_df_norm[&quot;median_house_value_is_high&quot;].value_counts() Out[&nbsp;]:0.0 142231.0 2777Name: median_house_value_is_high, dtype: int64In&nbsp;[&nbsp;]: train_df_norm[&quot;median_house_value_is_high&quot;].value_counts()/train_df_norm.shape[0] Out[&nbsp;]:0.0 0.8366471.0 0.163353Name: median_house_value_is_high, dtype: float64In&nbsp;[&nbsp;]: # train/test의 X와 y를 분리합니다.X_train = train_df_norm.drop(columns=[&quot;median_house_value_is_high&quot;, &quot;median_house_value&quot;])X_test = test_df_norm.drop(columns=[&quot;median_house_value_is_high&quot;, &quot;median_house_value&quot;])y_train = train_df_norm[&quot;median_house_value_is_high&quot;]y_test = test_df_norm[&quot;median_house_value_is_high&quot;] 6-4. Train a classifier&#182;In&nbsp;[&nbsp;]: # MLPClassifier을 생성하고 훈련시킵니다.from sklearn.neural_network import MLPClassifierclf = MLPClassifier(random_state=42, max_iter=1000, learning_rate_init=0.01)clf.fit(X_train, y_train) Out[&nbsp;]:MLPClassifier(learning_rate_init=0.01, max_iter=1000, random_state=42)In&nbsp;[&nbsp;]: # 모델 평가를 위해서 예측값과 예측된 확률값을 만듭니다.y_pred = clf.predict(X_test)y_prob = clf.predict_proba(X_test)[:,1] 6-5. Evaluate the model&#182;Confusion Matrix&#182;scikit-learn confusion_matrixIn&nbsp;[&nbsp;]: # confusion matrix를 출력from sklearn.metrics import confusion_matrixconfusion_matrix(y_test, y_pred) Out[&nbsp;]:array([[2416, 110], [ 149, 325]])scikit-learn ConfusionMatrixDisplay.from_predictionsIn&nbsp;[&nbsp;]: # confusion matrix를 시각화해봅니다.from sklearn.metrics import ConfusionMatrixDisplayConfusionMatrixDisplay.from_predictions(y_test, y_pred) Out[&nbsp;]:&lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7f4a7430e550&gt; scikit-learn ConfusionMatrixDisplay.from_estimatorIn&nbsp;[&nbsp;]: # confusion matrix를 시각화ConfusionMatrixDisplay.from_estimator(clf, X_test, y_test) Out[&nbsp;]:&lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7f4a741b6110&gt; Precision-Recall&#182;모델의 분류 작업을 결정하는 임계값을 바꾸는 것은 해당 분류기의 precision과 recall의 상충 관계를 조정하는 일 입니다.예를 들어 양성 샘플의 실수(FN)을 10%보다 작게 하여 90% 이상의 recall을 원할 수 있습니다. 이런 결정은 데이터와 애플리케이션에 따라 다르며 목표에 따라 달리 결정됩니다. 어떤 목표가 선택되면 (즉, 어떤 클래스에 대한 특정 recall 또는 precision의 값) 적절한 임계값을 지정할 수 있습니다. 다시 말해 90% recall과 같은 특정 목적을 충족하는 임계값을 설정하는 것은 언제든 가능합니다. 어려운 부분은 이 임계값을 유지하면서 적절한 precision을 내는 모델을 만드는 일 입니다.만약 모든 것을 양성이라고 분류하면 recall이 100이 되지만 이러한 모델은 쓸모가 없을 것 입니다.&lt;img src=https://developers.google.com/static/machine-learning/crash-course/images/PrecisionVsRecallRaiseThreshold.svg width=600px&gt;새로운 모델을 만들 때에는 임계값이 명확하지 않은 경우가 많습니다. 이런 경우에는 문제를 더 잘 이해하기 위해 모든 임계값을 조사해보거나, 한번에 precision과 recall의 모든 장단점을 살펴보는 것이 좋습니다. 이를 위해 precision-recall curve를 사용합니다.scikit-learn PrecisionRecallDisplayIn&nbsp;[&nbsp;]: from sklearn.metrics import PrecisionRecallDisplayPrecisionRecallDisplay.from_estimator(clf, X_test, y_test) Out[&nbsp;]:&lt;sklearn.metrics._plot.precision_recall_curve.PrecisionRecallDisplay at 0x7f4a740a5d10&gt; 곡선의 각 포인트는 가능한 임계값에 대하여 precision과 recall입니다. 곡선이 오른쪽 위로 갈 수록 더 좋은 분류기입니다. 오른쪽 위 지점은 한 임계값에서 precision과 recall이 모두 높은 곳 입니다. 곡선은 임계값이 매우 높아 전부 양성 클래스가 되는 왼쪽 위에서 시작합니다. 임계값이 작아지면서 곡선은 recall이 높아지는 쪽으로 이동하게 되지만 precision은 낮아집니다. precision이 높아져도 recall이 높게 유지될수록 더 좋은 모델입니다.ROC and AUC&#182;ROC Curve는 여러 임계값에서 분류기의 특성을 분석하는데 널리 사용하는 도구입니다. Precision-Recall curve와 비슷하게 ROC curve는 분류기의 모든 임계값을 고려하지만, 정밀도와 재현률 대신 True Positive Rate(TPR)에 대한 False Positive Rate(FPR)을 나타냅니다. True Positive Rate은 Recall의 다른 이름이여, False Positive Rate은 전체 음성 샘플 중에서 거짓 양성으로 잘못 분류한 비율입니다. ROC curve는 roc_curve 함수를 사용하여 만들 수 있습니다. ROC curve는 왼쪽 위에 가까울 수록 이상적입니다. False Positive Rate이 낮게 유지되면서 recall이 높은 분류기가 좋은 것 입니다.In&nbsp;[&nbsp;]: import matplotlib.pyplot as pltfrom sklearn.metrics import roc_curvedef plot_roc(name, labels, predictions, **kwargs): fp, tp, _ = roc_curve(labels, predictions) plt.plot(100*fp, 100*tp, label=name, linewidth=2, **kwargs) plt.xlabel(&#39;False positives [%]&#39;) plt.ylabel(&#39;True positives [%]&#39;) plt.xlim([-5,100]) plt.ylim([0,105]) plt.grid(True) ax = plt.gca() ax.set_aspect(&#39;equal&#39;) In&nbsp;[&nbsp;]: plot_roc(&#39;baseline Classifier&#39;, y_test, y_prob) 곡선 아래의 면적값 하나로 ROC curve를 요약할 수 있습니다. 이 면적을 보통 AUC(Area Under the Curve)라고 합니다. 이 ROC curve 아래 면적은 roc_auc_score 함수로 계산합니다.In&nbsp;[&nbsp;]: # roc_auc_score 출력하기from sklearn.metrics import roc_auc_scoreroc_auc_score(y_test, y_prob) Out[&nbsp;]:0.9421551727017916scikit-learn의 classification_report는 accuracy, precision, recall, f1 score의 점수 모두를 한번에 계산해서 깔끔하게 출력해줍니다.In&nbsp;[&nbsp;]: # classification report 출력하기from sklearn.metrics import classification_reportprint(classification_report(y_test, y_pred)) precision recall f1-score support 0.0 0.94 0.96 0.95 2526 1.0 0.75 0.69 0.72 474 accuracy 0.91 3000 macro avg 0.84 0.82 0.83 3000weighted avg 0.91 0.91 0.91 30007. Cross Validation&#182;Machine Learning model training process:성능이 좋은 모델을 만들기 위해 데이터에 반복적으로 모델을 학습시키는 것은 잘못된 방법입니다. 이렇게 훈련된 모델은 train data에만 매우 잘 작동하는 모델이 되며 이전에 학습하지 않은 데이터를 예측해야 할 경우에는 성능이 떨어지게 됩니다. Overfitting(과적합)을 방지하기 위해서 test set을 따로 분류하여 모델의 성능을 확인하게 됩니다.&lt;img src=https://scikit-learn.org/stable/_images/grid_search_workflow.png width=400px&gt;Cross Validation:모델 훈련 과정 중 test data을 활용하여 최적의 모델 파라미터를 찾는 방법은 적합하지 않습니다. 그 이유는 test data에 가장 좋은 성능을 낼 때까지 모델 파라미터를 조정하게 되기 때문에, test data에 과적합될 가능성이 있기 때문입니다. 따라서 test data에 존재하는 분포가 모델에 노출되어 모델의 일반화 성능을 떨어뜨릴 수 있기 때문입니다. 이와 같은 문제를 해결하기 위해서 파라미터를 조정하기 위한 validation data을 따로 구성합니다. 최적의 모델 파라미터를 validation data을 통해 찾고, 최적의 파라미터로 훈련된 모델의 최종 성능은 test data로 평가하게 됩니다. 비율은 train:test=8:2, 분리된 train data에서 다시 train:val=8:2로 설정되거나 train:val:test=6:2:2 또한 자주 사용됩니다. 데이터 크기에 따라 그 비율 또한 조정되어야 합니다.데이터 크기가 작아 validation data을 따로 구성할 수 없거나, 특정한 분포의 데이터가 train 혹은 test data에 포함되어 학습에 영향을 주는 것을 방지하기 위해 cross validation을 사용합니다. train data은 k개의 fold으로 나뉘게 되며, k번 반복되어 훈련되지만 각 훈련마다 valiation set을 k 번째 fold으로 사용합니다. k=5가 가장 흔하며, 5-fold CV라고 부릅니다. 성능은 validation data로 사용된 fold들의 평균 정확도로 계산됩니다.&lt;img src=https://scikit-learn.org/stable/_images/grid_search_cross_validation.png width=500px&gt;Scikit-learn Cross Validation Score 다큐멘테이션In&nbsp;[&nbsp;]: from sklearn.model_selection import cross_val_scoreclf = MLPClassifier(random_state=42, max_iter=1000, learning_rate_init=0.01)scores = cross_val_score(clf, X_train, y_train, cv=5)print(scores) [0.85852941 0.91382353 0.87264706 0.87852941 0.51 ]Scikit-learn K-Folds cross-validator 다큐멘테이션In&nbsp;[&nbsp;]: # 데이터셋을 train과 validation으로 나누어주는 KFold를 알아봅니다.from sklearn.model_selection import KFoldkf = KFold(n_splits=5)for train_index, val_index in kf.split(X_train): X_train_cv, X_val_cv = X_train.iloc[train_index], X_train.iloc[val_index] y_train_cv, y_val_cv = y_train.iloc[train_index], y_train.iloc[val_index] print(y_train_cv.value_counts()) 0.0 111711.0 2429Name: median_house_value_is_high, dtype: int640.0 111941.0 2406Name: median_house_value_is_high, dtype: int640.0 117421.0 1858Name: median_house_value_is_high, dtype: int640.0 110411.0 2559Name: median_house_value_is_high, dtype: int640.0 117441.0 1856Name: median_house_value_is_high, dtype: int64Scikit-learn Stratified K-Folds cross-validator 다큐멘테이션In&nbsp;[&nbsp;]: from sklearn.model_selection import StratifiedKFoldskf = StratifiedKFold(n_splits=5)for train_index, val_index in skf.split(X_train, y_train): X_train_cv, X_val_cv = X_train.iloc[train_index], X_train.iloc[val_index] y_train_cv, y_val_cv = y_train.iloc[train_index], y_train.iloc[val_index] print(y_train_cv.value_counts()) 0.0 113781.0 2222Name: median_house_value_is_high, dtype: int640.0 113781.0 2222Name: median_house_value_is_high, dtype: int640.0 113781.0 2222Name: median_house_value_is_high, dtype: int640.0 113791.0 2221Name: median_house_value_is_high, dtype: int640.0 113791.0 2221Name: median_house_value_is_high, dtype: int64&#44284;&#51228;&#182;Credit Card Fraud dataset&#182;Credit Card Fraud dataset는 2013년 9월 유럽 사용자의 이틀 동안의 신용카드 결제내역입니다.284,807개의 지불내역 중 492개의 이상 거래가 있습니다. 데이터의 불균형이 매우 심하며, positive class(이상 거래)은 모든 지불내역의 0.172% 입니다.데이터에는 PCA(Principal component analysis) 변환의 결과인 숫자 입력 변수만 포함되며, 기밀성 문제로 인해 데이터의 원래 기능과 더 많은 배경 정보가 제공되지 않습니다. 특징 V1, V2 … V28은 PCA를 통해 얻은 주요 feature이며, PCA를 통해 변환되지 않은 feature들은 time과 amount입니다. time feature에는 각 지불내역과 데이터 집합의 첫 번째 지불내역 사이에 경과된 초입니다. amount 기능은 지불내역 금액입니다. Class feature은 label이며 이상 거래 발생 시 1이며 그렇지 않으면 0입니다.In&nbsp;[&nbsp;]: import numpy as npraw_df = pd.read_csv(&#39;https://storage.googleapis.com/download.tensorflow.org/data/creditcard.csv&#39;) In&nbsp;[&nbsp;]: # 데이터의 column을 프린트해봅니다.raw_df.columns Out[&nbsp;]:Index([&#39;Time&#39;, &#39;V1&#39;, &#39;V2&#39;, &#39;V3&#39;, &#39;V4&#39;, &#39;V5&#39;, &#39;V6&#39;, &#39;V7&#39;, &#39;V8&#39;, &#39;V9&#39;, &#39;V10&#39;, &#39;V11&#39;, &#39;V12&#39;, &#39;V13&#39;, &#39;V14&#39;, &#39;V15&#39;, &#39;V16&#39;, &#39;V17&#39;, &#39;V18&#39;, &#39;V19&#39;, &#39;V20&#39;, &#39;V21&#39;, &#39;V22&#39;, &#39;V23&#39;, &#39;V24&#39;, &#39;V25&#39;, &#39;V26&#39;, &#39;V27&#39;, &#39;V28&#39;, &#39;Amount&#39;, &#39;Class&#39;], dtype=&#39;object&#39;)In&nbsp;[&nbsp;]: raw_df[[&#39;Time&#39;, &#39;V1&#39;, &#39;V2&#39;, &#39;V3&#39;, &#39;V4&#39;, &#39;V5&#39;, &#39;V26&#39;, &#39;V27&#39;, &#39;V28&#39;, &#39;Amount&#39;, &#39;Class&#39;]].describe() Out[&nbsp;]: Time V1 V2 V3 V4 V5 V26 V27 V28 Amount Class count 284807.000000 2.848070e+05 2.848070e+05 2.848070e+05 2.848070e+05 2.848070e+05 2.848070e+05 2.848070e+05 2.848070e+05 284807.000000 284807.000000 mean 94813.859575 1.168375e-15 3.416908e-16 -1.379537e-15 2.074095e-15 9.604066e-16 1.683437e-15 -3.660091e-16 -1.227390e-16 88.349619 0.001727 std 47488.145955 1.958696e+00 1.651309e+00 1.516255e+00 1.415869e+00 1.380247e+00 4.822270e-01 4.036325e-01 3.300833e-01 250.120109 0.041527 min 0.000000 -5.640751e+01 -7.271573e+01 -4.832559e+01 -5.683171e+00 -1.137433e+02 -2.604551e+00 -2.256568e+01 -1.543008e+01 0.000000 0.000000 25% 54201.500000 -9.203734e-01 -5.985499e-01 -8.903648e-01 -8.486401e-01 -6.915971e-01 -3.269839e-01 -7.083953e-02 -5.295979e-02 5.600000 0.000000 50% 84692.000000 1.810880e-02 6.548556e-02 1.798463e-01 -1.984653e-02 -5.433583e-02 -5.213911e-02 1.342146e-03 1.124383e-02 22.000000 0.000000 75% 139320.500000 1.315642e+00 8.037239e-01 1.027196e+00 7.433413e-01 6.119264e-01 2.409522e-01 9.104512e-02 7.827995e-02 77.165000 0.000000 max 172792.000000 2.454930e+00 2.205773e+01 9.382558e+00 1.687534e+01 3.480167e+01 3.517346e+00 3.161220e+01 3.384781e+01 25691.160000 1.000000 &lt;svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\" width=\"24px\"&gt; &lt;/svg&gt; In&nbsp;[&nbsp;]: # Class의 분포를 확인해봅니다.raw_df.Class.value_counts() Out[&nbsp;]:0 2843151 492Name: Class, dtype: int64In&nbsp;[&nbsp;]: cleaned_df = raw_df.copy()# time column을 삭제합니다.cleaned_df.pop(&#39;Time&#39;)# amount feature은 범위가 매우 넓습니다. log-space로 변환합니다.eps = 0.001cleaned_df[&#39;Log_Amount&#39;] = np.log(cleaned_df.pop(&#39;Amount&#39;) + eps) &#44284;&#51228; &#45236;&#50857;:&#182;데이터를 train 및 test 데이터로 나눕니다. (비율은 8:2)1-1. train/test데이터를 각각 X, y로 나누고(.pop()), numpy array로 변환합니다.(np.array())1-2. 아래 코드를 사용하여 데이터를 표준화합니다. from sklearn.preprocessing import StandardScaler scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) X_train = np.clip(X_train, -5, 5) X_test = np.clip(X_test, -5, 5)1-3. Scikit-learn의 StandardScaler 다큐멘테이션을 읽고 위 코드가 무엇을 하는지 서술합니다.현재까지 실습에서 다룬 classifier (CategoricalNB, DecisionTreeClassifier, LogisticRegression, MLPClassifier) 중 하나를 선택하여 모델을 생성합니다.선택한 모델의 파라미터 중, 최소 두 개의 파라미터를 최소 두 개의 각기 다른 값으로 설정하여 아래 두 가지의 k-fold CV 방법으로 각각 훈련시켜봅니다.3-1. Scikit-learn의 Kfold을 사용하여 해당 모델을 훈련시키고 평균 성능을 출력합니다.3-2. Scikit-learn의 StratifiedKFold을 사용하여 해당 모델을 훈련시키고 평균 성능을 출력합니다.2번 및 3번 과정에서 얻어진 최적의 파라미터로 최종 모델을 train 데이터에 학습시키고 test 데이터에 대하여 정확도를 출력합니다.confusion matrix을 출력하고, 이를 시각화합니다Precision-Recall curve을 시각화합니다ROC curve을 시각화합니다.AUC을 출력합니다.classification report을 출력합니다.위의 결과를 바탕으로 모델이 불균형한 데이터의 관점으로 성능을 평가합니다. 정확도, precision, recall, f1 score, auc에 대한 분석과 더불어 precision-recall curve, roc curve에 대하여도 서술하여야 합니다.In&nbsp;[&nbsp;]: # 데이터를 train 및 test 데이터로 나눕니다. (비율은 8:2)from sklearn.model_selection import train_test_splittrain_df, test_df = train_test_split(cleaned_df, test_size=0.2, random_state=42) In&nbsp;[&nbsp;]: # 1-1. train/test데이터를 각각 X, y로 나누고(.pop()), numpy array로 변환합니다.y_train = np.array(train_df.pop(&#39;Class&#39;))y_test = np.array(test_df.pop(&#39;Class&#39;))X_train = np.array(train_df)X_test = np.array(test_df) In&nbsp;[&nbsp;]: # 아래 코드를 사용하여 데이터를 표준화합니다.from sklearn.preprocessing import StandardScalerscaler = StandardScaler()X_train = scaler.fit_transform(X_train)X_test = scaler.transform(X_test)X_train = np.clip(X_train, -5, 5)X_test = np.clip(X_test, -5, 5) StandardScalar는 features를 표준화한다.sample X에 대해 평균을 빼고 표준 편차를 나눈 형태의 식 z = (x - u) / s 를 수행하는데 이는 학습 과정에서 데이터가 0 주변에 집중되어 원 데이터와 동일한 순서로 분산되게 만든다. 편차가 큰 경우 올바르게 학습할 수 없게 하는 점을 보완할 수 있다. (centering &amp; scailing)위의 코드는 data를 fit하고 위에 언급한 방식으로 변환하며 np.clip은 리스트의 값들을 최솟값고 최댓값 사이의 값들로 변환시킨 array를 변환시키는데 outlier를 제한하는 효과가 있다.In&nbsp;[&nbsp;]: from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_scorefrom sklearn.neural_network import MLPClassifierfrom sklearn.model_selection import cross_val_scorefrom sklearn.model_selection import KFoldimport pandas as pd In&nbsp;[&nbsp;]: mean_cred_KF1 = []mean_cred_KF2 = []mean_cred_KF1_prec = []mean_cred_KF2_prec = []mean_cred_KF1_rec = []mean_cred_KF2_rec = []mean_cred_KF1_f1 = []mean_cred_KF2_f1 = []kf = KFold(n_splits=5)X_train_pd = pd.DataFrame(X_train)y_train_pd = pd.DataFrame(y_train)for train_index, val_index in kf.split(X_train): X_train_cv, X_val_cv = X_train_pd.iloc[train_index], X_train_pd.iloc[val_index] y_train_cv, y_val_cv = y_train_pd.iloc[train_index], y_train_pd.iloc[val_index] clf_cred1 = MLPClassifier(random_state=42, max_iter = 200, learning_rate_init=0.01, hidden_layer_sizes=150) clf_cred2 = MLPClassifier(random_state=42, max_iter = 200, learning_rate_init=0.05, hidden_layer_sizes=75) clf_cred1.fit(X_train_cv, y_train_cv) clf_cred2.fit(X_train_cv, y_train_cv) cred1_pred = clf_cred1.predict(X_val_cv) cred2_pred = clf_cred2.predict(X_val_cv) mean_cred_KF1.append(accuracy_score(cred1_pred, y_val_cv)) mean_cred_KF2.append(accuracy_score(cred2_pred, y_val_cv)) mean_cred_KF1_prec.append(precision_score(cred1_pred, y_val_cv)) mean_cred_KF2_prec.append(precision_score(cred2_pred, y_val_cv)) mean_cred_KF1_rec.append(recall_score(cred1_pred, y_val_cv)) mean_cred_KF2_rec.append(recall_score(cred2_pred, y_val_cv)) mean_cred_KF1_f1.append(f1_score(cred1_pred, y_val_cv)) mean_cred_KF2_f1.append(f1_score(cred2_pred, y_val_cv)) print(&quot;model 1 : &quot;)print(&quot;accuracy : &quot;, np.mean(mean_cred_KF1), &quot;precision : &quot;, np.mean(mean_cred_KF1_prec), &quot;recall : &quot;, np.mean(mean_cred_KF1_rec),&quot;f1 : &quot;, np.mean(mean_cred_KF1_f1))print(&quot;model 2 : &quot;)print(&quot;accuracy : &quot;, np.mean(mean_cred_KF2), &quot;precision : &quot;, np.mean(mean_cred_KF2_prec), &quot;recall : &quot;, np.mean(mean_cred_KF2_rec),&quot;f1 : &quot;, np.mean(mean_cred_KF2_f1)) model 1 : accuracy : 0.999297768219623 precision : 0.7482972136222911 recall : 0.8355309814075593 f1 : 0.7856648910938665model 2 : accuracy : 0.999174877658057 precision : 0.6710990712074303 recall : 0.8523132466503552 f1 : 0.7016065940336538In&nbsp;[&nbsp;]: from sklearn.model_selection import StratifiedKFoldmean_cred_SKF1 = []mean_cred_SKF2 = []mean_cred_SKF1_prec = []mean_cred_SKF2_prec = []mean_cred_SKF1_rec = []mean_cred_SKF2_rec = []mean_cred_SKF1_f1 = []mean_cred_SKF2_f1 = []skf = StratifiedKFold(n_splits=5)for train_index, val_index in skf.split(X_train, y_train): X_train_cv, X_val_cv = X_train_pd.iloc[train_index], X_train_pd.iloc[val_index] y_train_cv, y_val_cv = y_train_pd.iloc[train_index], y_train_pd.iloc[val_index] clf_cred1 = MLPClassifier(random_state=42, max_iter = 200, learning_rate_init=0.01, hidden_layer_sizes=150) clf_cred2 = MLPClassifier(random_state=42, max_iter = 200, learning_rate_init=0.05, hidden_layer_sizes=75) clf_cred1.fit(X_train_cv, y_train_cv) clf_cred2.fit(X_train_cv, y_train_cv) cred1_pred = clf_cred1.predict(X_val_cv) cred2_pred = clf_cred2.predict(X_val_cv) mean_cred_SKF1.append(accuracy_score(cred1_pred, y_val_cv)) mean_cred_SKF2.append(accuracy_score(cred2_pred, y_val_cv)) mean_cred_SKF1_prec.append(precision_score(cred1_pred, y_val_cv)) mean_cred_SKF2_prec.append(precision_score(cred2_pred, y_val_cv)) mean_cred_SKF1_rec.append(recall_score(cred1_pred, y_val_cv)) mean_cred_SKF2_rec.append(recall_score(cred2_pred, y_val_cv)) mean_cred_SKF1_f1.append(f1_score(cred1_pred, y_val_cv)) mean_cred_SKF2_f1.append(f1_score(cred2_pred, y_val_cv)) print(&quot;model 1 : &quot;)print(&quot;accuracy : &quot;, np.mean(mean_cred_SKF1), &quot;precision : &quot;, np.mean(mean_cred_SKF1_prec), &quot;recall : &quot;, np.mean(mean_cred_SKF1_rec),&quot;f1 : &quot;, np.mean(mean_cred_SKF1_f1))print(&quot;model 2 : &quot;)print(&quot;accuracy : &quot;, np.mean(mean_cred_SKF2), &quot;precision : &quot;, np.mean(mean_cred_SKF2_prec), &quot;recall : &quot;, np.mean(mean_cred_SKF2_rec),&quot;f1 : &quot;, np.mean(mean_cred_SKF2_f1)) model 1 : accuracy : 0.9992802124251137 precision : 0.7488802336903603 recall : 0.8212524403700874 f1 : 0.7825478617983272model 2 : accuracy : 0.9992714345278587 precision : 0.7312560856864654 recall : 0.8342979721021179 f1 : 0.7735200656174485In&nbsp;[&nbsp;]: &quot;&quot;&quot;2번 및 3번 과정에서 얻어진 최적의 파라미터로 최종 모델을 train 데이터에 학습시키고 test 데이터에 대하여 정확도를 출력합니다.&quot;&quot;&quot;clf_final = MLPClassifier(random_state=42, max_iter = 200, learning_rate_init=0.01, hidden_layer_sizes=150) clf_final.fit(X_train, y_train)y_pred = clf_final.predict(X_test)accuracy_final = accuracy_score(y_pred, y_test) In&nbsp;[&nbsp;]: # confusion matrix을 출력하고, 이를 시각화합니다from sklearn.metrics import confusion_matrixconfusion_matrix(y_test, y_pred) Out[&nbsp;]:array([[56857, 7], [ 26, 72]])In&nbsp;[&nbsp;]: # Precision-Recall curve을 시각화합니다from sklearn.metrics import PrecisionRecallDisplayPrecisionRecallDisplay.from_estimator(clf_final, X_test, y_test) Out[&nbsp;]:&lt;sklearn.metrics._plot.precision_recall_curve.PrecisionRecallDisplay at 0x7f667f5c9ed0&gt; In&nbsp;[&nbsp;]: # ROC curve을 시각화합니다.from sklearn.metrics import roc_curvey_prob = clf_final.predict_proba(X_test)[:,1]plot_roc(&#39;baseline Classifier&#39;, y_test, y_prob) In&nbsp;[&nbsp;]: # AUC을 출력합니다.from sklearn.metrics import roc_auc_scoreroc_auc_score(y_test, y_prob) Out[&nbsp;]:0.9631875875701997In&nbsp;[&nbsp;]: # classification report을 출력합니다.from sklearn.metrics import classification_reportprint(classification_report(y_test, y_pred)) precision recall f1-score support 0 1.00 1.00 1.00 56864 1 0.91 0.73 0.81 98 accuracy 1.00 56962 macro avg 0.96 0.87 0.91 56962weighted avg 1.00 1.00 1.00 56962해당 분석의 데이터는 label 0이 1보다 많은 불균형 데이터로, 위와 같은 데이터 상에서는 instance들을 0으로 예측하면 모델의 실제 성능과 관계 없이 높은 정확도를 보이는 모델을 만들 수 있다. 하지만, 전체 데이터에서 소수 존재하는 label인 1을 예측하는 정확도는 낮게 나타날 수 있다. 따라서 불균형한 데이터에서 정확도 지표는 모델의 성능을 파악할 수 없다.precision은 positive로 분류한 instance 중 실제 positive를 나타내며 recall은 실제 positive 중 positive로 분류한 것을 나타내는 지표이다.불균형한 데이터를 분석한 모델을 평가할 때 이 두 가지는 정확도보다 더 나은 평가지표가 될 수 있다. f1 score는 precision과 recall의 조화평균으로 recall과 precision 둘 중 하나가 매우 낮게 나오는 경우를 반영하므로 정밀도와 재현율을 종합하는 성능평가가 가능하다.위의 데이터를 학습한 모델을 평가한 score를 확인해보면 accuracy를 기반으로 성능 평가를 했을 경우 해당 모델은 몇 개의 데이터만 놓쳤을 뿐 대부분의 데이터를 예측할 수 있는 모델로 보인다. 하지만 f1 등의 다른 평가 지표를 고려하면 약 70~80% 정도의 성능을 보이는 적절한 모델임을 확인할 수 있다.ROC curve는 FPR이 변할 떄, TPR의 변화를 나타내는 곡선이다. ROC curve의 밑 면적을 뜻하는 AUC는 1에 가까울 수록 예측을 잘한다고 분석한다. precision recall curve는 x축이 recall이고 y축이 precision이다. parameter인 threshold에 변화를 주면서 precision과 recall을 Plot한 curve이다. 아 curve의 아래 면적이 AUC-PR이다. AUC-PR는 불균형한 데이터를 평가하는 좋은 평가지표다.해당 모델은 불균형한 데이터를 통해 학습되었으므로 정확도를 통한 성능 평가는 적합하지 않다. AUC 관점에서 score가 약 0.9632 이므로 해당 모델의 성능은 높다고 볼 수 있다."
    } ,
  
    {
      "title"       : "Machine Learning - KNN",
      "category"    : "",
      "tags"        : "machine learning, study_model, perceptron",
      "url"         : "./Machine-Learning-KNN.html",
      "date"        : "2023-01-11 00:00:00 +0900",
      "description" : "ML - K-nearest neighbor classifier",
      "content"     : "&lt;!DOCTYPE html&gt;Ensemble_model K-Nearest Neighbor Classifier (KNN)&#182;1-1. &#45936;&#51060;&#53552; &#54252;&#51064;&#53944; &#44036;&#51032; &#44144;&#47532; - 2D&#182;KNN 모델로 영화 평가 분류기를 구현합니다.두 점이 서로 가깝거나 멀리 떨어져 있는 정도를 측정하기 위해 거리 공식을 사용할 것입니다.이 예제의 경우 데이터의 차원은 다음과 같습니다.영화의 러닝타임영화 개봉 연도스타워즈와 인디아나 존스를 예로 들겠습니다. 스타워즈는 125분이며 1977년에 개봉했습니다. 인디아나 존스는 115분이며 1981년에 개봉했습니다.이 두 영화를 의미하는 두 데이터 포인트들의 거리는 아래와 같이 계산됩니다.$$\\sqrt{(125 - 115)^2 + (1977-9181)^2} = 10.77$$Practice 1&#182;movie1과 movie2라는 두 리스트을 매개 변수로 사용하는 distance라는 함수를 선언합니다.각 리스트의 첫 번째 인덱스는 영화의 러닝타임이고 두 번째 인덱스는 영화의 개봉 연도입니다. distance 함수는 두 리스트 사이의 거리를 반환해야 합니다.In&nbsp;[&nbsp;]: # distance 함수 구현하기def distance(movie1, movie2): length_difference = (movie1[0] - movie2[0]) ** 2 year_difference = (movie1[1] - movie2[1]) ** 2 distance = (length_difference + year_difference) ** 0.5 return distance 아래 영화에 대해 distance 함수를 호출하여 거리를 비교해봅니다.In&nbsp;[&nbsp;]: star_wars = [125, 1977]raiders = [115, 1981]mean_girls = [97, 2004] In&nbsp;[&nbsp;]: # star_wars와 raiders의 거리print(distance(star_wars, raiders))# star_wars와 mean_girls의 거리print(distance(star_wars, mean_girls))# 스타워즈는 어느 영화와 더 비슷합니까?# 정답: 10.77032961426900738.8973006775534461-2. &#45936;&#51060;&#53552; &#54252;&#51064;&#53944; &#44036;&#51032; &#44144;&#47532; - 3D&#182;영화의 길이와 개봉일만을 기준으로 영화 평가 분류기를 만드는 것은 상당히 제한적입니다. 영화의 다른 속성을 데이터에 포함하여 세번째 차원을 추가해 보겠습니다.추가된 세 번째 차원은 영화 예산입니다. 이제 영화를 나타내는 두 데이터 포인트들의 거리를 3차원으로 찾아야 합니다.데이터의 차원이 삼차원보다 많아지면 시각화 하는 것은 어렵지만 그래도 거리를 구할 수 있습니다.N차원의 데이터 포인트 A와 B 사이의 거리 공식은 다음과 같습니다.$$\\sqrt{(A_1 - B_1)^2 + (A_2 - B_2)^2 + ... + (A_n - B_n)^2}$$여기서 $A_1 - B_1$은 각 데이터 포인트의 첫 번째 feature 간의 차입니다. $A_n - B_n$은(는) 각 점의 마지막 feature 간의 차입니다.이 공식을 이용하면 N차원 공간에서 한 데이터 포인트의 K-Nearest Neighbors를 찾을 수 있습니다.이 거리를 사용하여 label되지 않은 데이터 포인트의 가장 가까운 이웃을 찾아 결과적으로 분류를 하게 됩니다.Practice 2&#182;distance 함수를 N개의 차원의 데이터의 거리를 반환하는 함수로 수정합니다.In&nbsp;[&nbsp;]: # distance 함수 수정def distance(movie1, movie2): squared_difference = 0 for i in range(len(movie1)): squared_difference += (movie1[i] - movie2[i]) ** 2 final_distance = squared_difference ** 0.5 return final_distance 위 활동에서 주어진 영화들에 영화 예산 속성이 추가되었습니다. 아래 영화에 대해 distance 함수를 호출하여 거리를 비교해봅니다.In&nbsp;[&nbsp;]: star_wars = [125, 1977, 11000000]raiders = [115, 1981, 18000000]mean_girls = [97, 2004, 17000000] In&nbsp;[&nbsp;]: # star_wars와 raiders의 거리print(distance(star_wars, raiders))# star_wars와 mean_girls의 거리print(distance(star_wars, mean_girls))# 스타워즈는 어느 영화와 더 비슷합니까?# 정답: mean girls 7000000.0000082866000000.0001260831-3. &#52377;&#46020;&#44032; &#45796;&#47480; &#45936;&#51060;&#53552;: &#51221;&#44508;&#54868;&#182;이 활동에서는 K-Nearest Neighborhood 알고리즘의 첫 번째 단계를 구현합니다.데이터를 정규화합니다.가장 가까운 이웃인 k를 찾습니다.이러한 이웃을 기준으로 새로운 포인트를 분류합니다.우리가 영화 예산 속성을 추가하면 데이터의 범위가 변화한것을 확인할 수 있습니다.영화 개봉 연도와 영화 예산을 살펴보겠습니다. 어떠한 두 영화의 개봉 날짜의 최대 차이는 약 125년입니다((루미에르 브라더스는 1890년대에 영화를 만들고 있었습니다). 하지만, 어떠한 두 영화의 예산 차이는 수백만 달러가 될 수 있습니다.거리 공식의 문제는 규모에 상관없이 모든 차원을 동등하게 취급한다는 것입니다. 이는 1년 차이가 나는 영화 개봉 년도를 1달러의 차이의 영화 예산과 같은 정도로 취급한다는 뜻입니다.이 문제에 대한 해결책은 정규화입니다. 이는 모든 값은 0과 1 사이로 변환합니다.이번 실습은 최소-최대 정규화(Min-Max Normalization)를 사용할 것입니다.$$x_{norm} = \\frac{x-min(x)}{max(x)-min(x)}$$참고: scikit-learn MinMaxScaler1-3-1. Normalization vs Standardization&#182;nomalization : 데이터 전체 분포를 어떤 범위 안에 꾸겨 넣는 것Practice 3&#182;In&nbsp;[&nbsp;]: # min_max_normalize 함수 구현def min_max_normalize(lst): # 1. minimum = min(lst) maximum = max(lst) # 2. normalized = [] for value in lst: normalized_num = (value - minimum) / (maximum - minimum) normalized.append(normalized_num) return normalized In&nbsp;[&nbsp;]: release_dates = [1897, 1998, 2000, 1948, 1962, 1950, 1975, 1960, 2017, 1937, 1968, 1996, 1944, 1891, 1995, 1948, 2011, 1965, 1891, 1978] In&nbsp;[&nbsp;]: from pprint import pprint# 결과값 출력하기pprint(min_max_normalize(release_dates)) [0.047619047619047616, 0.8492063492063492, 0.8650793650793651, 0.4523809523809524, 0.5634920634920635, 0.46825396825396826, 0.6666666666666666, 0.5476190476190477, 1.0, 0.36507936507936506, 0.6111111111111112, 0.8333333333333334, 0.42063492063492064, 0.0, 0.8253968253968254, 0.4523809523809524, 0.9523809523809523, 0.5873015873015873, 0.0, 0.6904761904761905]1-4. Nearest Neighbors &#52286;&#44592;&#182;K-Nearest Neighbors 알고리즘의 두번째 단계를 구현해보겠습니다.데이터를 정규화합니다.가장 가까운 이웃 k를 찾습니다.이러한 이웃을 기준으로 새로운 포인트를 분류합니다.Label되지 않은 데이터 포인터를 분류하기 위해서는 이 데이터 포인트와 가장 가까운 k개의 이웃을 찾아야 합니다. 가장 적합한 k를 찾는 방법도 있지만, 일단은 k=5로 정하도록 하겠습니다.가장 가까운 5개의 이웃을 찾기 위해서는 label되지 않은 데이터 포인트를 데이터의 다른 모든 데이터 포인트와의 거리를 계산하여 비교합니다. 이번 실습에서 구현할 함수의 반환값은 label되지 않은 한 영화에 대해 데이터의 모든 영화와의 거리가 정렬된 리스트입니다.반환값의 예시는 다음과 같습니다.[ [0.30, &#39;Superman II&#39;], [0.31, &#39;Finding Nemo&#39;], ... ... [0.38, &#39;Blazing Saddles&#39;]]이 예에서 label되지 않은 영화와 슈퍼맨2 영화와의 거리는 0.30입니다.In&nbsp;[&nbsp;]: # 영화 데이터 받아오기import urllib.requestimport astmovie_dataset_url = &quot;https://raw.githubusercontent.com/inikoreaackr/ml_datasets/main/movie_dataset.txt&quot;movie_datasets = []for line in urllib.request.urlopen(movie_dataset_url): movie_datasets.append(line.decode(&#39;utf-8&#39;))movie_dataset = ast.literal_eval(movie_datasets[0][16:])movie_labels = ast.literal_eval(movie_datasets[1][15:]) Practice 4&#182;위에 주어진 영화 데이터 중 The Avengers 영화를 출력해봅니다. 각 영화의 세 가지 feature은 다음과 같습니다.정규화된 영화 예산(달러)정규화된 러닝타임(분)정규화된 개봉 연도위 데이터의 label은 좋은 영화와 나쁜 영화를 나타냅니다. The Avengers 영화의 label은 1로, 좋은 영화입니다. 분류 기준은 IMDb에서 7.0 이상의 평가를 받으면 좋은 영화로 분류됩니다.In&nbsp;[&nbsp;]: print(movie_dataset[&#39;The Avengers&#39;])print(movie_labels[&#39;The Avengers&#39;]) [0.018009887923225047, 0.4641638225255973, 0.9550561797752809]1classify 함수를 선언하고 구현해봅니다.In&nbsp;[&nbsp;]: # classify 함수를 구현def classify(unknown, dataset, k): distances = [] #Looping through all points in the dataset for title in dataset: movie = dataset[title] distance_to_point = distance(movie, unknown) #Adding the distance and point associated with that distance distances.append([distance_to_point, title]) distances.sort() #Taking only the k closest points neighbors = distances[0:k] return neighbors 아래 주어진 매개변수로 classify 함수를 테스트하고 결과를 출력합니다.[.4, .2, .9]movie_dataset5In&nbsp;[&nbsp;]: # `classify` 함수를 테스트하고 결과를 출력합니다.print(classify([.4, .2, .9], movie_dataset, 5)) [[0.08273614694606074, &#39;Lady Vengeance&#39;], [0.22989623153818367, &#39;Steamboy&#39;], [0.23641372358159884, &#39;Fateless&#39;], [0.26735445689589943, &#39;Princess Mononoke&#39;], [0.3311022951533416, &#39;Godzilla 2000&#39;]]1-5. Neighbors &#49464;&#44592;&#182;이 활동에서는 K-Nearest Neighbor 알고리즘 중 마지막 단계를 구현합니다.데이터를 정규화합니다.가장 가까운 이웃인 k를 찾습니다.이웃을 기준으로 새로운 포인트를 분류합니다이전 활동에서 어떠한 label되지 않은 데이터 포인트에 대해 가장 가까운 이웃 k개를 찾아 다음과 같은 형태로 리스트에 저장했습니다.[ [0.083, &#39;Lady Vengeance&#39;], [0.236, &#39;Steamboy&#39;], ... ... [0.331, &#39;Godzilla 2000&#39;]]이 리스트의 좋고 나쁜 영화의 갯수를 셈으로써 분류를 할 수 있습니다. 만약 좋은 영화로 분류된 이웃들의 갯수가 더 많아면, label되지 않은 영화는 좋은 영화로 분류됩니다. 반대의 경우에는 나쁜 영화로 분류됩니다.만약 좋은 영화로 분류된 이웃들의 갯수와 나쁜 영화로 분류된 이웃들의 갯수가 같다면 가장 거리가 가까운 영화의 label을 선택하게 됩니다.Practice 5&#182;이 활동에서는 위의 classify 함수를 수정하여 unknown 데이터 포인트에 대한 분류값을 반환하겠습니다. labels 매개 변수를 classify함수에 추가합니다.num_good와 num_bad라는 두 변수를 만들고 각각 0으로 초기화합니다.labels과 title을 사용하여 각 영화의 label을 받아옵니다.해당 라벨이 0이면 num_bad에 1을 증가시킵니다.해당 라벨이 1이면 num_good에 1을 증가시킵니다.이제 label되지 않은 영화를 분류할 수 있습니다.num_good가 num_bad보다 크면 1을 반환합니다.그렇지 않으면 0을 반환합니다.In&nbsp;[&nbsp;]: # classify 함수 수정하기# 1.def classify(unknown, dataset, labels, k): distances = [] for title in dataset: movie = dataset[title] distance_to_point = distance(movie, unknown) #Adding the distance and point associated with that distance distances.append([distance_to_point, title]) distances.sort() #Taking only the k closest points neighbors = distances[0:k] # 2. num_good = 0 num_bad = 0 # 3. for neighbor in neighbors: title = neighbor[1] if labels[title] == 0: num_bad += 1 elif labels[title] == 1: num_good += 1 # 4. if num_good &gt; num_bad: return 1 else: return 0 다음 파라미터를 사용하여 classify 함수를 호출하여 결과를 출력해봅니다.분류하려는 영화는 [.4, .2, .9] 입니다.훈련 데이터는 movie_dataset 입니다.훈련 데이터의 label은 movie_labels입니다.k는 5로 지정합니다.In&nbsp;[&nbsp;]: # `classify` 함수를 테스트하고 결과를 출력합니다.print(classify([.4, .2, .9], movie_dataset, movie_labels, 5)) 11-6. &#54616;&#45208;&#51032; &#45936;&#51060;&#53552; &#54252;&#51064;&#53944; &#48516;&#47448;&#54616;&#44592;&#182;위의 활동까지는 임의의 데이터인 [.4, .2, .9]에만 테스트 해보았습니다. 이번 활동에서는 실제 영화 Call Me By Your Name 데이터를 정규화하고 예측해 볼 것입니다.먼저 분류하고자 하는 영화가 훈련 데이터에 있지는 않은지 확인해야 합니다.In&nbsp;[&nbsp;]: # &quot;Call Me By Your Name&quot;이 훈련 데이터에 있는지 확인하기print(&quot;Call Me By Your Name&quot; in movie_dataset) False분류하고자 하는 영화가 훈련 데이터에 없는 것으로 확인 되면 my_movie 변수를 생성하고 영화 예산, 러닝타임, 개봉 연도의 순으로 데이터를 구성합니다.Call Me By Your Name의 예산은 35만 달러, 런타임은 132분, 개봉 연도는 2017년입니다.In&nbsp;[&nbsp;]: # my_movie 변수 생성하기my_movie = [3500000, 132, 2017] 주어진 normalize_dimension 함수를 사용하여 my_movie를 정규화합니다. normalized_my_movie라는 변수를 만들고 my_movie의 정규화된 값을 저장합니다. 결과를 출력해봅니다.In&nbsp;[&nbsp;]: # x, y, z points array 받아오기import urllib.requestimport astimport numpy as nppoints_url = &quot;https://raw.githubusercontent.com/inikoreaackr/ml_datasets/main/points_xyz.txt&quot;points = []for line in urllib.request.urlopen(points_url): points.append(line.decode(&#39;utf-8&#39;))training_x = np.array(ast.literal_eval(points[0]))training_y = np.array(ast.literal_eval(points[1]))training_z = np.array(ast.literal_eval(points[2])) In&nbsp;[&nbsp;]: def normalize_dimension(value, lst): minimum = min(lst) maximum = max(lst) return (value - minimum) / (maximum - minimum)def normalize_point(pt): global training_x global training_y global training_z newx = normalize_dimension(pt[0], training_x) newy = normalize_dimension(pt[1], training_y) newz = normalize_dimension(pt[2], training_z) return [newx, newy, newz] In&nbsp;[&nbsp;]: # my_movie 변수를 정규화합니다.normalized_my_movie = normalize_point(my_movie)print(normalized_my_movie) [0.00028650338197026213, 0.3242320819112628, 1.0112359550561798]normalized_my_movie, movie_dataset, movie_labels, k=5 매개 변수를 사용하여 \"classify\"를 호출하고 결과를 출력합니다.In&nbsp;[&nbsp;]: # classify 함수를 호출합니다.classify(normalized_my_movie, movie_dataset, movie_labels, 5)# TODO: Call Me By Your Name 영화의 분류된 lable은 무엇인가요?# 정답: Out[&nbsp;]:11-7. scikit-learn&#51004;&#47196; KNN &#44396;&#54788;&#54616;&#44592;&#182;사용 데이터: Breast Cancer Wisconsin (Diagnostic) Data Set)사용 모델: sklearn.neighbors.KNeighborsClassifierIn&nbsp;[&nbsp;]: import pandas as pddata_url = &#39;https://raw.githubusercontent.com/inikoreaackr/ml_datasets/master/breastcancer.csv&#39;df = pd.read_csv(data_url)df.head() Out[&nbsp;]: id diagnosis radius_mean texture_mean perimeter_mean area_mean smoothness_mean compactness_mean concavity_mean concave points_mean ... texture_worst perimeter_worst area_worst smoothness_worst compactness_worst concavity_worst concave points_worst symmetry_worst fractal_dimension_worst Unnamed: 32 0 842302 M 17.99 10.38 122.80 1001.0 0.11840 0.27760 0.3001 0.14710 ... 17.33 184.60 2019.0 0.1622 0.6656 0.7119 0.2654 0.4601 0.11890 NaN 1 842517 M 20.57 17.77 132.90 1326.0 0.08474 0.07864 0.0869 0.07017 ... 23.41 158.80 1956.0 0.1238 0.1866 0.2416 0.1860 0.2750 0.08902 NaN 2 84300903 M 19.69 21.25 130.00 1203.0 0.10960 0.15990 0.1974 0.12790 ... 25.53 152.50 1709.0 0.1444 0.4245 0.4504 0.2430 0.3613 0.08758 NaN 3 84348301 M 11.42 20.38 77.58 386.1 0.14250 0.28390 0.2414 0.10520 ... 26.50 98.87 567.7 0.2098 0.8663 0.6869 0.2575 0.6638 0.17300 NaN 4 84358402 M 20.29 14.34 135.10 1297.0 0.10030 0.13280 0.1980 0.10430 ... 16.67 152.20 1575.0 0.1374 0.2050 0.4000 0.1625 0.2364 0.07678 NaN 5 rows × 33 columns &lt;svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\" width=\"24px\"&gt; &lt;/svg&gt; In&nbsp;[&nbsp;]: # id 와 Unnamed: 32 column을 삭제한 후 column들의 리스트를 출력합니다.df.drop(columns=[&#39;id&#39;, &#39;Unnamed: 32&#39;], inplace=True)df.columns Out[&nbsp;]:Index([&#39;diagnosis&#39;, &#39;radius_mean&#39;, &#39;texture_mean&#39;, &#39;perimeter_mean&#39;, &#39;area_mean&#39;, &#39;smoothness_mean&#39;, &#39;compactness_mean&#39;, &#39;concavity_mean&#39;, &#39;concave points_mean&#39;, &#39;symmetry_mean&#39;, &#39;fractal_dimension_mean&#39;, &#39;radius_se&#39;, &#39;texture_se&#39;, &#39;perimeter_se&#39;, &#39;area_se&#39;, &#39;smoothness_se&#39;, &#39;compactness_se&#39;, &#39;concavity_se&#39;, &#39;concave points_se&#39;, &#39;symmetry_se&#39;, &#39;fractal_dimension_se&#39;, &#39;radius_worst&#39;, &#39;texture_worst&#39;, &#39;perimeter_worst&#39;, &#39;area_worst&#39;, &#39;smoothness_worst&#39;, &#39;compactness_worst&#39;, &#39;concavity_worst&#39;, &#39;concave points_worst&#39;, &#39;symmetry_worst&#39;, &#39;fractal_dimension_worst&#39;], dtype=&#39;object&#39;)In&nbsp;[&nbsp;]: # 각 column에 na 값이 있지는 않은지 확인합니다.df.isna().any() Out[&nbsp;]:diagnosis Falseradius_mean Falsetexture_mean Falseperimeter_mean Falsearea_mean Falsesmoothness_mean Falsecompactness_mean Falseconcavity_mean Falseconcave points_mean Falsesymmetry_mean Falsefractal_dimension_mean Falseradius_se Falsetexture_se Falseperimeter_se Falsearea_se Falsesmoothness_se Falsecompactness_se Falseconcavity_se Falseconcave points_se Falsesymmetry_se Falsefractal_dimension_se Falseradius_worst Falsetexture_worst Falseperimeter_worst Falsearea_worst Falsesmoothness_worst Falsecompactness_worst Falseconcavity_worst Falseconcave points_worst Falsesymmetry_worst Falsefractal_dimension_worst Falsedtype: boolIn&nbsp;[&nbsp;]: # label이 어떻게 구성되어있나 봅니다.df[&#39;diagnosis&#39;].value_counts() Out[&nbsp;]:B 357M 212Name: diagnosis, dtype: int64In&nbsp;[&nbsp;]: # 데이터를 X와 y로 나누고 train, test 셋으로 나눕니다.from sklearn.model_selection import train_test_splity = df[&#39;diagnosis&#39;]X = df.drop(columns=[&#39;diagnosis&#39;])X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True) In&nbsp;[&nbsp;]: # scikit-learn의 KNN classifier을 생성하고 훈련시킵니다.from sklearn.neighbors import KNeighborsClassifierknn = KNeighborsClassifier()knn.fit(X_train, y_train) Out[&nbsp;]:KNeighborsClassifier()In&nbsp;[&nbsp;]: # test 셋에 대하여 정확도를 출력해봅니다.knn.score(X_test, y_test) Out[&nbsp;]:0.95906432748538011-8. &#44032;&#51109; &#51201;&#54633;&#54620; k &#52286;&#44592;&#182;In&nbsp;[&nbsp;]: n_neighbors_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]score = []# 가장 적합한 k를 찾아봅니다.for k in n_neighbors_list: knn = KNeighborsClassifier(n_neighbors=k) knn.fit(X_train, y_train) score.append(knn.score(X_test, y_test)) In&nbsp;[&nbsp;]: n_neighbors_list[np.argmax(score)] Out[&nbsp;]:9In&nbsp;[&nbsp;]: import matplotlib.pyplot as pltplt.plot(n_neighbors_list, score) Out[&nbsp;]:[&lt;matplotlib.lines.Line2D at 0x7f0adc331190&gt;]"
    } ,
  
    {
      "title"       : "ML summary - logistic regression",
      "category"    : "",
      "tags"        : "machine learning, study_theory, logistic regression",
      "url"         : "./ML-summary-Logistic-Regression.html",
      "date"        : "2023-01-11 00:00:00 +0900",
      "description" : "ML summary - logistic regression",
      "content"     : "Logistic Regression예측 값이 연속적인 값을 가지지 않는 분류 문제를 binary classification이라고 한다. 이는 class가 2종류인 분류 문제이며, 대표적으로는 스팸 메일 분류가 있다. 이러한 문제를 푸는 방법 중 하나로 Logistic Regression이 있다.E.g Email : Spam / Not Spam Online Transactions : Fraudulent (Yes / No)-Tumor : Malignant / Benign\\[y \\in \\{ 0, 1 \\}\\] binary classification for tumors\\[if h_\\theta(x) \\ge 0.5, \\quad predict \\quad y = 1\\\\ if h_\\theta(x) \\le 0.5, \\quad predict \\quad y = 0\\]target variable이 0 또는 1로 표현이 된다면 위와 같은 linear regression을 사용할 수도 있다. 모델은 예측과 관찰의 error가 최소치가 되도록 설계해야 한다. result of outlierTraining data가 위와는 다르게 종양의 크기가 극명하게 큰 환자가 있다고 가정하자. linear regression은 x의 값이 커지면 커질수록 y값도 극명하게 커지게 된다. 만약 이렇게 된다면 이 환자의 y값은 여전히 1이지만 실제 분류선에서의 y값의 크기는 커지게 될 것이다.이는 에러값의 증가를 불러오고 좀 더 완만한 경사도를 지닌 linear model을 만들어내게 된다. 기존에 3.6를 기준으로 악성과 양성을 구분하던 모델은 기준점을 높이게 될 것이다. 결론적으로 잘못된 분류 모델을 만들어 낼 수 있다.선형식으로 이를 해결할 수 없기 때문에 우리는 Logistic regression을 도입할 수 있다. logistic regression\\[h_{\\theta} (x) = g(\\theta^{T} x) = {1 \\over {1 + e^{-\\theta^Ts}}} \\\\ e.g. : y = ax + b \\\\ y = \\sin (ax + b)\\]입력 features가 1개 이상일 때, \\(h_{\\theta} (x) = g(\\theta_0 +\\theta _1 x _1 +\\theta _2 x _2 )\\)decision boundary정의된 classification model에 대해 target variable을 구분 짓는 선을 그릴 수 있다.E.g.\\(if \\quad -3+x_1 + x_2 \\ge 0, \\quad y = 1\\) Decision boundarycost function주어진 target variable에 대해 i번째 training data에 대해 이를 분류하는 model이 있다면 실제 label에 대한 loss를 계산할 수 있다. 만약 label 이 1인 data에 대해 1이라고 판단한다면 loss는 0일 것이고 이는 잘 분류했다고 볼 수 있다.loss는 0에 가까울 수록 좋다고 볼 수 있다. 하지만 이같은 단순한 loss function은 사용되지 않는다. 이는 분류 확률을 고려하기 때문이다. 95%의 확률로 분류된 것과 55%의 확률로 분류된 것이 다르기 때문에 label을 정확히 예측할 수 있도록 분류 확률을 높이는 방식으로 loss를 설계할 필요가 있다. cost functionGradient descent in Logistic Regression\\[J(\\theta) = {-1 \\over m}[\\Sigma_{i=1}^m h_\\theta(x^{(i)}) + (1-y^{(i)})\\log(1 - h_\\theta(x^{(i)}))]\\]이를 최소화하는 것이 Gradient descent의 목적이다.\\[J(a, b) = - \\log _e {1 \\over {1 + e^{-(ax+b)}}}\\\\ {dJ(a, b) \\over da } = {e^{-(ax+b)} \\over 1 + e^{-(ax+b)}} x\\]ax + b -&gt; z로 치환하여 정리하면\\[{dJ(a, b) \\over da } = -{e^{-z}\\over1+e^{-z}} = - (1 - {1 \\over1+e^{-z}})\\]와 같은 형태로 정리할 수 있다.Multiclass classificatonbinary classification 보다 좀 더 세분화된 target label이 있을 때, binary classification VS multiclass classificationone or All(rest) –각각의 class에 대해 대응하는 각각의 classification model을 반드는 모델. 여러 카테고리 중 하나만을 positive로 가정하여 학습하고 모든 i번째 카테고리에 대해 학습을 한다. How to classify multiclass labels [One or All]E.g. \\(x_1 = 1, \\quad x_2 = 2 \\\\ \\quad 2x_1 + 3x_2 +4 = 12 \\quad about \\quad 99.95\\% \\\\ \\quad -x_1 + 5x_2 - 3 = 6 \\quad about \\quad 98\\% \\\\ \\quad -4x_1 + 3_2 +22 = 0 \\quad about \\quad 5\\% \\\\\\) \\({e^{12} \\over {e^{12} + e^6 + e^0}} = 92\\% \\\\ {e^{6} \\over {e^{12} + e^6 + e^0}} = 7\\% \\\\ {e^{0} \\over {e^{12} + e^6 + e^0}} = 1\\% \\\\\\)"
    } ,
  
    {
      "title"       : "Machine Learning - Ensemble model",
      "category"    : "",
      "tags"        : "machine learning, study_model, Ensemble model",
      "url"         : "./Ensemble-model.html",
      "date"        : "2023-01-11 00:00:00 +0900",
      "description" : "ML - Ensemble model",
      "content"     : "&lt;!DOCTYPE html&gt;Ensemble_model (1) Random Forest&#182;앙상블(ensemble)은 기존에 존재하는 기계학습 알고리즘으로 구축된 여러 모델의 예측을 결합하여 단일 모델에 비해 generalizability / robustness을 향상시키는 방법입니다.앙상블 방법은 보통 두 가지 계열로 구분됩니다.평균화(averaging) 방법: 여러 개의 모델을 독립적으로 생성 후 예측을 평균화합니다. 결합 추정기는 분산(variance)이 감소하기 때문에 일반적으로 단일 기본 모델보다 낫습니다.부스팅(boosting) 방법: 기본 모델이 순차적으로 구축되고 결합된 모델의 편향(bias)을 줄이려고 합니다. 강력한 앙상블을 만들기 위해 몇 가지 약한 모델을 결합하는 방법입니다.가장 대표적인 앙상블 모델은 랜덤 포레스트(random forest)이며, 이 모델은 기본 구성 요소로 결정 트리를 사용합니다.결정 트리의 단점은 훈련 데이터에 과적합되는 경향이 있다는 것입니다. 랜덤 포레스트는 조금씩 다른 여러 결정 트리를 묶어 과적합 문제를 피할 수 있습니다. 랜덤 포레스트의 원리는, 각 트리는 비교적 예측을 잘 할 수 있지만 데이터의 일부에 과적합하는 경향을 가진다는 데 기초합니다. 잘 작동하되 서로 다른 방향으로 과적합된 트리를 많이 만들면 그 결과를 평균냄으로써 과적합된 양을 줄일 수 있습니다. 이렇게 하면 트리 모델의 예측 성능이 유지되면서 과적합이 줄어드는 것이 수학적으로 증명되었습니다.이런 전략을 구현하려면 결정 트리를 많이 만들어야 합니다. 각각의 트리는 타깃 예측을 잘 해야 하고 다른 트리와는 구별되어야 합니다. 랜덤 포레스트는 이름에서 알 수 있듯이 트리들이 달라지도록 트리 생성 시 무작위성을 주입합니다.랜덤 포레스트에서 트리를 랜덤하게 만드는 방법은 두 가지 입니다. 1) 트리를 만들 때 사용되는 데이터 포인트를 무작위로 선택하는 방법(Bagging = Bootstrap Aggregation)과 2) 분할 테스트에서 특성을 무작위로 선택(Feature Randomness)하는 방법입니다.1-1. Random Forest: Bagging (=Bootstrap Aggregation)&#182;Bootstrap sample은 더 큰 샘플에서 \"bootstrap\"된 더 작은 샘플입니다. Bootstraping은 동일한 크기의 작은 샘플을 하나의 원래 샘플에서 대량으로 반복적으로 추출하는 리샘플링의 한 유형입니다.통계에 대한 표본 분포를 생성하기 위해 모집단(population)에서 반복되지 않는 큰 표본을 그리는 것이 이상적입니다. 그렇지 않은 작은 데이터에서 bootstrap sample은 모집단 모수(population parameter)에 대해 상당히 좋은 근사치가 될 수 있습니다.Decision tree은 훈련 데이터에 매우 민감합니다. 훈련 데이터를 약간 변경하면 트리 구조가 크게 달라질 수 있습니다. 랜덤 포레스트는 각 개별 트리가 데이터 세트에서 무작위로 대체하여 샘플을 추출할 수 있도록 하여 서로 다른 트리를 생성함으로써 이를 활용합니다. 이 과정을 bagging이라고 합니다.1-2. Random Forest: Feature Randomness&#182;일반적인 의사 결정 트리에서 노드를 분할할 때, 모든 feature을 고려하여 왼쪽 노드와 오른쪽 노드 사이에 불순도가 가장 낮도록 분할합니다. 반면에 랜덤 포리스트의 각 트리는 feature의 random subset에서만 선택할 수 있습니다. 이로 인해 모델의 트리 간에 훨씬 더 많은 변동이 발생하고 궁극적으로 트리 간 상관 관계가 낮아지고 더 다양화됩니다.결론적으로, 이러한 과정을 통해 구성된 랜덤 포레스트는 bagging을 통해 각기 다른 데이터셋에 훈련됨과 동시에 각기 다른 feature을 사용한 여러개의 결정 트리로 이루어지게 됩니다.1-3. Scikit-learn Random Forest&#182;In&nbsp;[&nbsp;]: from sklearn.datasets import make_moonsfrom sklearn.model_selection import train_test_split# two_moon 데이터셋을 가지고 실습합니다.X, y = make_moons(n_samples=200, noise=0.25, random_state=42)Xm_train, Xm_test, ym_train, ym_test = train_test_split(X, y, stratify=y, random_state=42)# two_moon 데이터를 시각화해봅니다.plt.scatter(X[:, 0], X[:, 1], c=y)plt.show() In&nbsp;[&nbsp;]: # 랜덤 포레스트의 결정 경계를 시각화하는 함수입니다.from matplotlib.colors import ListedColormapdef plot_decision_boundary(forest): figure = plt.figure(figsize=(36, 5)) x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5 y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5 xx, yy = np.meshgrid(np.arange(x_min, x_max, .02), np.arange(y_min, y_max, .02)) cm = plt.cm.RdBu cm_bright = ListedColormap([&#39;#FF0000&#39;, &#39;#0000FF&#39;]) for i in range(len(forest.estimators_)): ax = plt.subplot(1, 6, i+1) Z = forest.estimators_[i].predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1] Z = Z.reshape(xx.shape) ax.contourf(xx, yy, Z, cmap=cm, alpha=.8) ax.scatter(Xm_train[:, 0], Xm_train[:, 1], c=ym_train, cmap=cm_bright, edgecolors=&#39;k&#39;) ax.set_xticks(()) ax.set_yticks(()) ax = plt.subplot(1, 6, 6) Z = forest.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1] Z = Z.reshape(xx.shape) ax.contourf(xx, yy, Z, cmap=cm, alpha=.8) ax.scatter(Xm_train[:, 0], Xm_train[:, 1], c=ym_train, cmap=cm_bright, edgecolors=&#39;k&#39;) ax.set_xticks(()) ax.set_yticks(()) scikit-learn RandomForestClassifier 다큐멘테이션In&nbsp;[&nbsp;]: from sklearn.ensemble import RandomForestClassifier# 트리 5개로 구성된 랜덤 포레스트 모델을 만들어보겠습니다.forest_m = RandomForestClassifier(n_estimators=5, random_state=2)forest_m.fit(Xm_train, ym_train) Out[&nbsp;]:RandomForestClassifier(n_estimators=5, random_state=2)In&nbsp;[&nbsp;]: # 주어진 함수를 사용하여 다섯개의 트리와 랜덤 포레스트의 결정 경계를 시각화해봅니다. plot_decision_boundary(forest_m) 다섯 개의 트리가 만든 결정 경계는 확연하게 다르다는 것을 알 수 있습니다. Bootstrap sampling 때문에 한쪽 트리에 나타나는 훈련 포인트가 다른 트리에는 포함되지 않을 수 있어 각 트리는 불완전합니다.랜덤 포레스트는 개개의 트리보다는 덜 과적합되고 훨씬 좋은 결정 경계를 만들어줍니다. 실제 애플리케이션에서는 매우 많은 트리를 사용하기 때문에(수백, 수천 개) 더 부드러운 결정 경계가 만들어집니다.scikit-learn에서 제공하는 유방암 데이터셋에 100개의 트리로 이뤄진 랜덤 포레스트를 적용해보겠습니다.In&nbsp;[&nbsp;]: from sklearn.datasets import load_breast_cancer# 유방암 데이터를 로드하고 train/test셋으로 나눕니다.cancer = load_breast_cancer()Xc_train, Xc_test, yc_train, yc_test = train_test_split(cancer.data, cancer.target, random_state=42) In&nbsp;[&nbsp;]: # 데이터에 어떤 특성이 있는지 살펴봅니다.cancer.feature_names Out[&nbsp;]:array([&#39;mean radius&#39;, &#39;mean texture&#39;, &#39;mean perimeter&#39;, &#39;mean area&#39;, &#39;mean smoothness&#39;, &#39;mean compactness&#39;, &#39;mean concavity&#39;, &#39;mean concave points&#39;, &#39;mean symmetry&#39;, &#39;mean fractal dimension&#39;, &#39;radius error&#39;, &#39;texture error&#39;, &#39;perimeter error&#39;, &#39;area error&#39;, &#39;smoothness error&#39;, &#39;compactness error&#39;, &#39;concavity error&#39;, &#39;concave points error&#39;, &#39;symmetry error&#39;, &#39;fractal dimension error&#39;, &#39;worst radius&#39;, &#39;worst texture&#39;, &#39;worst perimeter&#39;, &#39;worst area&#39;, &#39;worst smoothness&#39;, &#39;worst compactness&#39;, &#39;worst concavity&#39;, &#39;worst concave points&#39;, &#39;worst symmetry&#39;, &#39;worst fractal dimension&#39;], dtype=&#39;&lt;U23&#39;)In&nbsp;[&nbsp;]: # 랜덤 포레스트 분류기를 유방암 데이터에 훈련시킵니다. n_estimators=100, random_state=42forest_c = RandomForestClassifier(n_estimators=100, random_state=42)forest_c.fit(Xc_train, yc_train) Out[&nbsp;]:RandomForestClassifier(random_state=42)In&nbsp;[&nbsp;]: print(&quot;Random Forest Training Set Accuracy: {:.3f}&quot;.format(forest_c.score(Xc_train, yc_train)))print(&quot;Random Forest Test Set Accuracy: {:.3f}&quot;.format(forest_c.score(Xc_test, yc_test))) Random Forest Training Set Accuracy: 1.000Random Forest Test Set Accuracy: 0.965랜덤 포레스트는 아무런 매개변수 튜닝 없이도 선형 모델이나 단일 결정 트리보다 높은 정확도를 내고 있습니다. 단일 결정 트리에서 한 것처럼 max_features 매개변수를 조정하거나 사전 가지치기를 할 수도 있습니다. 하지만 랜덤 포레스트는 기본 설정으로도 좋은 결과를 만들어줄 때가 많습니다.결정 트리처럼 랜덤 포레스트도 특성 중요도를 제공하는데 각 트리의 특성 중요도를 취합하여 계산한 것입니다. 일반적으로 랜덤 포레스트에서 제공하는 특성 중요도가 하나의 트리에서 제공하는 것보다 더 신뢰할 만합니다.In&nbsp;[&nbsp;]: def plot_imp(clf): sorted_idx = clf.feature_importances_.argsort() y_ticks = np.arange(0, len(cancer.feature_names)) fig, ax = plt.subplots() ax.barh(y_ticks, clf.feature_importances_[sorted_idx]) ax.set_yticklabels(cancer.feature_names[sorted_idx]) ax.set_yticks(y_ticks) fig.tight_layout() plt.show() In&nbsp;[&nbsp;]: from sklearn.tree import DecisionTreeClassifier# 랜덤 포레스트와 비교하기 위해 결정트리 분류기를 생성하고 유방암 데이터에 훈련시킵니다.dec_tree = DecisionTreeClassifier(random_state=42)dec_tree.fit(Xc_train, yc_train) Out[&nbsp;]:DecisionTreeClassifier(random_state=42)In&nbsp;[&nbsp;]: print(&quot;Decision Tree Training Set Accuracy: {:.3f}&quot;.format(dec_tree.score(Xc_train, yc_train)))print(&quot;Decision Tree Test Set Accuracy: {:.3f}&quot;.format(dec_tree.score(Xc_test, yc_test))) Decision Tree Training Set Accuracy: 1.000Decision Tree Test Set Accuracy: 0.951In&nbsp;[&nbsp;]: # 하나의 트리에서 제공하는 feature importance를 살펴보겠습니다.dec_tree.feature_importances_ Out[&nbsp;]:array([0. , 0.02601101, 0. , 0. , 0. , 0. , 0. , 0.69593688, 0. , 0. , 0. , 0. , 0. , 0.01277192, 0.00155458, 0. , 0.00670697, 0.01702539, 0. , 0. , 0.0877369 , 0.10787925, 0. , 0.03452044, 0.00985664, 0. , 0. , 0. , 0. , 0. ])In&nbsp;[&nbsp;]: # 결정 트리의 특성 중요도를 시각화해봅니다.plot_imp(dec_tree) In&nbsp;[&nbsp;]: # 랜덤포레스트의 feature importance를 살펴봅니다.forest_c.feature_importances_ Out[&nbsp;]:array([0.03971058, 0.01460399, 0.05314639, 0.04277978, 0.00816485, 0.01140166, 0.08321459, 0.0902992 , 0.00443533, 0.00443395, 0.01951684, 0.00459978, 0.00868228, 0.04355077, 0.00464415, 0.0036549 , 0.00701442, 0.00504716, 0.00371411, 0.00658253, 0.08127686, 0.01649014, 0.07138828, 0.12319232, 0.01033481, 0.01580059, 0.03174022, 0.17229521, 0.01310266, 0.00518165])In&nbsp;[&nbsp;]: # 랜덤 포레스트의 feature importance를 시각화해봅니다.plot_imp(forest_c) 그림에서 알 수 있듯이 랜덤 포레스트에서는 단일 트리의 경우보다 훨신 많은 특성이 0 이상의 중요도 값을 갖습니다. 랜덤 포레스트를 만드는 무작위성은 알고리즘이 가능성 있는 많은 경우를 고려할 수 있도록 하므로, 그 결과 랜덤 포레스트가 단일 트리보다 더 넓은 시각으로 데이터를 바라볼 수 있습니다.2. Ensemble Methods: Bagging and Boosting&#182;2-1. Bagging (=Bootstrap Aggregation)&#182;scikit-learn BaggingClassifierIn&nbsp;[&nbsp;]: from sklearn.linear_model import LogisticRegressionfrom sklearn.ensemble import BaggingClassifier# Bagging을 사용하여 cancer 데이터셋에 LogisticRegression 모델을 100개 훈련하여 앙상블해봅니다.bagging = BaggingClassifier(LogisticRegression(), n_estimators=100, oob_score=True, n_jobs=-1, random_state=42)bagging.fit(Xm_train, ym_train) Out[&nbsp;]:BaggingClassifier(base_estimator=LogisticRegression(), n_estimators=100, n_jobs=-1, oob_score=True, random_state=42)BaggingClassifier은 분류기가 predict_proba() 메서드를 지원하는 경우 확률값을 평균하여 예측을 수행합니다. 그렇지 않은 분류기를 사용할 때는 가장 빈도가 높은 클래스 레이블이 예측 결과가 됩니다.2-1-1. Out-of-Bag error (OOB)&#182;Out-of-Bag error은 기계학습에서 bagging을 사용한 모델의 성능을 측정합니다. OOB error은 모델 훈련에 사용되지 않은 데이터에 대해 계산됩니다. OOB error은 bootstrap 훈련 데이터셋에 OOB 샘플이 포함되지 않은 결정 트리의 하위 집합만 사용하여 계산됩니다.OOB error은 validation score와 다른 성능 측정치입니다. 때때로 데이터 세트의 크기가 충분하지 않기 때문에 validation dataset을 구성하기 어려운 경우가 있습니다. 대규모 데이터가 아니며, 모든 데이터를 훈련 데이터셋으로 사용하려는 경우 OOB error으로 모델의 성능을 판단할 수 있습니다.In&nbsp;[&nbsp;]: print(&quot;Train set accuracy: {:.3f}&quot;.format(bagging.score(Xm_train, ym_train)))print(&quot;Test set accuracy: {:.3f}&quot;.format(bagging.score(Xm_test, ym_test)))# OOB 오차를 출력합니다.print(&quot;OOB sample score: {:.3f}&quot;.format(bagging.oob_score_)) Train set accuracy: 0.813Test set accuracy: 0.880OOB sample score: 0.807In&nbsp;[&nbsp;]: from sklearn.tree import DecisionTreeClassifier#결정 트리로 배깅을 수행하는 것보다 랜덤 포레스트를 사용하는 것이 편리하지만 여기서는 직접 결정 트리에 배깅을 적용해보겠습니다.bagging = BaggingClassifier(DecisionTreeClassifier(), n_estimators=5, n_jobs=-1, random_state=42)bagging.fit(Xm_train, ym_train) Out[&nbsp;]:BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=5, n_jobs=-1, random_state=42)In&nbsp;[&nbsp;]: # 랜덤 포레스트에서처럼 이 배깅 분류기에 있는 결정 트리의 결정 경계를 시각화해보겠습니다.plot_decision_boundary(bagging) 결과 그래프는 랜덤 포레스트의 결정 경계와 매우 비슷합니다.In&nbsp;[&nbsp;]: # `n_estimators=100`으로 늘려서 cancer 데이터셋에 훈련시켜보고 훈련 세트의 테스트 세트 성능을 확인해보겠습니다.bagging = BaggingClassifier(DecisionTreeClassifier(), n_estimators=100, oob_score=True, n_jobs=-1, random_state=42)bagging.fit(Xm_train, ym_train) Out[&nbsp;]:BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=100, n_jobs=-1, oob_score=True, random_state=42)In&nbsp;[&nbsp;]: print(&quot;Train set accuracy: {:.3f}&quot;.format(bagging.score(Xm_train, ym_train)))print(&quot;Test set accuracy: {:.3f}&quot;.format(bagging.score(Xm_test, ym_test)))print(&quot;OOB sample score: {:.3f}&quot;.format(bagging.oob_score_)) Train set accuracy: 1.000Test set accuracy: 0.900OOB sample score: 0.900배깅은 랜덤 포레스트와 달리 max_samples 매개변수에서 부트스트랩 샘플의 크기를 지정할 수 있습니다. 또한 랜덤 포레스트는 DecisionTreeClassifier(splitter=“best”)를 사용하도록 고정되어 있습니다. 결정 트리를 splitter=‘random’으로 설정하면 무작위로 분할한 후보 노드 중에서 최선의 분할을 찾습니다.2-2. Boosting&#182;Adaboost(=Adaptive Boosting)은 ensemble method의 boosting method 중 가장 유명한 방법입니다. 다른 boosting 방법들 중에는 XGBoost, GradientBoost, 그리고 BrownBoost 등이 있습니다. Adaboost은 일련의 단일 모델을 반복적으로 가중치가 부여된 데이터에 학습시킵니다. 그 다음 모든 예측은 가중 다수결 (혹은 합계)를 통해 결합되어 최종 예측을 생성합니다.처음에는 가중치가 모든 데이터 포인트에 $1/N$으로 동일하게 설정됩니다. 순차적으로 훈련을 반복하면서 가중치는 개별적으로 수정되고, 단일 모델은 재조정된 데이터에 다시 훈련됩니다. 잘못 예측된 데이터는 가중치가 증가하며, 올바르게 예측된 데이터는 가중치가 감소합니다. 훈련이 반복될수록 예측하기 어려운 데이터가 더욱 많은 가중치를 받게 됩니다. 이러한 원리 때문에 boosting이라고 불립니다.scikit-learn의 AdaBoostClassifier는 기본적으로 DecisionTreeClassifier(max_depth=1)를 사용하고 AdaBoostRegressor는 decisionTreeRegressor(max_depth=3)를 사용하지만 base_estimator 매개변수에서 다른 모델을 지정할 수도 있습니다. 순차적으로 학습해야 하기 때문에 n_jobs 매개변수를 지원하지 않습니다.scikit-learn AdaBoostClassifierscikit-learn AdaBoostRegressorIn&nbsp;[&nbsp;]: from sklearn.ensemble import AdaBoostClassifier# AdaBoost를 two_moons 데이터에 적용해봅니다.ada = AdaBoostClassifier(n_estimators=5, random_state=42)ada.fit(Xm_train, ym_train) ---------------------------------------------------------------------------NameError Traceback (most recent call last)&lt;ipython-input-1-d7b7492dffe3&gt; in &lt;module&gt; 3 # AdaBoost를 two_moons 데이터에 적용해봅니다. 4 ada = AdaBoostClassifier(n_estimators=5, random_state=42)----&gt; 5 ada.fit(Xm_train, ym_train)NameError: name &#39;Xm_train&#39; is not definedIn&nbsp;[&nbsp;]: # AdaBoost의 각 estimator의 결정 경계를 시각화해봅니다.plot_decision_boundary(ada) AdaBoostClassifier는 깊이가 1인 결정 트리를 사용하기 때문에 각 트리의 결정 경계가 하나의 직선입니다. 앙상블된 결정 경계도 다른 앙상블 모델에 비해 더 단순합니다.In&nbsp;[&nbsp;]: # 이번에는 AdaBoost를 breast cancer 데이터에 적용해봅니다.ada = AdaBoostClassifier(n_estimators=100, random_state=42)ada.fit(Xc_train, yc_train) Out[&nbsp;]:AdaBoostClassifier(n_estimators=100, random_state=42)In&nbsp;[&nbsp;]: print(&quot;Train set accuracy: {:.3f}&quot;.format(ada.score(Xc_train, yc_train)))print(&quot;Test set accuracy: {:.3f}&quot;.format(ada.score(Xc_test, yc_test))) Train set accuracy: 1.000Test set accuracy: 0.944In&nbsp;[&nbsp;]: # AdaBoost의 특성 중요도를 시각화해봅니다.plot_imp(ada) AdaBoost의 특성 중요도를 확인해보면 다른 모델에서 부각되지 않았던 compactness error 특성을 크게 강조하고 있습니다.2-2-1. Bagging vs Boosting&#182;3. Feature Selection&#182;새로운 특성을 만드는 방법이 많으므로 데이터의 차원이 원본 특성의 수 이상으로 증가하기 쉽습니다. 그러나 특성이 추가되면 모델은 더 복잡해지고 과적합될 가능성도 높아집니다. 보통 새로운 특성을 추가할 때나 고차원 데이터셋을 사용할 때, 가장 유용한 특성만 선택하고 나머지는 무시해서 특성의 수를 줄이는 것이 좋습니다. 이렇게 하면 모델이 간단해지고 일반화 성능이 올라갑니다.이를 위한 전략으로 1) 일변량 통계 (univariate statistics), 2) 모델 기반 선택 (model-based selection), 3) 반복적 선택 (iterative selection)이 있습니다. 이 방법들은 모두 지도 학습 방법이므로 최적값을 찾으려면 타깃값이 필요합니다. 그리고 데이터를 훈련 세트와 테스트 세트로 나눈 다음 훈련 데이터만 특성 선택에 사용해야 합니다.3-1. &#51068;&#48320;&#47049; &#53685;&#44228;&#182;일변량 통계에서는 개개의 특성과 타깃 사이에 중요한 통계적 관계가 있는지를 계산합니다. 그런 다음 깊게 관련되어 있다고 판단되는 특성을 선택합니다. 분산분석(ANOVA)이라고도 합니다.이 방식의 핵심 요소는 일변량, 즉 각 특성이 독립적으로 평가된다는 점입니다. 따라서 다른 특성과 깊게 연관된 특성은 선택되지 않을 것입니다. 일변량 분석은 계산이 매우 빠르고 평가를 위해 모델을 만들 필요가 없습니다. 한편으로 이 방식은 특성을 선택한 후 적용하려는 모델에 상관없이 사용할 수 있습니다.scikit-learn에서 일변량 분석으로 특성을 선택하려면 분류에서는 f-classif을, 회귀에서는 f_regression을 보통 선택하여 테스트하고, 계산한 p-value에 기초하여 특성을 제외하는 방식을 선택합니다. 이런 방식들은 매우 높은 p-value를 가진 특성(즉, 타깃값과 연관성이 작은 특성)을 제외할 수 있도록 임계값을 조정하는 매개변수를 사용합니다. 임계값을 계산하는 방법은 각각 다르며, 가장 간단한 SelectKBest는 고정된 k개의 특성을 선택하고, SelectPercentile은 지정된 비율만큼 특성을 선택합니다. 그럼 cancer 데이터셋에 분류를 위한 특성 선택을 적용해보겠습니다. 문제를 조금 복잡하게 하기위해 의미 없는 노이즈 특성을 데이터에 추가하겠습니다. 특성 선택이 이 의미 없는 특성을 식별해서 제거하는지 보겠습니다.scikit-learn SelectPercentilescikit-learn f_classifIn&nbsp;[&nbsp;]: from sklearn.datasets import load_breast_cancerfrom sklearn.feature_selection import SelectPercentile, f_classiffrom sklearn.model_selection import train_test_splitcancer = load_breast_cancer()# 고정된 난수를 발생시킵니다.rng = np.random.RandomState(42)noise = rng.normal(size=(len(cancer.data), 50))# 데이터에 노이즈 특성을 추가합니다. 처음 30개는 원본 특성이고 다음 50개는 노이즈입니다.X_w_noise = np.hstack([cancer.data, noise]) In&nbsp;[&nbsp;]: # 원 데이터와 노이즈가 추가된 데이터의 크기를 비교해봅니다.print(cancer.data.shape)print(X_w_noise.shape) (569, 30)(569, 80)In&nbsp;[&nbsp;]: # 데이터를 train/test셋으로 나눕니다. random_state=42, test_size=0.5X_train, X_test, y_train, y_test = train_test_split(X_w_noise, cancer.target, random_state=42, test_size=0.5) In&nbsp;[&nbsp;]: # f_classif(기본값)과 SelectPercentile을 사용하여 특성의 50%를 선택합니다.select = SelectPercentile(score_func=f_classif, percentile=50)select.fit(X_train, y_train)# 훈련 세트에 적용합니다.X_train_selected = select.transform(X_train) In&nbsp;[&nbsp;]: print(&quot;X_train.shape&quot;, X_train.shape)print(&quot;X_train_selected.shape&quot;, X_train_selected.shape) X_train.shape (284, 80)X_train_selected.shape (284, 40)결과에서 볼 수 있듯이 특성 개수가 80개에서 40개로 줄었습니다. (원본 특성의 50%)In&nbsp;[&nbsp;]: # `get_support` 메서드는 선택된 특성을 boolean 값으로 표시해줍니다. 어떤 특성이 선택되었는지 확인해봅니다.mask = select.get_support()print(mask) [ True True True True True True True True True True True False True True False True True True False False True True True True True True True True True True False False False True False True False False False False False False False True False False False True False False False False False False True False False False True False False False True False False False True True True False False False True True False False False False True True]In&nbsp;[&nbsp;]: # 위 마스크를 시각화해봅니다.plt.matshow(mask.reshape(1, -1), cmap=&#39;gray_r&#39;)plt.xlabel(&quot;feature number&quot;)plt.yticks([0]) Out[&nbsp;]:([&lt;matplotlib.axis.YTick at 0x7f0abe733590&gt;], &lt;a list of 1 Text major ticklabel objects&gt;) 마스킹된 그래프에서 볼 수 있듯이 선택된 특성은 대부분 원본 특성이고 노이즈 특성이 거의 모두 제거되었습니다. 그러나 원본 특성이 완벽하게 복원된 것은 아닙니다. 전체 특성을 이용했을 때와 선택된 특성만 사용했을 때 로지스틱 회귀의 성능을 비교해보겠습니다.In&nbsp;[&nbsp;]: from sklearn.linear_model import LogisticRegression# 테스트 데이터 변환X_test_selected = select.transform(X_test)# LogisticRegression모델을 생성하고 원본 데이터에 훈련시킨 성능과 중요한 특성들이 선택된 데이터에 훈련시킨 성능을 비교해봅니다.lr = LogisticRegression(max_iter=10000)lr.fit(X_train, y_train)print(&quot;전체 특성을 사용한 정확도&quot;, lr.score(X_test, y_test))lr.fit(X_train_selected, y_train)print(&quot;선택된 일부 특성을 사용한 정확도&quot;, lr.score(X_test_selected, y_test)) 전체 특성을 사용한 정확도 0.9614035087719298선택된 일부 특성을 사용한 정확도 0.9649122807017544이 경우에서는 일부 원본 특성이 없더라도 노이즈 특성을 제거한 쪽의 성능이 더 높습니다. 이 예는 인위적으로 간단하게 만든 예제이고 실제 데이터에서의 결과는 보통 엇갈리는 경우도 많습니다. 하지만 너무 많은 특성때문에 모델을 만들기가 현실적으로 어려울 때 일변량 분석으로 사용하여 특성을 선택하면 큰 도움이 될 수 있습니다. 또는 많은 특성들이 확실히 도움이 안 된다고 생각될 때 사용할 수 있습니다.3-1-1. Selecting K best features&#182;SelectPercentile 말고도 K 개의 중요한 특성을 선택하는 방법이 있습니다. SelectKBest를 이용하면 가능합니다. SelectKBest는 사용자가 지정한 k개의 중요한 특성을 반환해줍니다. 다만 위에서 사용한 데이터에는 음의 값이 있기 때문에 SelectKBest를 사용할 수 없습니다. 따라서 이 예제는 Iris 데이터를 사용합니다.scikit-learn SelectKBestscikit-learn chi2In&nbsp;[&nbsp;]: from sklearn.datasets import load_irisiris_X, iris_y = load_iris(return_X_y=True)iris_X.shape Out[&nbsp;]:(150, 4)In&nbsp;[&nbsp;]: from sklearn.feature_selection import SelectKBest, chi2# SelectKBest와 chi2 테스트를 사용하여 iris data에서 두개의 가장 중요한 특성을 선택합니다.X_new = SelectKBest(chi2, k=2).fit_transform(iris_X, iris_y)print(X_new.shape) (150, 2)3.2 &#47784;&#45944; &#44592;&#48152; &#53945;&#49457; &#49440;&#53469;&#182;모델 기반 특성 선택은 지도 학습 머신러닝 모델을 사용하여 특성의 중요도를 평가해서 가장 중요한 특성들만 선택합니다. 특성 선택에 사용되는 지도 학습 모델은 최종적으로 사용할 지도 학습 모델과 같을 필요는 없습니다. 특성 선택을 위한 모델은 각 특성의 중요도를 측정하여 순서를 매길 수 있어야 합니다. 결정 트리와 이를 기반으로 한 모델은 각 특성의 중요도가 담겨있는 feature_importance_ 속성을 제공합니다. 일변량 분석과는 반대로 모델 기반 특성 선택은 한번에 모든 특성을 고려하므로 (사용된 모델이 상호작용을 잡아낼 수 있다면) 상호작용 부분을 반영할 수 있습니다. 모델 기반의 특성 선택은 SelectFromModel에 구현되어 있습니다.scikit-learn SelectFromModelIn&nbsp;[&nbsp;]: from sklearn.feature_selection import SelectFromModelfrom sklearn.ensemble import RandomForestClassifier# RandomForestClassifier와 SelectFromModel을 이용하여 중요한 특성을 선택합니다.select = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=42), threshold=&#39;median&#39;) SelectFromModel은 지도 학습 모델로 계산된 중요도가 지정한 임계치보다 큰 모든 특성을 선택합니다. 일변량 분석으로 선택한 특성과 결과를 비교하기 위해 절반 가량의 특성이 선택될 수 있도록 중간값을 임계치로 사용하겠습니다. 트리 100개로 만든 랜덤포레스트 분류기를 사용해 특성 중요도를 계산합니다. 이는 매우 복잡한 모델이고 일변량 분석보다는 훨씬 강력한 방법입니다.In&nbsp;[&nbsp;]: # 모델을 데이터에 훈련시키고 선택된 특성의 shape을 출력합니다.select.fit(X_train, y_train)X_train_l1 = select.transform(X_train)print(&quot;X_train.shape:&quot;, X_train.shape)print(&quot;X_train_l1.shape:&quot;, X_train_l1.shape) X_train.shape: (284, 80)X_train_l1.shape: (284, 40)In&nbsp;[&nbsp;]: # 선택된 특성을 같은 방식으로 그려보겠습니다.mask = select.get_support()plt.matshow(mask.reshape(1, -1), cmap=&#39;gray_r&#39;)plt.xlabel(&quot;featue number&quot;) Out[&nbsp;]:Text(0.5, 0, &#39;featue number&#39;) 이번에는 두 개를 제외한 모든 원본 특성이 선택되었습니다. 특성을 40개 선택하도록 지정했으므로 일부 노이즈 특성도 선택되었습니다.In&nbsp;[&nbsp;]: # LogisticRegression 모델로 성능이 얼마나 향상되었는지 확인해봅니다.X_test_l1 = select.transform(X_test)score = LogisticRegression(max_iter=10000).fit(X_train_l1, y_train).score(X_test_l1, y_test)print(&quot;test score&quot;, score) test score 0.95438596491228073.3 &#48152;&#48373;&#51201; &#53945;&#49457; &#49440;&#53469;&#182;반복적 특성 선택(iterative feature selection)에서는 특성의 수가 각기 다른 일련의 모델이 만들어집니다. 기본적으로 두가지 방법이 있습니다.특성을 하나도 선택하지 않은 상태로 시작해서 어떤 종료 조건에 도달할 때 까지 하나씩 추가하는 방법모든 특성을 가지고 시작해서 어떤 종료 조건이 될 때까지 특성을 하나씩 제거해가는 방법일련의 모델이 만들어지기 때문에 이 방법은 앞서 소개한 방법들보다 계산 비용이 월씬 많이 듭니다. 재귀적 특성 제거 (Recursive Feature Elimination, RFE)가 이런 방법의 하나입니다. 이 방법은 모든 특성으로 시작해서 모델을 만들고 특성 중요도가 가장 낮은 특성을 제거합니다. 그런 다음 제거한 특성을 빼고 나머지 특성 전체로 새로운 모델을 만듭니다. 이런 식으로 미리 정의한 특성 개수가 남을 때까지 계속합니다. 이를 위해 모델 기반 선택에서처럼 특성 선택에 사용할 모델은 특성의 중요도를 결정하는 방법을 제공해야 합니다. 다음은 앞에서와 같은 랜덤 포레스트 모델을 사용합니다.In&nbsp;[&nbsp;]: from sklearn.feature_selection import RFE# RFE를 생성하고 데이터에 훈련시킵니다.select = RFE(RandomForestClassifier(n_estimators=100, random_state=42), n_features_to_select=40)select.fit(X_train, y_train) In&nbsp;[&nbsp;]: # 선택된 특성을 시각화합니다.mask = select.get_support()plt.matshow(mask.reshape(1, -1), cmap=&#39;gray_r&#39;)plt.xlabel(&quot;feature number&quot;) 일변량 분석이나 모델 기반 선택보다 특성 선택이 나아졌지만, 여전히 특성 한개를 놓쳤습니다. 랜덤포레스트 모델은 특성이 누락될 때마다 다시 학습하므로 40번이나 실행됩니다. 그래서 이 코드를 실행하면 모델 기반 선택보다 훨씬 오래 걸립니다.In&nbsp;[&nbsp;]: # RFE를 사용해서 특성을 선택했을 때 로지스틱 회귀의 정확도를 확인해보겠습니다.X_train_rfe = select.transform(X_train)X_test_rfe = select.transform(X_test)score = LogisticRegression(max_iter=10000).fit(X_train_rfe, y_train).score(X_test_rfe, y_test)print(&quot;test score&quot;, score) In&nbsp;[&nbsp;]: # 또한 RFE에 사용된 모델을 이용해서도 예측을 할 수 있습니다. 이 경우 선택된 특성만 사용됩니다.score = select.score(X_test, y_test)print(score) RFE안에 있는 랜덤 포레스트의 성능이 이 모델에서 선택한 특성으로 만든 로지스틱 회귀의 성능과 비슷합니다. 다른 말로 하면, 특성 선택이 제대로 되면 선형 모델의 성능은 랜덤 포레스트와 견줄만 합니다.머신 러닝 알고리즘에 어떤 입력값을 넣을지 확신이 안선다면 특성 자동 선택이 도움이 될 수 있습니다. 또한 예측속도를 높이거나 해석하기 더 쉬운 모델을 만드는 데 필요한 만큼 특성의 수를 줄이는 데도 효과적입니다."
    } ,
  
    {
      "title"       : "Machine Learning - Simple Regression",
      "category"    : "",
      "tags"        : "machine learning, study_model, linear regression",
      "url"         : "./MachineLearning_Simple.html",
      "date"        : "2021-03-24 00:00:00 +0900",
      "description" : "ML Basic",
      "content"     : "Simple Regression &#49892;&#49845; 1&#52264;&#49884; - ZeroR, OneR, Naive Bayes Classifier&#182;&#49892;&#49845; &#45236;&#50857;:&#182;ZeroROneRNaive Bayes classifierIn&nbsp;[&nbsp;]: import numpy as npimport pandas as pd In&nbsp;[&nbsp;]: import sklearnprint(sklearn.__version__) 1.0.2&#49892;&#49845; &#45936;&#51060;&#53552;&#182;In&nbsp;[&nbsp;]: # 데이터 받기url = &quot;https://raw.githubusercontent.com/inikoreaackr/ml_datasets/main/playgolf.csv&quot;df = pd.read_csv(url) In&nbsp;[&nbsp;]: # 데이터 첫 다섯 instance 확인df.head() Out[&nbsp;]: OUTLOOK TEMPERATURE HUMIDITY WINDY PLAY GOLF 0 Rainy Hot High False No 1 Rainy Hot High True No 2 Overcast Hot High False Yes 3 Sunny Mild High False Yes 4 Sunny Cool Normal False Yes &lt;svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\" width=\"24px\"&gt; &lt;/svg&gt; In&nbsp;[&nbsp;]: # 데이터 타입 확인df.dtypes Out[&nbsp;]:OUTLOOK objectTEMPERATURE objectHUMIDITY objectWINDY boolPLAY GOLF objectdtype: objectIn&nbsp;[&nbsp;]: # object 타입을 category로 변경for col in df.columns: df[col] = df[col].astype(&#39;category&#39;) In&nbsp;[&nbsp;]: # 변경이 되었는지 확인df.dtypes Out[&nbsp;]:OUTLOOK categoryTEMPERATURE categoryHUMIDITY categoryWINDY categoryPLAY GOLF categorydtype: object1. ZeroR&#182;ZeroR은 가장 간단한 분류 방법이며, 다른 모든 feature들을 무시하고 label에만 의존합니다.ZeroR 분류기는 단순히 데이터의 class를 다수 카테고리로 예측합니다.ZeroR에는 예측 능력이 없지만, 이것은 표준 성능을 가늠하여 다른 분류 방법 성능의 기준점이 됩니다.In&nbsp;[&nbsp;]: # PLAY GOLF feature 출력df[&#39;PLAY GOLF&#39;] Out[&nbsp;]:0 No1 No2 Yes3 Yes4 Yes5 No6 Yes7 No8 Yes9 Yes10 Yes11 Yes12 Yes13 NoName: PLAY GOLF, dtype: categoryCategories (2, object): [&#39;No&#39;, &#39;Yes&#39;]In&nbsp;[&nbsp;]: # PLAY GOLF는 binary 변수입니다. 각 카테고리의 갯수를 세어봅니다.df[&#39;PLAY GOLF&#39;].value_counts(sort = True) Out[&nbsp;]:Yes 9No 5Name: PLAY GOLF, dtype: int64In&nbsp;[&nbsp;]: # 이 데이터셋에서 &quot;Play Golf = Yes&quot;로 예측하는 ZeroR 모델의 정확도를 계산해봅니다.9 / (9 + 5) Out[&nbsp;]:0.6428571428571429 &lt;img src=https://www.saedsayad.com/images/ZeroR_3.png&gt;위의 데이터셋에서 \"Play Golf = Yes\"로 예측하는 ZeroR모델의 정확도는 0.64가 됩니다.OneR&#182;OneR은 One Rule의 약자이며, 간단하고 정확한 분류 알고리즘입니다.OneR은 데이터의 각 feature 마다 하나의 룰 셋(Rule Set)을 생성합니다. 그리고 생성된 룰 셋 중에서, 전체데이터에 대해 오차가 가장 작은 룰 셋을 One Rule로 결정합니다.각 feature당 룰 셋은 frequency table을 이용하여 만들 수 있습니다.OneR Algorithm각 feature 마다, 각 feature의 value 마다, 룰을 아래와 같이 만듭니다. 그 feature의 value에 해당되는 instance중에 target class가 몇개인지 셉니다. 가장 갯수가 많은 class를 찾습니다. 그 feature의 value가 해당되면 그 갯수가 많은 class로 예측되도록 룰을 하나 만듭니다. 각 feature의 룰들의 전체 에러를 계산합니다. (반대로 정확도를 계산할 수도 있습니다.)가장 작은 에러를 보이는 feature을 선택합니다.아래 그림에서는 outlook과 humidity feature 모두 에러의 갯수가 4이므로 제일 작습니다. 하지만 활동에서는 첫번째 feature인 outlook만 고려할 것입니다.For example:&lt;img src=https://www.saedsayad.com/images/OneR_1.png&gt; In&nbsp;[&nbsp;]: # 수도코드 구현from collections import Countertotal_errors = []for col in df.columns[:-1]: error = 0 for val in df[col].unique(): length = len(df[df[col] == val]) print(f&quot;{col} : {val}, length : {length}&quot;) print(Counter(df[df[col] == val][&#39;PLAY GOLF&#39;]).most_common()) error += (length - Counter(df[df[col] == val][&#39;PLAY GOLF&#39;]).most_common()[0][1]) print(f&quot;\\nerror of {col}: [{error}] \\n&quot;) total_errors.append(error) OUTLOOK : Rainy, length : 5[(&#39;No&#39;, 3), (&#39;Yes&#39;, 2)]OUTLOOK : Overcast, length : 4[(&#39;Yes&#39;, 4)]OUTLOOK : Sunny, length : 5[(&#39;Yes&#39;, 3), (&#39;No&#39;, 2)]error of OUTLOOK: [4] TEMPERATURE : Hot, length : 4[(&#39;No&#39;, 2), (&#39;Yes&#39;, 2)]TEMPERATURE : Mild, length : 6[(&#39;Yes&#39;, 4), (&#39;No&#39;, 2)]TEMPERATURE : Cool, length : 4[(&#39;Yes&#39;, 3), (&#39;No&#39;, 1)]error of TEMPERATURE: [5] HUMIDITY : High, length : 7[(&#39;No&#39;, 4), (&#39;Yes&#39;, 3)]HUMIDITY : Normal, length : 7[(&#39;Yes&#39;, 6), (&#39;No&#39;, 1)]error of HUMIDITY: [4] WINDY : False, length : 8[(&#39;Yes&#39;, 6), (&#39;No&#39;, 2)]WINDY : True, length : 6[(&#39;No&#39;, 3), (&#39;Yes&#39;, 3)]error of WINDY: [5] In&nbsp;[&nbsp;]: # 오류가 가장 작은 feature를 고릅니다.best_feature = df.columns[np.argmin(total_errors)]print(best_feature) OUTLOOKIn&nbsp;[&nbsp;]: # best feature에 대해 룰셋을 생성합니다.oneRules = []for val in df[best_feature].unique(): print(f&quot;{best_feature} : {val}&quot;, &quot;-&gt; &quot;, end = &#39; &#39;) print(Counter(df[df[best_feature] == val][&#39;PLAY GOLF&#39;]).most_common()[0][0]) oneRules.append((best_feature, val, Counter(df[df[best_feature] == val][&#39;PLAY GOLF&#39;]).most_common()[0][0])) OUTLOOK : Rainy -&gt; NoOUTLOOK : Overcast -&gt; YesOUTLOOK : Sunny -&gt; YesThe best feature is:&lt;img src=https://www.saedsayad.com/images/OneR_3.png&gt;Naive Bayes Classifier with scikit-learn&#182;scikit-learn의 Naive Bayes classifier 다큐멘테이션: https://scikit-learn.org/stable/modules/naive_bayes.htmlIn&nbsp;[&nbsp;]: df.head() Out[&nbsp;]: OUTLOOK TEMPERATURE HUMIDITY WINDY PLAY GOLF 0 Rainy Hot High False No 1 Rainy Hot High True No 2 Overcast Hot High False Yes 3 Sunny Mild High False Yes 4 Sunny Cool Normal False Yes &lt;svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\" width=\"24px\"&gt; &lt;/svg&gt; In&nbsp;[&nbsp;]: df.describe() Out[&nbsp;]: OUTLOOK TEMPERATURE HUMIDITY WINDY PLAY GOLF count 14 14 14 14 14 unique 3 3 2 2 2 top Rainy Mild High False Yes freq 5 6 7 8 9 &lt;svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\" width=\"24px\"&gt; &lt;/svg&gt; In&nbsp;[&nbsp;]: # 카테고리 데이터를 정수로 인코딩df_enc = pd.DataFrame()df_enc[&#39;OUTLOOK&#39;] = df[&#39;OUTLOOK&#39;].cat.codesdf_enc[&#39;TEMPERATURE&#39;] = df[&#39;TEMPERATURE&#39;].cat.codesdf_enc[&#39;HUMIDITY&#39;] = df[&#39;HUMIDITY&#39;].cat.codesdf_enc[&#39;WINDY&#39;] = df[&#39;WINDY&#39;].cat.codesdf_enc[&#39;PLAY GOLF&#39;] = df[&#39;PLAY GOLF&#39;].cat.codes In&nbsp;[&nbsp;]: df_enc.head() Out[&nbsp;]: OUTLOOK TEMPERATURE HUMIDITY WINDY PLAY GOLF 0 1 1 0 0 0 1 1 1 0 1 0 2 0 1 0 0 1 3 2 2 0 0 1 4 2 0 1 0 1 &lt;svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\" width=\"24px\"&gt; &lt;/svg&gt; In&nbsp;[&nbsp;]: # 인코딩된 데이터의 타입을 프린트해봅니다.df_enc.dtypes Out[&nbsp;]:OUTLOOK int8TEMPERATURE int8HUMIDITY int8WINDY int8PLAY GOLF int8dtype: objectIn&nbsp;[&nbsp;]: # 분류기에 넣을 feature과 해당 label을 구분합니다.features = df_enc.drop(columns=[&#39;PLAY GOLF&#39;])label = df_enc[&#39;PLAY GOLF&#39;] In&nbsp;[&nbsp;]: from sklearn.naive_bayes import CategoricalNBmodel = CategoricalNB() In&nbsp;[&nbsp;]: model.fit(features.values, label) Out[&nbsp;]:CategoricalNB()In&nbsp;[&nbsp;]: score = model.score(features.values, label)score Out[&nbsp;]:0.9285714285714286In&nbsp;[&nbsp;]: # p(x_i|y_i) 출력from pprint import pprintfeature_log_prior = model.feature_log_prob_for feature_prior in feature_log_prior: pprint(np.exp(feature_prior)) array([[0.125 , 0.5 , 0.375 ], [0.41666667, 0.25 , 0.33333333]])array([[0.25 , 0.375 , 0.375 ], [0.33333333, 0.25 , 0.41666667]])array([[0.71428571, 0.28571429], [0.36363636, 0.63636364]])array([[0.42857143, 0.57142857], [0.63636364, 0.36363636]])In&nbsp;[&nbsp;]: # p(y_j) 출력np.exp(model.class_log_prior_) Out[&nbsp;]:array([0.35714286, 0.64285714])In&nbsp;[&nbsp;]: # instances에 대해서 예측을 해봅니다. # (&quot;Sunny&quot;, &quot;Hot&quot;, &quot;Normal&quot;, False) [2, 1, 1, 0]# (&quot;Rainy&quot;, &quot;Mild&quot;, &quot;High&quot;, False) [1, 2, 0, 0]print(model.predict_proba([[2, 1, 1, 0]]), model.predict([[2, 1, 1, 0]]))print(model.predict_proba([[1, 2, 0, 0]]), model.predict([[1, 2, 0, 0]])) [[0.22086561 0.77913439]] [1][[0.5695011 0.4304989]] [0]1. &#44592;&#44228;&#54617;&#49845; &#44592;&#48376; &#50857;&#50612; &#51221;&#51032;/&#51032;&#48120;&#182;기계학습: 인공지능의 한 분야로 사람의 학습과 같은 능력을 컴퓨터를 통해 실현하고자 하는 기술로, 데이터로부터 모델을 만들어내는 과정을 의미한다.Label (=target): label은 model이 예측하려는 값으로 training을 한 후의 output이다. 데이터를 차별화 할 수 있는 범주이다.class: 데이터를 분류하는 범주Features: feature은 training data의 분석 대상이 되는 속성들로 input set에 있는 column으로 표현된다.Input: input은 일반적으로 model에 전당되는 데이터 집합(X)를 나타낸다. 예를 들어 (X,Y(label)) 형태의 데이터 세트에서 X는 입력이며 레이블인 Y는 대상 또는 출력이 된다.Numerical data (=Quantitative data): Numerical data는 숫자로 표현되는 것을 의미한다.Categorical data (=Qualitative data): Categorical data는 일반적으로 숫자로 표현되지 않는 것을 의미하며 이산적인 형태의 데이터를 표현하기 위해 사용된다.Unlabeled example (=instance): Unlabeled example은 주로 unsupervised learning에 사용되는 데이터로 의미 있는 label이 존재하지 않는다. training data의 한 예시로 예상되는 결과에 대해 정보를 내포하지 않은 데이터이다.Labeled example:labeled example은 주로 supervised learning에 사용되는 데이터로 의미 있는 label이나 class를 가지고 있다.Training (=learning): 주로 성능의 향상을 의미한다. 경험(data)에 따라 예측하려는 값에 대해 배우는 것으로 명시적인 지시가 아닌 경험에 의해 작업의 성능이 향상되는 것을 의미한다.Predict (=inference): prediction이란 과거의 data set에 대한 training을 거쳐 특정 결과의 가능성을 예측하는 model의 출력을 의미한다.Train Set: Train set은 기계 학습 model을 훈련하는데 사용되는 데이터이다.Test Set: Test set은 학습된 모델을 테스트하기 위한 data의 하위 set이다.Regression: feature와 outcome 간의 관계를 이해하기 위한 방법으로 이를 통해 관계가 추정되면 결과를 예측할 수 있게 된다. 기계학습에서는 일반적으로 best fit을 그리는 작업을 의미하며 각 point 사이의 거리를 최소화하는 방법을 통해 best fit을 찾는다.Error (=loss): model의 오류 합계를 나타내는 값으로 model이 얼마나 잘 훈련되었는지를 판단한다. loss가 크면 model이 제대로 작동하지 않는다는 것을 의미한다.Classification: classification은 input의 주어진 data에 대하여 특정 class label을 예측하는 model을 의미한다.Accuracy: Accuracy는 model이 얼마나 잘 예측하고 있는지를 측정한다.2. &#51201;&#50857; &#44284;&#51228;&#182;Data documentation: Car Evaluation, BalloonsCar Evaluation의 feature: buying, maint, doors, persons, lung_boot, satefyCar Evalaution의 label: acceptBalloons의 feature: color, size, act, ageBalloons의 label: inflated2.1 pandas&#182;아래 data url을 통해 각 데이터마다 dataframe을 생성합니다.Car Evalaution의 모든 column name을 출력하시오.Car Evaluation의 buying feature에 어떤 카테고리가 있는지 출력하시오.Car Evaluation의 accept label의 각 class와 해당 class의 instance 개수를 메소드 하나로 출력하시오.Balloons에서 color feature이 yellow인 instance을 모두 출력하시오.In&nbsp;[&nbsp;]: # load datacar_url = &#39;https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data&#39;balloons_url = &#39;https://archive.ics.uci.edu/ml/machine-learning-databases/balloons/adult+stretch.data&#39; In&nbsp;[&nbsp;]: import numpy as npimport pandas as pdimport sklearn In&nbsp;[&nbsp;]: # 데이터 프레임 생성car_df = pd.read_csv(car_url, header = None)car_df.columns = [&quot;buying&quot;, &quot;maint&quot;, &quot;doors&quot;, &quot;persons&quot;, &quot;lung_boot&quot;, &quot;satefy&quot;, &quot;accept&quot;]bal_df = pd.read_csv(balloons_url, header = None)bal_df.columns = [&quot;color&quot;, &quot;size&quot;, &quot;act&quot;, &quot;age&quot;, &quot;inflated&quot;] In&nbsp;[&nbsp;]: # 데이터의 column name 출력print(car_df.columns.to_list()[:-1])print(bal_df.columns.to_list()[:-1]) [&#39;buying&#39;, &#39;maint&#39;, &#39;doors&#39;, &#39;persons&#39;, &#39;lung_boot&#39;, &#39;satefy&#39;][&#39;color&#39;, &#39;size&#39;, &#39;act&#39;, &#39;age&#39;]In&nbsp;[&nbsp;]: # Car evaluation &#39;buying&#39; feature 의 카테고리 출력print(car_df[&#39;buying&#39;].unique()) [&#39;vhigh&#39; &#39;high&#39; &#39;med&#39; &#39;low&#39;]In&nbsp;[&nbsp;]: # Car evaluation &#39;accept&#39; label 의 각 class 와 instance 개수car_df[&#39;accept&#39;].value_counts(sort = True) Out[&nbsp;]:unacc 1210acc 384good 69vgood 65Name: accept, dtype: int64In&nbsp;[&nbsp;]: # Balloons에서 &#39;color&#39; feature이 yellow인 instance 출력bal_df[&#39;color&#39;].value_counts(sort = True)print(bal_df.loc[(bal_df[&#39;color&#39;] == &#39;YELLOW&#39;)]) color size act age inflated0 YELLOW SMALL STRETCH ADULT T1 YELLOW SMALL STRETCH ADULT T2 YELLOW SMALL STRETCH CHILD F3 YELLOW SMALL DIP ADULT F4 YELLOW SMALL DIP CHILD F5 YELLOW LARGE STRETCH ADULT T6 YELLOW LARGE STRETCH ADULT T7 YELLOW LARGE STRETCH CHILD F8 YELLOW LARGE DIP ADULT F9 YELLOW LARGE DIP CHILD F2.2 &#45936;&#51060;&#53552; &#51060;&#54644; &#48143; &#51204;&#52376;&#47532;&#182;데이터 정보를 읽고 각 feature이 무슨 의미인지 파악하여 서술합니다. 실습했던 내용을 바탕으로 Car Evaluation 데이터와 Balloons 데이터를 scikit-learn의 Categorical Naive Bayesian Classifier에 적합하도록, Object 타입을 정수형으로 전처리합니다.2.3 &#47784;&#45944; &#49373;&#49457;, &#54984;&#47144; &#48143; &#44208;&#44284; &#54644;&#49437;&#182;scikit-learn 패키지를 사용하여 car와 balloons 두 데이터에:Categorical Naive Bayesian Classifier을 fit합니다.두 데이터에 대하여 score를 출력합니다.각 class probability와 각 feature probability을 출력합니다.본인이 임의로 만든 두개의 각기 다른 instances에 대하여 예측을 출력합니다. (car 두 개, balloons 두 개, 총 네 개)모델 예측 결과를 데이터의 맥락으로 해설합니다. (ex. 자동차1의 가격이 높고 보수비용이 낮으며... 할 때, 모델은 자동차1의 평가를 매우 좋음으로 예측하였다.)Car Evaluation의 feature: buying, maint, doors, persons, lung_boot, satefybuying :차량의 구매 가격 maint : 차량을 유지보수하기 위한 가격doors : 문의 개수persons : 차량이 운반할 수 있는 사람의 수 (탑승인원)lung_boot : 차량 트렁크의 크기safety : 안전 측정에서 평가된 차량 안전Car Evalaution의 label: accept** accept 차량의 용인 가능성(구매 가능성)Balloons의 feature: color, size, act, agecolor : 풍선의 색상size : 풍선의 크기act : 풍선을 늘어나게 했는지 줄어들게 했는지 여부age : 나이 / 어른 아이 여부 Balloons의 label: inflated** inflated 풍선이 부풀려진 여부In&nbsp;[&nbsp;]: # Object 타입을 정수형으로 전처리car_df_enc = pd.DataFrame()bal_df_enc = pd.DataFrame()# Car evaluationfor col in car_df.columns: car_df[col] = car_df[col].astype(&#39;category&#39;)car_df_enc[&#39;buying&#39;] = car_df[&#39;buying&#39;].cat.codescar_df_enc[&#39;maint&#39;] = car_df[&#39;maint&#39;].cat.codescar_df_enc[&#39;doors&#39;] = car_df[&#39;doors&#39;].cat.codescar_df_enc[&#39;persons&#39;] = car_df[&#39;persons&#39;].cat.codescar_df_enc[&#39;lung_boot&#39;] = car_df[&#39;lung_boot&#39;].cat.codescar_df_enc[&#39;satefy&#39;] = car_df[&#39;satefy&#39;].cat.codescar_df_enc[&#39;accept&#39;] = car_df[&#39;accept&#39;].cat.codes# Balloonsfor col in bal_df.columns: bal_df[col] = bal_df[col].astype(&#39;category&#39;)bal_df_enc[&#39;color&#39;] = bal_df[&#39;color&#39;].cat.codesbal_df_enc[&#39;size&#39;] = bal_df[&#39;size&#39;].cat.codesbal_df_enc[&#39;act&#39;] = bal_df[&#39;act&#39;].cat.codesbal_df_enc[&#39;age&#39;] = bal_df[&#39;age&#39;].cat.codesbal_df_enc[&#39;inflated&#39;] = bal_df[&#39;inflated&#39;].cat.codes In&nbsp;[&nbsp;]: from sklearn.naive_bayes import CategoricalNBcar_model = CategoricalNB()bal_model = CategoricalNB() In&nbsp;[&nbsp;]: # Car evaluation fit &amp; score 출력car_features = car_df_enc.drop(columns=[&#39;accept&#39;])car_label = car_df_enc[&#39;accept&#39;]car_model.fit(car_features.values, car_label)car_score = car_model.score(car_features.values, car_label)car_score Out[&nbsp;]:0.8715277777777778In&nbsp;[&nbsp;]: # Balloons fit &amp; score 출력bal_features = bal_df_enc.drop(columns=[&#39;inflated&#39;])bal_label = bal_df_enc[&#39;inflated&#39;]bal_model.fit(bal_features.values, bal_label)bal_score = bal_model.score(bal_features, bal_label)bal_score /usr/local/lib/python3.8/dist-packages/sklearn/base.py:443: UserWarning: X has feature names, but CategoricalNB was fitted without feature names warnings.warn( Out[&nbsp;]:1.0In&nbsp;[&nbsp;]: # class probability&amp; feature probability 출력# Car evaluationfrom pprint import pprintcar_feature_log_prior = car_model.feature_log_prob_for featue_prior in car_feature_log_prior: pprint(np.exp(featue_prior))print(np.exp(car_model.class_log_prior_)) array([[0.28092784, 0.23195876, 0.29896907, 0.18814433], [0.01369863, 0.64383562, 0.32876712, 0.01369863], [0.26771005, 0.21334432, 0.22158155, 0.29736409], [0.01449275, 0.57971014, 0.39130435, 0.01449275]])array([[0.27319588, 0.23969072, 0.29896907, 0.18814433], [0.01369863, 0.64383562, 0.32876712, 0.01369863], [0.25947282, 0.22158155, 0.22158155, 0.29736409], [0.20289855, 0.39130435, 0.39130435, 0.01449275]])array([[0.21134021, 0.25773196, 0.26546392, 0.26546392], [0.21917808, 0.26027397, 0.26027397, 0.26027397], [0.2693575 , 0.24794069, 0.24135091, 0.24135091], [0.15942029, 0.23188406, 0.30434783, 0.30434783]])array([[0.00258398, 0.51421189, 0.48320413], [0.01388889, 0.51388889, 0.47222222], [0.47568013, 0.25803792, 0.26628195], [0.01470588, 0.45588235, 0.52941176]])array([[0.374677 , 0.35142119, 0.27390181], [0.34722222, 0.34722222, 0.30555556], [0.30420445, 0.32399011, 0.37180544], [0.60294118, 0.38235294, 0.01470588]])array([[0.52971576, 0.00258398, 0.46770026], [0.43055556, 0.01388889, 0.55555556], [0.22918384, 0.47568013, 0.29513603], [0.97058824, 0.01470588, 0.01470588]])[0.22222222 0.03993056 0.70023148 0.03761574]In&nbsp;[&nbsp;]: # Balloonsbal_feature_log_prior = bal_model.feature_log_prob_for featue_prior in bal_feature_log_prior: pprint(np.exp(featue_prior))print(np.exp(bal_model.class_log_prior_)) In&nbsp;[&nbsp;]: # car evaluation instances 예측# (&quot;vhigh&quot;, &quot;vhigh&quot;, 2, 2, &quot;small&quot;, &quot;high&quot;) [3, 3, 0, 0, 2, 0]# (&quot;low&quot;, &quot;low&quot;, &quot;5more&quot;, &quot;more&quot;, &quot;big, &quot;med) [1, 1, 3, 2, 0, 0]print(car_model.predict_proba([[3, 3, 0, 0, 2, 0]]), car_model.predict([[3, 3, 0, 0, 2, 0]]))print(car_model.predict_proba([[1, 1, 3, 2, 0, 0]]), car_model.predict([[1, 1, 3, 2, 0, 0]])) [[9.21115137e-04 4.43484909e-06 9.99074059e-01 3.90721613e-07]] [2][[0.20014669 0.19352277 0.09437552 0.51195502]] [3]In&nbsp;[&nbsp;]: # balloons instances 예측#(&quot;YELLOW&quot;,&quot;LARGE&quot;, &quot;STRETCH&quot;, &quot;ADULT&quot;) [1, 0, 1, 0]#(&quot;PULPLE&quot;, &quot;SMALL&quot;, &quot;DIP&quot;, &quot;ADULT&quot;) [0, 1, 0, 0]print(bal_model.predict_proba([[1, 0, 1, 0]]), bal_model.predict([[1, 0, 1, 0]]))print(bal_model.predict_proba([[0, 1, 0, 0]]), bal_model.predict([[0, 1, 0, 0]])) [[0.19107307 0.80892693]] [1][[0.79281184 0.20718816]] [0]In&nbsp;[&nbsp;]: bal_df_enc.head(15) Out[&nbsp;]: color size act age inflated 0 1 1 1 0 1 1 1 1 1 0 1 2 1 1 1 1 0 3 1 1 0 0 0 4 1 1 0 1 0 5 1 0 1 0 1 6 1 0 1 0 1 7 1 0 1 1 0 8 1 0 0 0 0 9 1 0 0 1 0 10 0 1 1 0 1 11 0 1 1 0 1 12 0 1 1 1 0 13 0 1 0 0 0 14 0 1 0 1 0 &lt;svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\" width=\"24px\"&gt; &lt;/svg&gt; &#47784;&#45944; &#50696;&#52769; &#44208;&#44284; &#54644;&#49444;&#182;car evaluation&#182;자동차 3(index 2)는 가격이 매우 높고 보수 비용도 매우 높으며 차량의 문이 2개이고 탑승 가능 인원이 2명이다. 트렁크 크기는 작고, 안전평가에서 매우 높은 평가를 받았는데 모델은 자동차 3을 수용불가로 예측했다.자동차 1728(index 1727) 가격이 낮고 보수 비용도 매우 낮으며 차량의 문은 5개 이상이고 인원도 이상이며 트렁크 크기가 크고 안전평가에서는 중간 단계의 평가를 받았다. 모델은 자동차 1728을 매우 좋음으로 예측하였다.balloons&#182;풍선 6(index5)는 노란색이고 크기가 크며 풍선을 늘리는 행동을 했고 어른이 불었다. 모델은 풍선6이 부풀려졌을 것으로 예측했다풍선14(index13)은 보라색이고 크기가 작으며 풍선을 줄이는 행위를 했고 어른이 불었다 모델은 풍선14이 부풀려지지 않았을 것으로 예측했다.3. Naive Bayes Classifier &#44396;&#54788;&#182;Naive Bayes Classifier을 코드로 구현하는 것의 문제는 feature dimension이 매우 커질 경우에, 0과 1사이의 확률을 곱하기 때문에 전체 곱이 0와 매우 가까워지며 가끔은 long double으로도 표현하기 어려울정도로 매우 작은 확률이 계산될 수 있습니다.따라서 log probability을 사용하며, 이에 대한 이점은log probability range가 $[-∞, 0]$ 으로 넓어집니다.if $a &lt; b$, then $ log(a) &lt; log(b)$$log(a \\cdot b) = log(a) + log(b)$ 와 같은 규칙을 적용할 수 있습니다.$P(y|x_1, ..., x_n) = argmax_y \\left[ \\prod_{i=1}^{n} P(x_i|y) \\right] P(y)$$P(x_i|y)$: likelihood probability$P(y)$: class prior probability위 식에 로그를 씌우면$\\log(P(y|x_1, ..., x_n)) = argmax_y \\left[ \\sum_{i=1}^{n} \\log(P(x_i|y)) \\right] + log(P(y))$식이 합으로 바뀌어 다룰 수 있는 숫자로 계산됩니다.log likelihood probability 계산 함수 작성log class prior probability 계산 함수 작성log posterior probability을 사용하여 예측하는 Naive Bayes Classifier 함수 작성car data instance, balloon data instance에 대해 예측 출력In&nbsp;[&nbsp;]: import math In&nbsp;[&nbsp;]: import pandas as pd In&nbsp;[&nbsp;]: cdf = pd.read_csv(car_url, header = None)bdf = pd.read_csv(balloons_url, header = None)cdf.columns = [&quot;buying&quot;, &quot;maint&quot;, &quot;doors&quot;, &quot;persons&quot;, &quot;lung_boot&quot;, &quot;satefy&quot;, &quot;accept&quot;]bdf.columns = [&quot;color&quot;, &quot;size&quot;, &quot;act&quot;, &quot;age&quot;, &quot;inflated&quot;] In&nbsp;[&nbsp;]: # log likelihood probabilitydef calculate_likelihood(df): likelihood = dict() y = df[df.columns[-1]] sz = df.size/df.columns.size for feature in df.columns[:-1]: likelihood[feature] ={} for categ in y.unique(): class_count = y.value_counts()[categ] feature_count = df[df.columns[:-1]][feature][y[y == categ].index.values.tolist()].value_counts().to_dict() for feat_cat, feat_count in feature_count.items(): likelihood[feature][feat_cat + &quot;_&quot; + categ] = feat_count/class_count return likelihooddef calc_prior(df): prior={} for feat in df.columns.to_list()[:-1]: values = df[feat].value_counts().to_dict() prior[feat] = {} for value, count in values.items(): prior[feat][value] = count/df[df.columns[:-1]].size return prior In&nbsp;[&nbsp;]: # log class prior probability def calculate_class_prob(y): class_prior = {} for categ in y.unique(): class_prior[categ] = math.log(y.value_counts(normalize = True)[categ]) return class_priorcalculate_class_prob(cdf[cdf.columns[-1]])[&#39;unacc&#39;] Out[&nbsp;]:-0.3563443107732141In&nbsp;[&nbsp;]: def naive_bayes_classifier(df, inst): likelihood = calculate_likelihood(df) prior = calc_prior(df) prob_out = dict() for categ in df[df.columns[-1]].unique(): calculate_class_prob(df[df.columns[-1]]) likesum = 0 for feature, feature_value in zip (df.columns[:-1], inst): if feature_value + &#39;_&#39; + categ not in likelihood[feature]: continue else: likesum += math.log(likelihood[feature][feature_value + &#39;_&#39; + categ]) class_prior = calculate_class_prob(df[df.columns[-1]]) if categ in class_prior: prob_out[categ] = likesum + class_prior[categ] else: continue result = min(prob_out, key = lambda x :prob_out[x]) print(prob_out) In&nbsp;[&nbsp;]: naive_bayes_classifier(cdf, [&quot;vhigh&quot;, &quot;vhigh&quot;, &quot;2&quot;, &quot;2&quot;, &quot;small&quot;, &quot;high&quot;]) {&#39;unacc&#39;: -7.298119948403321, &#39;acc&#39;: -8.33742842300862, &#39;vgood&#39;: -5.152134856369955, &#39;good&#39;: -6.769162938070731}In&nbsp;[&nbsp;]: naive_bayes_classifier(bdf, [&quot;PULPLE&quot;, &quot;SMALL&quot;, &quot;DIP&quot;, &quot;CHILD&quot;]) {&#39;T&#39;: -1.6094379124341003, &#39;F&#39;: -2.014903020542265}참조https://medium.com/@rangavamsi5/na%C3%AFve-bayes-algorithm-implementation-from-scratch-in-python-7b2cc39268b9https://machinelearningmastery.com/naive-bayes-classifier-scratch-python/"
    } ,
  
    {
      "title"       : "Machine Learning - Logistic Regression",
      "category"    : "",
      "tags"        : "machine learning, study_model, logistic regression",
      "url"         : "./MachineLearning_Logistic.html",
      "date"        : "2021-03-24 00:00:00 +0900",
      "description" : "ML Logistic",
      "content"     : "2015410052_김태원_3차시 &#54876;&#46041; 1: Simple Linear Regression&#182;가장 간단하고 직관적인 기계학습 모델은 데이터의 경향에 맞게 선을 그어주는 것입니다. 이때 데이터에 대해 가장 잘 맞는 선을 찾아가는 과정을 \"Linear Regression\"이라고 합니다.[Example: Sandra&#8217;s lemonade stand&#8217;s revenue over its first 12 months of being open]&#182;In&nbsp;[&nbsp;]: import matplotlib.pyplot as pltmonths = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]revenue = [52, 74, 79, 95, 115, 110, 129, 126, 147, 146, 156, 184]plt.plot(months, revenue, &quot;o&quot;)plt.title(&quot;Sandra&#39;s Lemonade&quot;)plt.xlabel(&quot;months&quot;)plt.ylabel(&quot;revenue&quot;)plt.show() [Points and Lines]&#182;line은 아래의 수식처럼 slope(=$m$)와 intercept(=$b$)에 의해 결정됩니다. $$y = mx + b$$Linear Regression에서의 목표는 우리가 가지고 있는 data에서 \"가장 좋은\"(=\"최적의\") $m$과 $b$를 찾는 것입니다.위의 데이터에 대해 최적의 $m$과 $b$를 미리 구해 놓았다고 가정하겠습니다. 이때 최적의 $m$과 $b$ 는 각각 10과 53입니다.In&nbsp;[&nbsp;]: m = 10b = 53 [Practice 1]&#182;위에서 주어진 최적의 $m$과 $b$를 이용하여 months에 대한 예측값 y를 생성하고 이를 실제 관측값인 revenue와 그래프를 그려 비교해봅시다.In&nbsp;[&nbsp;]: y = [m*x + b for x in months] In&nbsp;[&nbsp;]: plt.plot(months, revenue, &quot;o&quot;)plt.plot(months, y)plt.show() Loss&#182;최적의 모델 파라미터($m$과 $b$)를 찾기 위해서는 loss, 혹은 cost을 정의해야합니다. 이때 loss은 모델의 예측값이 실제값과 얼마나 차이가 있는지를 수치로 표현한 것입니다.아래 그림처럼 해당 실제값에서 예측값까지의 제곱 거리를 loss라고 정의합니다.결론적으로, 이러한 loss를 기준으로 최적의 모델 파라미터인지 아닌지를 판단합니다. 즉, 주어진 전체 데이터에 대해 loss를 최소로 하는 파라미터(m, b)를 찾는 것이 목표입니다. 이를 식으로 표현하면 아래와 같습니다.$$\\frac{1}{N}\\sum_{i=1}^{N}(y_i-(mx_i+b))^2$$[&#50696;&#49884;]&#182;3개의 점 (1, 5), (2, 1), (3, 3)가 주어지고, $y = x$와 $y = 0.5x +1$ 두가지 선이 주어졌습니다.In&nbsp;[&nbsp;]: #주어진 3개의 pointx = [1, 2, 3]y = [5, 1, 3] In&nbsp;[&nbsp;]: # y = x m1 = 1b1 = 0 In&nbsp;[&nbsp;]: # y = 0.5x + 1m2 = 0.5b2 = 1 주어진 점들에 대해 예측값을 계산해봅니다.In&nbsp;[&nbsp;]: y_pred_1 = [m1*x_val + b1 for x_val in x]y_pred_2 = [m2*x_val + b2 for x_val in x] [ Practice 2 ]&#182;이 두가지 선 중 위의 식을 토대로 loss를 계산하고, 그 중 loss가 더 작은 선을 골라봅시다.In&nbsp;[&nbsp;]: #result = 0#for x_val, y_val in zip(x, y):# value = m1*x_val +b1# value = y_val-value# value *= valuetotal_loss1 = 0total_loss2 = 0N = len(x)for i in range(N): total_loss1 += (y[i] - y_pred_1[i])**2/N total_loss2 += (y[i] - y_pred_2[i])**2/N print(&quot;y = x loss&quot;, total_loss1)print(&quot;y = 0.5x + 1 loss&quot;, total_loss2) y = x loss 5.666666666666666y = 0.5x + 1 loss 4.499999999999999Gradient Descent for Intercept&#182;Gradient Descent은 최적화 알고리즘 중 하나로서 loss function 혹은 cost function의 global 혹은 local minima을 찾을 때 사용됩니다.목표는 데이터의 관계를 잘 표현하는 파라미터 $m$와 $b$을 찾는 것이며, 이것은 gradient descent을 사용하여 loss function을 최소화함을 통해 얻을 수 있습니다.최소의 loss를 찾는 것은 마치 아래의 그림처럼 언덕을 내려가다가 바닥에 도착하면 멈추는 것과 비슷합니다. 즉, 파라미터를 loss가 작아지는 방향으로 조정하다가 최소가되면 멈추게됩니다. 이때 loss가 작아지는 방향은 현재의 경사(=gradient)의 반대 방향을 의미합니다. &lt;img src=https://miro.medium.com/max/640/1*lYpF8xJ3TiDoq461I0AcOQ.jpeg width=500px&gt;먼저 intercept(=$b$)에 대해서 gradient descent를 실행해봅니다. 앞에서 정의한 loss를 b에 대해 미분하여 gradient를 구할 수 있습니다.$$\\frac{2}{N}\\sum_{i=1}^{N}-(y_i-(mx_i+b))$$In&nbsp;[&nbsp;]: #intercept에 대하여 gradient descent를 수행하는 함수를 구현해봅니다.def get_gradient_at_b(x, y, b, m): N = len(x) diff = 0 for i in range(N): x_val = x[i] y_val = y[i] diff += y_val - ((m * x_val) + b) b_gradient = -(2/N) * diff return b_gradient Gradient Descent for Slope&#182;마찬가지로 slope(=$m$)에 대한 gradient descent를 실행해봅니다.이번에는 앞에서 정의한 loss를 $m$에 대해 미분하여 gradient를 구하면 됩니다.결과는 아래와 같습니다.$$\\frac{2}{N}\\sum_{i=1}^{N}-x_i(y_i-(mx_i+b))$$In&nbsp;[&nbsp;]: #intercept에 대하여 gradient descent를 수행하는 함수를 구현해봅니다.def get_gradient_at_m(x, y, b, m): N = len(x) diff = 0 for i in range(N): x_val = x[i] y_val = y[i] diff +=(x_val) *( y_val - ((m * x_val) + b)) m_gradient = -(2/N) * diff return m_gradient Weight Update&#182;$b$와 $m$의 gradient를 이용하여 loss가 감소하는 방향으로 $b$와 $m$을 update합니다. 그리고 loss가 최소가 되면 weight update가 멈추게 되는데, 그때의 $b$와 $m$이 최적의 파라미터가 됩니다.이때 내려가는 보폭을 조절할 수 있는데 이때 사용되는 것이 \"learning rate\"입니다. 즉, learning rate가 크면 큰 보폭으로 언덕을 내려가고 learning rate가 작으면 작은 보폭으로 언덕을 내려갑니다. 즉, \"learning rate\"를 gradient에 곱해주어 보폭을 사용자가 정할 수 있게합니다.그러나 learning rate는 신중하게 정할 필요가 있습니다.learning rate가 너무 작으면 loss의 최솟값에 수렴하는데 시간이 오래 걸립니다.learning rate가 너무 크면 최적의 parameter를 얻지 못할 수 있습니다.learning rate을 사용하여 gradient descent를 수행하여 최적의 파라미터를 찾는 함수를 구현해봅시다.In&nbsp;[&nbsp;]: #step_gradient 함수def step_gradient(b_current, m_current, x, y, learning_rate): b_gradient = get_gradient_at_b(x, y, b_current, m_current) m_gradient = get_gradient_at_m(x, y, b_current, m_current) b = b_current - (learning_rate * b_gradient) m = m_current - (learning_rate * m_gradient) return [b, m] Example: Sandra&#8217;s lemonade stand&#8217;s revenue over its first 12 months of being open&#182;이 함수들을 통해 앞에서 생성했던 data에 대해 한 step 후의 update된 parameter를 구해보겠습니다.In&nbsp;[&nbsp;]: months = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]revenue = [52, 74, 79, 95, 115, 110, 129, 126, 147, 146, 156, 184]b = 0m = 0learning_rate = 0.01[b, m] = step_gradient(b, m, months, revenue, learning_rate)print(&quot;b : &quot;, b)print(&quot;m : &quot;, m) b : 2.355m : 17.78333333333333최적의 parameter를 찾을 위 과정을 반복해봅니다.In&nbsp;[&nbsp;]: def gradient_descent(x, y, learning_rate, num_iter): b = 0. m = 0 for i in range(num_iter): [b, m] = step_gradient(b, m, x, y, learning_rate) return [b, m] 위에서 찾은 최적의 파라미터로 linear regression 모델을 만들어 시각화해봅니다.In&nbsp;[&nbsp;]: [optimal_b, optimal_m] = gradient_descent(months, revenue, 0.01, 1000)print(optimal_b, optimal_m) 49.60215351339813 10.463427732364998In&nbsp;[&nbsp;]: y = [optimal_m * x + optimal_b for x in months]plt.plot(months, revenue, &quot;o&quot;)plt.plot(months, y)plt.show() Scikit-Learn &#46972;&#51060;&#48652;&#47084;&#47532; &#49324;&#50857;&#182;지금까지 linear regression algorithm을 직접 구현했습니다. scikit-learn library를 이용하여 보다 간단하게 linear regression을 사용할 수 있습니다. 다큐멘테이션scikit-learn에 있는 linear_model 모듈을 통해 linear regression을 실습해보겠습니다.(단, 위에서 언급했던 learning_rate와 num_iterations은 scikit-learn의 기본값을 사용합니다.)아래의 temperature/sales 데이터를 scikit-learn을 이용하여 fitting 해보겠습니다.In&nbsp;[&nbsp;]: import matplotlib.pyplot as pltimport numpy as nptemperature = np.array(range(60, 100, 2))temperature = temperature.reshape(-1, 1)sales = [65, 58, 46, 45, 44, 42, 40, 40, 36, 38, 38, 28, 30, 22, 27, 25, 25, 20, 15, 5]plt.plot(temperature, sales, &#39;o&#39;) Out[&nbsp;]:[&lt;matplotlib.lines.Line2D at 0x7f12283b24d0&gt;] In&nbsp;[&nbsp;]: from sklearn.linear_model import LinearRegression In&nbsp;[&nbsp;]: lr = LinearRegression()lr.fit(temperature, sales)sales_predict = lr.predict(temperature)plt.plot(temperature, sales, &quot;o&quot;)plt.plot(temperature, sales_predict,)plt.show() &#44228;&#49688;(Coefficients)&#182;fitting된 모델에서 계수(coefficients)를 출력해봅니다.In&nbsp;[&nbsp;]: lr.coef_ Out[&nbsp;]:array([-1.15225564])In&nbsp;[&nbsp;]: lr.intercept_ Out[&nbsp;]:125.47819548872182&#47784;&#45944; &#54217;&#44032;&#182;R-Squared (Coefficient of Determination)은 0과 1사이의 값으로 linear regression 모델이 데이터에 얼마나 잘 학습되었는지 나타냅니다. R-Squared가 1에 가까울 수록 모델은 종속 변수(dependent variable)를 잘 예측할 수 있습니다.$$R^2 = 1 - \\frac{SS_{RES}}{SS_{TOT}} = 1 - \\frac{\\sum_{i}(y_i - \\hat{y}_i)^2}{\\sum_{i}(y_i - \\bar{y}_i)^2}$$In&nbsp;[&nbsp;]: #scikit-learn에서 제공하는 score함수 사용하여 r-square값 구해보기print(&quot;R-squared:&quot;)print(lr.score(temperature, sales))#lr model sales -&gt; temp 에 대해 아래 확률 만큼 설명 가능하다 R-squared:0.9114088011031334&#54876;&#46041; 2: Logistic Regression&#182;Logistic Regression은 데이터가 어떠한 특정 카테고리에 속할지를 0과 1사이의 연속적인 확률로 예측하는 회귀 알고리즘 중 하나입니다. 그런 다음, 확률에 기반하여 특정 데이터가 어떤 카테고리에 속할지를 결정하게 되고, 궁극적으로 classification문제를 풀게 됩니다.[Linear Regression Approach]&#182;대학교 강의에서 학생들이 기말 시험을 통과할 수 있을지를 예측해보려고 합니다. 각 학생들이 시험을 통과할 확률을 예측함으로써 통과 여부를 예측할 수 있습니다. 여기서 Linear Regression을 활용하면 어떨까요? 한번 해봅시다.우선 기말 시험 데이터를 확인 해보겠습니다.In&nbsp;[&nbsp;]: import numpy as npimport matplotlib.pyplot as pltpassed_exam = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1])passed_exam = passed_exam.reshape(-1, 1) hours_studied = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19])hours_studied = hours_studied.reshape(-1, 1) #시험에 패스/페일 vs 공부한 시간에 대한 산점도 그려보기plt.scatter(hours_studied, passed_exam)plt.show() 각 학생들이 공부한 시간을 num_hours_studied이라 하고 해당 학생이 중간 시험을 통과한 여부를 y (y 는 통과한 경우 1, 그렇지 않은 경우 0) 라고 한다면 linear regression을 통해 다음과 같이 직선을 그릴 수 있습니다.In&nbsp;[&nbsp;]: model = LinearRegression()model.fit(hours_studied, passed_exam) Out[&nbsp;]:LinearRegression()In&nbsp;[&nbsp;]: import numpy as npsample_x = np.linspace(0, 20,100).reshape(-1,1)probability = model.predict(sample_x).ravel() In&nbsp;[&nbsp;]: plt.plot(hours_studied, passed_exam, &quot;o&quot;)plt.plot(sample_x, probability)plt.show() Logistic regression의 예측값은 0과 1사이이므로 위와 같은 linear regression의 한계를 극복할 수 있습니다.Logistic regression은 다음과 같은 과정으로 수행됩니다.모든 coefficients와 intercept(bias)를 0으로 초기화합니다.각각의 feature를 이에 상응하는 coefficient와 곱한 값과 intercept(bias)를 모두 더해 log-odds를 계산합니다.계산한 log-odds 값을 sigmoid 함수에 전달하여 0 과 1 사이의 확률값을 구합니다.계산한 확률값과 실제 label을 비교하여 loss를 계산하고, gradient descent로 최적의 파라미터를 찾습니다.최적의 파라미터를 찾았다면 classification threshold 값을 조절하여 positive class와 negative class를 어떻게 나눌지를 설정합니다.[ Log-Odds ]&#182;Linear regression에서는 각 feature에 상응하는 weight의 곱과 intercept(bias)를 더해 예측을 하였습니다. Logistic regression에서도 마찬가지지만 log-odds를 계산합니다.log-odds는 특정 데이터가 positive class에 속할 확률을 표현합니다. 통계에서 특정 사건의 odds(승산)을 계산하는 공식은 다음과 같습니다.$P(A)$: 특정 사건이 발생할 확률$1-P(A)$: 특정 사건이 발생하지 않는 확률$$Odds = \\frac{P(A)}{1 - P(A)}$$Odds는 특정 사건이 일어나는 횟수가 특정 사건이 일어나지 않는 횟수보다 얼마나 더 많은지를 의미합니다. 만약 특정 학생이 시험에서 pass할 확률이 0.7이라면, pass하지 못 할 확률은 1 - 0.7 = 0.3 이고, 이 경우 odds를 다음과 같이 계산할 수 있습니다.$$\\text{Odds of passing} = \\frac{0.7}{0.3} = 2.33$$Odds는 0과 양의 무한대의 값을 범위로 갖습니다. 그렇기 때문에 제약이 있고, 또, 확률값과 odds 값은 비대칭성을 띕니다.이러한 한계를 극복하기 위하여 odd에 로그를 취하는것을 log-odds라 하고, 음의 무한대부터 양의 무한대까지의 범위를 갖습니다.$$\\text{Log odds of passing} = log(2.33) = 0.847$$Logistic regression 모델에서 아래와 같이 z 값으로 나타내지는 log-odds 값을 계산할 수 있습니다.$$log(\\frac{P(A)}{1-P(A)})=z=b_0+b_1x_1+b_2x_2+\\cdots+b_nx_n$$이로써 특정 데이터의 feature values를 해당 데이터가 positive class에 속할 가능성으로 매핑할 수 있습니다. 이 때 이러한 곱의 합을 dot product(내적) 이라고 합니다. 내적은 numpy의 np.dot() 메서드를 활용하여 쉽게 계산할 수 있습니다.기말 시험 데이터에서 최적의 coefficient와 intercept가 각각 $0.03$, $-0.3$이라고 가정했을 때의 log-odds를 계산 해봅시다.In&nbsp;[&nbsp;]: calculated_coefficients = 0.03intercept = -0.3 In&nbsp;[&nbsp;]: # log_odds 함수를 정의해봅니다.def log_odds(features, coefficient, intercept): return np.dot(features, coefficient) + intercept In&nbsp;[&nbsp;]: # hours_studied 데이터에 대해서 log-odds를 계산해봅니다.calculated_log_odds = log_odds(hours_studied, calculated_coefficients, intercept)calculated_log_odds Out[&nbsp;]:array([[-0.3 ], [-0.27], [-0.24], [-0.21], [-0.18], [-0.15], [-0.12], [-0.09], [-0.06], [-0.03], [ 0. ], [ 0.03], [ 0.06], [ 0.09], [ 0.12], [ 0.15], [ 0.18], [ 0.21], [ 0.24], [ 0.27]])[ Sigmoid Function ]&#182;Sigmoid Function은 log-odds인 $z$ 값을 취해서 아래와 같이 0 과 1 사이의 값을 반환합니다.$$h_θ = σ(z) = \\frac{1}{1+e^{-z}}$$&lt;img src=https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/1200px-Logistic-curve.svg.png width=500px&gt;Logistic Regression이 특정 데이터가 positive class에 속할 확률을 계산합니다.In&nbsp;[&nbsp;]: # sigmoid 함수 정의하기def sigmoid(z): return 1/(1+np.exp(-z)) In&nbsp;[&nbsp;]: # 확률 계산해보기probabilities = sigmoid(calculated_log_odds)probabilities Out[&nbsp;]:array([[0.42555748], [0.4329071 ], [0.44028635], [0.44769209], [0.45512111], [0.46257015], [0.47003595], [0.47751518], [0.4850045 ], [0.49250056], [0.5 ], [0.50749944], [0.5149955 ], [0.52248482], [0.52996405], [0.53742985], [0.54487889], [0.55230791], [0.55971365], [0.5670929 ]])[ Log-Loss ]&#182;이제 최적의 coefficients와 intercept를 구해보겠습니다. 이를 구하기 위해서는 주어진 모델의 예측이 실제 데이터에 얼마나 가까운지 측정하는 기준이 필요합니다. 이를 loss function 혹은 cost function이라고 합니다.모델이 데이터에 ‘fit’ 하단걸 측정하기 위해선 먼저 각 데이터에 대한 loss를 계산한뒤 loss의 평균을 내야합니다. Logistic regression에서의 loss function은 Log Loss라고 불리며, 공식은 다음과 같습니다.$$J(b) = -\\frac{1}{m}\\sum_{i=1}^{m}[y^{(i)}log(a^{(i)}) + (1-y^{(i)})log(1-a^{(i)})]$$m 은 전체 데이터의 개수입니다.$y^{(i)}$는 $i$ 번째 데이터의 class 입니다.$a^{(i)}$는 $i$ 번째 데이터의 log-odds 값에 sigmoid 를 취한 값입니다. 즉 $i$ 번째 데이터가 positive class에 속할 확률을 나타냅니다.만약 $i$ 번째 데이터의 class가 $y=1$ 이라면 해당 데이터에 대한 loss는 다음과 같습니다.$$loss_{i(y=1)} = -log(a^{(i)})$$loss를 최소화 시키려면 $a^{(i)}$ 값이 커야 합니다. 즉, 예측된 확률 값이 원래 class인 1 에 가까울수록 loss는 줄어들게 됩니다.반대로 $i$ 번째 데이터의 class가 $y=0$ 인 경우는 다음과 같습니다..$$loss_{i(y=0)} = -log(1-a^{(i)})$$loss를 최소화 시키려면 $a^{(i)}$값이 작아야 합니다. 즉, 예측된 확률 값이 원래 class이 0에 가까울수록 loss는 줄어들게 됩니다.아래의 그래프는 class가 $y=1$, $y=0$ 일 때 $a$값에 따라 loss가 어떻게 변화하는지를 나타냅니다.그래프를 보면 올바르게 예측할수록 loss가 줄어드는 것을 볼 수 있습니다. 반대로 잘 못 예측하게 되면 loss가 크게 증가하는데, 이는 모델이 잘못 예측할 때 패널티를 강하게 줌으로써 올바른 예측을 할 수 있도록 유도할 수 있습니다.In&nbsp;[&nbsp;]: # log_loss 함수 구현해보기def log_loss(probabilities, actual_class): return np.sum(-(1 / actual_class.shape[0]) * (actual_class * np.log(probabilities) + (1 - actual_class) * np.log(1 - probabilities))) In&nbsp;[&nbsp;]: log_loss(probabilities, passed_exam) Out[&nbsp;]:0.6279073897953891[ Classification Thresholding ]&#182;Logistic Regression은 예측된 확률 값이 임계값을 넘느냐 못 넘느냐에 따라서 class를 분류합니다. 이 임계값을 classification threshold 라고 합니다.Classification threshold의 기본값은 0.5 입니다. 만약 특정 데이터의 예측된 확률 값이 0.5 보다 크거나 같다면 해당 데이터는 positive class로 분류됩니다. 반대로 예측된 확률 값이 0.5 보다 낮다면 negative class로 분류됩니다.만약 더욱 엄격하게하고자 한다면 threshold를 0.6이나 0.7로 조정할 수 있습니다. 즉, 모델이 positive class를 더 적게 예측할 수 있도록 하는 것입니다.이에 대해 예측된 확률값이 임계값을 넘으면 1, 그렇지 않으면 0 을 반환하는 함수를 구현해보겠습니다.In&nbsp;[&nbsp;]: # predict_class함수 구현하기def predict_class(features, coefficients, intercept, threshold): odd = log_odds(features, coefficients, intercept) predicted_probability = sigmoid(odd) result = [] for i in predicted_probability: if i &gt;= threshold: result.append([1]) else: result.append([0]) return result In&nbsp;[&nbsp;]: # threshold=0.5로 최종 예측 해보기predict_class(hours_studied, calculated_coefficients, intercept, 0.5) Out[&nbsp;]:[[0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1]]이번에는 더욱 엄격하게 threshold를 0.55로 설정하여 위의 결과와 비교해봅니다.In&nbsp;[&nbsp;]: #threshold=0.55로 최종 예측 해보기predict_class(hours_studied, calculated_coefficients, intercept, 0.55) Out[&nbsp;]:[[0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [1], [1], [1]][ Scikit-Learn ]&#182;scikit-learn에서 제공하는 메서드를 활용하여 Logistic Regression을 구현해봅니다.In&nbsp;[&nbsp;]: from sklearn.linear_model import LogisticRegressionmodel = LogisticRegression()model.fit(hours_studied, passed_exam) /usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). y = column_or_1d(y, warn=True) Out[&nbsp;]:LogisticRegression()In&nbsp;[&nbsp;]: probability = model.predict_proba(sample_x)[:, 1]plt.plot(hours_studied, passed_exam, &#39;o&#39;)plt.plot(sample_x, probability)plt.xlabel(&#39;hours studied&#39;)plt.show() In&nbsp;[&nbsp;]: predicted_class Out[&nbsp;]:array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])&#44284;&#51228;&#182;이전 실습2 과제의 코드를 이용하여 titanic.csv 데이터의 url를 통해 pandas DataFrame으로 데이터를 가져옵니다.이전 실습2 과제처럼 동일하게1) 전체 데이터에서 Survived, Pclass, Sex, Age, SibSp, Parch, Fare, Embarked feature을 고르고,2) Age feature의 NA값을 drop하고,3) Sex feature을 scikit-learn의 LabelEncoder을 사용하여 정수형으로 변환합니다..head()를 사용하여 데이터의 첫 다섯개 instance를 출력합니다.주어진 onehot() 함수를 읽어보고, 어떤 순서와 방식으로 데이터를 변환하는지 서술합니다. 필요하다면 각 메소드의 디큐멘테이션을 참고합니다.onehot() 함수를 사용하여 Embarked feature을 one-hot encoding 합니다.&lt;img src=https://i.imgur.com/mtimFxh.png width=400px&gt;.head()를 사용하여 one-hot encoding이 올바르게 되었는지 확인합니다.아래 주어진 코드를 이용하여 데이터를 feature 변수 x와 label 변수 y로 분리하고 train_test_split()함수를 이용하여 데이터를 train data과 test data으로 나눕니다. 만약 deterministic한 결과를 원한다면 random_state 파라미터를 지정해줍니다. (지정하지 않아도 과제 점수에는 상관이 없습니다.)train_test_split() 다큐멘테이션# 데이터를 feature X와 label y로 나눕니다.y = df[[&#39;Survived&#39;]].to_numpy().ravel()x = df.drop(columns=[&#39;Survived&#39;])LogisticRegression() 모델을 생성합니다. 이때, 모델 파라미터는 max_iter을 1000으로 지정해줍니다. 이 모델을 train data에 fit 해봅니다.test data에 대하여 score을 계산합니다.훈련된 Logistic regression 모델의 .coef_를 출력해보고, .coef_의 절대값이 큰 feature 2개와 절대값이 작은 feature 2개가 무엇인지, 그리고 각각에 대해 그 값이 몇인지 서술합니다. //의미 서술 XIn&nbsp;[&nbsp;]: import pandas as pddef onehot(data, feature): &#39;&#39;&#39; data의 feature column을 one hot으로 변환해줍니다. data: pandas DataFrame feature: string, 데이터 프레임의 column 이름 &#39;&#39;&#39; return pd.concat([data, pd.get_dummies(data[feature], prefix=feature)], axis=1).drop([feature], axis=1)data_url = &quot;https://raw.githubusercontent.com/inikoreaackr/ml_datasets/main/titanic.csv&quot; In&nbsp;[&nbsp;]: data = pd.read_csv(data_url) In&nbsp;[&nbsp;]: data.columnsdata = data [[&#39;Survived&#39;, &#39;Pclass&#39;, &#39;Sex&#39;, &#39;Age&#39;, &#39;SibSp&#39;, &#39;Parch&#39;, &#39;Fare&#39;, &#39;Embarked&#39;]] In&nbsp;[&nbsp;]: data = data.dropna() In&nbsp;[&nbsp;]: from sklearn.preprocessing import LabelEncoderle = LabelEncoder()data.Sex = le.fit_transform(data.Sex) In&nbsp;[&nbsp;]: data.head() Out[&nbsp;]: Survived Pclass Sex Age SibSp Parch Fare Embarked 0 0 3 1 22.0 1 0 7.2500 S 1 1 1 0 38.0 1 0 71.2833 C 2 1 3 0 26.0 0 0 7.9250 S 3 1 1 0 35.0 1 0 53.1000 S 4 0 3 1 35.0 0 0 8.0500 S &lt;svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\" width=\"24px\"&gt; &lt;/svg&gt; In&nbsp;[&nbsp;]: onehot? In&nbsp;[&nbsp;]: data = onehot(data, &#39;Embarked&#39;) 주어진 onehot() 함수를 읽어보고, 어떤 순서와 방식으로 데이터를 변환하는지 서술합니다. 필요하다면 각 메소드의 디큐멘테이션을 참고합니다.&lt;img src=https://i.imgur.com/mtimFxh.png width=400px&gt;data의 feature column을 0과 1의 값을 가지는 데이터로 구별해주는 인코딩이다.표현하고자 하는 인덱스에는 데이터 1을, 다른 인덱스는 0으로 표현된다.먼저 데이터에 대한 정수 인코딩이 진행된다. 각각의 요소는 다른 정수값을 가지는 데이터로 표현된다.EX 1 : RED 2 : YELLOW 3 : GREEN값의 개수만큼의 행렬 구조가 만들어지고 표현하고자 하는 데이터의 고유한 값을 index로 보아 해당하는 위치에는 1을 다른 값을 지니는 요소의 위치에 0을 부여한다.In&nbsp;[&nbsp;]: y = data[[&#39;Survived&#39;]].to_numpy().ravel()x = data.drop(columns=[&#39;Survived&#39;]) In&nbsp;[&nbsp;]: from sklearn.model_selection import train_test_splitx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = .3) In&nbsp;[&nbsp;]: model_titanic = LogisticRegression(max_iter=1000)model_titanic.fit(x_train, y_train) Out[&nbsp;]:LogisticRegression(max_iter=1000)In&nbsp;[&nbsp;]: print(model_titanic.score(x_test,y_test)) 0.7990654205607477In&nbsp;[&nbsp;]: coef = model_titanic.coef_print(coef, &quot;\\n&quot;)for i in range(len(x.columns)): print(x.columns[i],&quot;:&quot;,coef[0][i]) [[-1.13588865e+00 -2.58221815e+00 -4.56647796e-02 -3.35399948e-01 -4.22408897e-02 2.20349750e-03 4.47231111e-01 -5.75288314e-01 1.18618995e-01]] Pclass : -1.1358886533804295Sex : -2.5822181505918125Age : -0.045664779628737426SibSp : -0.33539994797221Parch : -0.04224088972835762Fare : 0.0022034975000360204Embarked_C : 0.4472311112889102Embarked_Q : -0.5752883138433935Embarked_S : 0.11861899501285339절대값이 큰 featureSex : 2.40791938831739Pclass : 1.174206879583307절대값이 작은 featureFare : 0.00041304172578507105Age : 0.05062299635153086&#44284;&#51228; 2&#182;실습에서는 logistic regression 함수의 cost fuction까지 구현을 해보았습니다. 두번째 과제는 logistic regression cost function의 gradient descent을 함수로 구현하여 최적의 파라미터까지 찾아 최종 모델을 훈련시키는 logistic regression 모델을 scratch부터 구현하는 것 입니다. 데이터는 logistic regression 실습에서 사용한 hours_studied, passed_exam 데이터를 사용합니다.$$z = x \\cdot w + b$$$$h_θ = \\frac{1}{(1+e^{-z})}$$$$cost_{(\\theta, y)} = -y \\cdot log(h_\\theta)-(1-y)\\cdot log(1-h_{\\theta})$$세부 사항:coefficient의 gradient을 계산하는 함수를 작성합니다. ($\\frac{\\delta cost}{\\delta w}$)intercept의 gradient을 계산하는 함수를 작성합니다. ($\\frac{\\delta cost}{\\delta b}$)두 gradient을 받아 step gradient을 실행하는 함수를 작성합니다.위 과정을 반복하여 최적의 파라미터를 찾는 logistic regression 함수를 작성합니다.데이터에 최종 모델을 학습시키고, classification을 잘 수행하는지 결과를 시각화해 확인해봅니다.안내 사항:learning rate, number of iteration은 임의로 적당한 값을 본인이 설정해봅니다.과제 2번은 타 수강생 한 명과 협업이 가능합니다. 협업한 수강생의 이름 및 학번을 개제해주시기 바랍니다.참고한 자료의 출처를 필히 기재해주시기 바랍니다.In&nbsp;[&nbsp;]: # coefficient gradient : SUM(x(a(i) - y))/mdef get_coef_gradient(x, y, y_predict): [[grad]] = (1/len(x)) * np.dot(x.T, y_predict - y) return grad In&nbsp;[&nbsp;]: get_coef_gradient(hours_studied, passed_exam, probabilities) Out[&nbsp;]:-1.5871075590969737In&nbsp;[&nbsp;]: # intercept gradient : SUM(a(i) - y)/mdef get_intercept_gradient(y, y_predict): return (1/len(y)) * np.sum(y_predict - y) In&nbsp;[&nbsp;]: get_intercept_gradient(passed_exam, probabilities) Out[&nbsp;]:0.046277874159417066In&nbsp;[&nbsp;]: def log_loss_step_gradient(weight_current, intercept_current, x, y, y_predicted, learning_rate): coef = get_coef_gradient (x, y, y_predicted) intercept = get_intercept_gradient(y, y_predicted) weight = weight_current - (learning_rate * coef) intercept = intercept_current - (learning_rate * intercept) return [weight, intercept] In&nbsp;[&nbsp;]: def log_loss_gradient_descent(x, y, learning_rate, num_iter): opt_weight = 0 opt_intercept = 0 odd = log_odds(x, weight, intercept) y_predict = sigmoid(odd) print(y_predict) trace = [] trace.append(log_loss(y_predict, y)) for j in range(num_iter): [opt_weight, opt_intercept] = log_loss_step_gradient(opt_weight, opt_intercept, x, y, y_predict, learning_rate) odd = log_odds(x, opt_weight, opt_intercept) y_predict = sigmoid(odd) trace.append(log_loss(y_predict, y)) return [opt_weight, opt_intercept], trace In&nbsp;[&nbsp;]: [weight, intercept], trace = log_loss_gradient_descent(hours_studied, passed_exam, 0.05, 1000)print(&quot;weight : &quot;, weight, &quot;\\n&quot;)print(&quot;intercept : &quot;, intercept, &quot;\\n&quot;) [[0.42555748] [0.51673783] [0.60681714] [0.69017318] [0.76276619] [0.82271945] [0.87010168] [0.90626282] [0.93313104] [0.95269988] [0.96674609] [0.97672306] [0.98375699] [0.98868999] [0.99213681] [0.99453899] [0.99621011] [0.99737121] [0.99817723] [0.99873643]]weight : 0.36710895563627843 intercept : -3.6293894250817496 In&nbsp;[&nbsp;]: def norm_x(x): return x-x.mean()y_prediction = sigmoid(log_odds(hours_studied, weight, intercept))plt.plot(hours_studied, passed_exam, &quot;o&quot;)plt.plot(hours_studied, y_prediction) Out[&nbsp;]:[&lt;matplotlib.lines.Line2D at 0x7f122808e390&gt;] In&nbsp;[&nbsp;]: # model의 log_loss cost변화plt.title(&#39;Log-loss over iterations&#39;)plt.plot(trace)plt.xlabel(&#39;iteration&#39;)plt.ylabel(&#39;Log-loss&#39;)plt.show() 참고한 사이트https://www.kaggle.com/code/paulrohan2020/logistic-regression-implementation-from-scratchhttps://towardsdatascience.com/logistic-regression-from-scratch-in-python-ec66603592e2https://www.kdnuggets.com/2022/04/logistic-regression-classification.html"
    } ,
  
    {
      "title"       : "Machine Learning - Decision Tree",
      "category"    : "",
      "tags"        : "machine learning, study_model, Decision Tree",
      "url"         : "./MachineLearning_DT.html",
      "date"        : "2021-03-24 00:00:00 +0900",
      "description" : "ML Decision Tree",
      "content"     : "Decision_Tree 결정 트리(Decision Tree)In&nbsp;[&nbsp;]: !pip install mglearn!pip install --upgrade joblib==1.1.0 Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/Requirement already satisfied: mglearn in /usr/local/lib/python3.7/dist-packages (0.1.9)Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from mglearn) (1.0.2)Requirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (from mglearn) (2.9.0)Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from mglearn) (1.3.5)Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from mglearn) (7.1.2)Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mglearn) (1.21.6)Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from mglearn) (1.1.0)Requirement already satisfied: cycler in /usr/local/lib/python3.7/dist-packages (from mglearn) (0.11.0)Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from mglearn) (3.2.2)Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;mglearn) (3.0.9)Requirement already satisfied: python-dateutil&gt;=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;mglearn) (2.8.2)Requirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;mglearn) (1.4.4)Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver&gt;=1.0.1-&gt;matplotlib-&gt;mglearn) (4.1.1)Requirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil&gt;=2.1-&gt;matplotlib-&gt;mglearn) (1.15.0)Requirement already satisfied: pytz&gt;=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas-&gt;mglearn) (2022.4)Requirement already satisfied: threadpoolctl&gt;=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn-&gt;mglearn) (3.1.0)Requirement already satisfied: scipy&gt;=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn-&gt;mglearn) (1.7.3)Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/Requirement already satisfied: joblib==1.1.0 in /usr/local/lib/python3.7/dist-packages (1.1.0)&#51648;&#45768; &#48520;&#49692;&#46020; (Gini Impurity)&#182;지니 불순도는 결정 트리의 분할기준 중 하나입니다.&lt;img src=https://s3.amazonaws.com/codecademy-content/programs/data-science-path/decision-trees/comparison_1.svg width=300px&gt;&lt;img src=https://s3.amazonaws.com/codecademy-content/programs/data-science-path/decision-trees/comparison_2.svg width=300px&gt;지니 불순도를 찾기 위해서는 1에서 시작해서 세트의 각 class 비율의 제곱을 빼면 됩니다.$$\\text{Gini Impurity} = 1 - \\text{Gini Index} \\\\ = 1 - \\sum_{i=1}^{K}p_{i}^{2}$$위 식에서 $K$은 class label의 개수이며, $p_i$은 $i$번째 class label의 비율입니다.예를 들어, A class인 instance가 3개 있고 B class인 instance가 1개 있는 데이터의 경우에는 지니 불순도는 아래와 같이 계산됩니다.$$1 - (3/4)^2 - (1/4)^2 = 0.375$$만약 데이터가 하나의 class만 있다면, 지니 불순도는 0이 됩니다. 불순도가 낮으면 낮을수록 결정 트리의 성능은 더 좋아집니다.&#49892;&#49845; 1&#182;위 정리에서 주어진 Tree의 불순도 계산In&nbsp;[&nbsp;]: 1 - (4/6)**2 - (2/6)**2 Out[&nbsp;]:0.4444444444444445sample_labels 리스트의 지니 불순도 계산In&nbsp;[&nbsp;]: sample_labels = [&quot;unacc&quot;, &quot;unacc&quot;, &quot;acc&quot;, &quot;acc&quot;, &quot;good&quot;, &quot;good&quot;]impurity = 1 sample labels에 포함되어있는 class의 개수In&nbsp;[&nbsp;]: from collections import Counterlabel_counts = Counter(sample_labels)print(label_counts) Counter({&#39;unacc&#39;: 2, &#39;acc&#39;: 2, &#39;good&#39;: 2})데이터셋에서 각 label의 확률 계산In&nbsp;[&nbsp;]: for label in label_counts: print(label) prob = label_counts[label]/len(sample_labels) print(prob) unacc0.3333333333333333acc0.3333333333333333good0.3333333333333333확률을 이용하여 sample_labels의 불순도 계산In&nbsp;[&nbsp;]: for label in label_counts: prob = label_counts[label]/len(sample_labels) impurity -= prob ** 2print(impurity) 0.6666666666666665지니 불순도를 계산하는 코드 함수 제작In&nbsp;[&nbsp;]: def gini(dataset): impurity = 1 label_counts =Counter(dataset) for label in label_counts: prob_of_label = label_counts[label] / len(dataset) impurity -= prob_of_label ** 2 return impurity &#51221;&#48372;&#51613;&#44032;&#47049; (Information Gain)&#182;이제 지니 불순도가 낮은 끝마디(leaf node)를 만들기 위해서 어떠한 feature에 따라 데이터를 나누어야하는지 결정해야 합니다.예를 들어, 학생들의 수면 시간 또는 학생들의 공부 시간 둘 중 어느 feature을 기준으로 학생들을 나누어야 더 좋은 tree를 만들 수 있을까요?위 질문에 답하기 위해 어떠한 feature에 대하여 데이터를 나누었을 때의 정보증가량을 계산해야 합니다.정보증가량은 데이터 분할 전과 후의 불순도 차이를 측정합니다.예를 들어, 불순도가 0.5인 데이터를 어떠한 feature에 대해 나누었을 때, 불순도가 각각 0, 0.375, 0 인 끝마디가 생긴다고 가정해봅니다.&lt;img src=https://s3.amazonaws.com/codecademy-content/programs/data-science-path/decision-trees/info.svg width=300px&gt;이 경우에 데이터를 나누는 정보증가량은 0.5 - 0 - 0.375 - 0 = 0.125 입니다.데이터를 나누었을때의 정보 증가량은 양수입니다. 따라서, 위처럼 결정 지점을 나눈 것은 결과적으로 불순도를 낮추었기 때문에 좋은 결정 지점입니다.정보증가량은 크면 클수록 좋습니다.&#49892;&#49845; 2&#182;unsplit_labels라는 임의의 데이터를 두가지 다른 분할 지점으로 나누었습니다. 이는 split_labels_1와 split_labels_2 입니다.각 분할에 대해 information gain 계산In&nbsp;[&nbsp;]: unsplit_labels = [&quot;unacc&quot;, &quot;unacc&quot;, &quot;unacc&quot;, &quot;unacc&quot;, &quot;unacc&quot;, &quot;unacc&quot;, &quot;good&quot;, &quot;good&quot;, &quot;good&quot;, &quot;good&quot;, &quot;vgood&quot;, &quot;vgood&quot;, &quot;vgood&quot;]split_labels_1 = [[&quot;unacc&quot;, &quot;unacc&quot;, &quot;unacc&quot;, &quot;unacc&quot;, &quot;unacc&quot;, &quot;unacc&quot;, &quot;good&quot;, &quot;good&quot;, &quot;vgood&quot;], [ &quot;good&quot;, &quot;good&quot;], [&quot;vgood&quot;, &quot;vgood&quot;]]split_labels_2 = [[&quot;unacc&quot;, &quot;unacc&quot;, &quot;unacc&quot;, &quot;unacc&quot;,&quot;unacc&quot;, &quot;unacc&quot;, &quot;good&quot;, &quot;good&quot;, &quot;good&quot;, &quot;good&quot;], [&quot;vgood&quot;, &quot;vgood&quot;, &quot;vgood&quot;]] In&nbsp;[&nbsp;]: # unsplit_labels의 지니 불순도를 계산해봅니다.info_gain_1 = gini(unsplit_labels)info_gain_1 Out[&nbsp;]:0.6390532544378698split_labels_1의 각 부분집합에 대하여 지니 불순도을 계산하여 정보 증가량 계산In&nbsp;[&nbsp;]: for subset in split_labels_1: info_gain_1 -= gini(subset)print(info_gain_1) 0.14522609394404257split_labels_2에 대해 정보증가량을 계산In&nbsp;[&nbsp;]: info_gain_2 = gini(unsplit_labels)for subset in split_labels_2: info_gain_2 -= gini(subset)print(info_gain_2) 0.15905325443786977정보증가량을 계산하는 함수In&nbsp;[&nbsp;]: def information_gain(starting_labels, split_labels): info_gain = gini(starting_labels) for subset in split_labels: info_gain -= gini(subset) return info_gain &#44032;&#51473; &#51221;&#48372;&#51613;&#44032;&#47049; (Weighted Information Gain)&#182;만약 정보증가량이 0이라면 그 feature에 대해 데이터를 나누는 것은 소용이 없습니다. 때에 따라서 데이터를 나누었을 때 정보증가량이 음수가 될 수 있습니다. 이 문제를 해결하기 위해서 가중 정보증가량 (weighted information gain)을 사용합니다.분할 후에 생성되는 데이터의 부분집합의 크기 또한 중요합니다. 예를 들어서, 아래 이미지에서는 불순도가 같은 두 부분집합이 있습니다.&lt;img src=https://s3.amazonaws.com/codecademy-content/programs/data-science-path/decision-trees/impurity-0.svg width=300px&gt;어느 부분집합을 결정 트리의 끝마디로 정하는게 좋은 결정트리를 만들 수 있을까요?두 부분집합은 모두 불순도가 0으로써 완전하지만, 두 번째 부분집합이 더욱 의미있습니다. 두 번째 부분집합에는 많은 개수의 instance들이 있기 때문에 이 부분집합이 구성된 것이 우연이 아님을 알수 있습니다.그 반대를 생각해보는 것도 도움이 됩니다. 아래 그림에서 같은 값의 불순도를 가지고 있는 두 부분집합이 있습니다.&lt;img src=https://s3.amazonaws.com/codecademy-content/programs/data-science-path/decision-trees/impurity-5.svg width=300px&gt;이 두 부분집합의 불순도는 굉장히 높습니다. 그렇지만 어느 부분집합의 불순도가 더 큰 의미를 가질까요? 왼쪽의 부분집합을 분할하는 것보다는 오른쪽 부분집합을 분할하여 불순도가 없는 집합을 만드는 것이 정보증가량이 더 클 것입니다. 따라서, 집합의 instance 개수를 고려하여 정보증가량을 계산해야 합니다.집합의 크기까지 고려하도록 정보증가량 함수를 수정할 것 입니다. 단순히 불순도를 빼는 것에서 더 나아가 분할된 부분집합의 가중 불순도를 뺄 것입니다. 만약 분할 전의 데이터가 10개의 instance을 가지고 있고 하나의 부분집합이 2개의 instance가 있다면, 그 부분집합의 가중 불순도는 2/10 * impurity가 되어 instance 숫자가 적은 세트의 중요도를 낮춥니다.가중 정보증가량 계산의 예시는 아래와 같습니다.&lt;img src=https://s3.amazonaws.com/codecademy-content/programs/data-science-path/decision-trees/weighted_info.svg&gt;&#49892;&#49845; 3&#182;아래는 데이터셋의 각 feature와 class label에 대한 설명입니다. Car dataset은 class에 해당하는 4가지 label과 각 차량의 특징을 나타내는 6개의 feature을 갖고 있습니다.Label은 4개의 class, unacc(unacceptable), acc(acceptable), good, vgood로 이루어져 있으며, 각 class는 차량 구매시의 만족도(acceptability)를 나타냅니다.각 차량은 6개의 feature을 가지고 있고, 아래와 같습니다.buying (차량의 가격): \"vhigh\",\"high\",\"med\", or \"low\".maint (차량 유지 비용): \"vhigh\",\"high\",\"med\", or \"low\".doors (차의 문 갯수): \"2\",\"3\",\"4\",\"5more\".persons (차량의 최대 탑승 인원): \"2\",\"4\", or \"more\".lug_boot (차량 트렁크의 사이즈): \"small\",\"med\", or \"big\"safety (차량의 안전성 등급): \"low\",\"med\", or \"high\".In&nbsp;[&nbsp;]: # 샘플 데이터cars = [[&#39;med&#39;, &#39;low&#39;, &#39;3&#39;, &#39;4&#39;, &#39;med&#39;, &#39;med&#39;], [&#39;med&#39;, &#39;vhigh&#39;, &#39;4&#39;, &#39;more&#39;, &#39;small&#39;, &#39;high&#39;], [&#39;high&#39;, &#39;med&#39;, &#39;3&#39;, &#39;2&#39;, &#39;med&#39;, &#39;low&#39;], [&#39;med&#39;, &#39;low&#39;, &#39;4&#39;, &#39;4&#39;, &#39;med&#39;, &#39;low&#39;], [&#39;med&#39;, &#39;low&#39;, &#39;5more&#39;, &#39;2&#39;, &#39;big&#39;, &#39;med&#39;], [&#39;med&#39;, &#39;med&#39;, &#39;2&#39;, &#39;more&#39;, &#39;big&#39;, &#39;high&#39;], [&#39;med&#39;, &#39;med&#39;, &#39;2&#39;, &#39;more&#39;, &#39;med&#39;, &#39;med&#39;], [&#39;vhigh&#39;, &#39;vhigh&#39;, &#39;2&#39;, &#39;2&#39;, &#39;med&#39;, &#39;low&#39;], [&#39;high&#39;, &#39;med&#39;, &#39;4&#39;, &#39;2&#39;, &#39;big&#39;, &#39;low&#39;], [&#39;low&#39;, &#39;low&#39;, &#39;2&#39;, &#39;4&#39;, &#39;big&#39;, &#39;med&#39;]]car_labels = [&#39;acc&#39;, &#39;acc&#39;, &#39;unacc&#39;, &#39;unacc&#39;, &#39;unacc&#39;, &#39;vgood&#39;, &#39;acc&#39;, &#39;unacc&#39;, &#39;unacc&#39;, &#39;good&#39;] information_gain 함수를 수정하여 가중 정보증가량을 계산가중치: 부분집합의 label 갯수 `len(subset)` / 분할 전의 집합의 label 갯수 `len(starting_labels)`In&nbsp;[&nbsp;]: def weighted_information_gain(starting_labels, split_labels): info_gain = gini(starting_labels) for subset in split_labels: info_gain -= gini(subset) * (len(subset) / len(starting_labels)) return info_gain 아래 split() 함수를 살펴보겠습니다.In&nbsp;[&nbsp;]: def split(dataset, labels, column): data_subsets = [] label_subsets = [] # empty list counts = list(set([data[column] for data in dataset])) # list 의 중복 항목 제거를 위한 set 변환 for k in counts: # k=counts element [&#39;2&#39;, &#39;4&#39;, &#39;more&#39;] new_data_subset = [] new_label_subset = [] for i in range(len(dataset)): # data set len -&gt; all looping if dataset[i][column] == k: new_data_subset.append(dataset[i]) new_label_subset.append(labels[i]) data_subsets.append(new_data_subset) label_subsets.append(new_label_subset) return data_subsets, label_subsets In&nbsp;[&nbsp;]: # split 함수 호출split_data, split_labels = split(cars, car_labels, 3) In&nbsp;[&nbsp;]: split_data Out[&nbsp;]:[[[&#39;med&#39;, &#39;vhigh&#39;, &#39;4&#39;, &#39;more&#39;, &#39;small&#39;, &#39;high&#39;], [&#39;med&#39;, &#39;med&#39;, &#39;2&#39;, &#39;more&#39;, &#39;big&#39;, &#39;high&#39;], [&#39;med&#39;, &#39;med&#39;, &#39;2&#39;, &#39;more&#39;, &#39;med&#39;, &#39;med&#39;]], [[&#39;med&#39;, &#39;low&#39;, &#39;3&#39;, &#39;4&#39;, &#39;med&#39;, &#39;med&#39;], [&#39;med&#39;, &#39;low&#39;, &#39;4&#39;, &#39;4&#39;, &#39;med&#39;, &#39;low&#39;], [&#39;low&#39;, &#39;low&#39;, &#39;2&#39;, &#39;4&#39;, &#39;big&#39;, &#39;med&#39;]], [[&#39;high&#39;, &#39;med&#39;, &#39;3&#39;, &#39;2&#39;, &#39;med&#39;, &#39;low&#39;], [&#39;med&#39;, &#39;low&#39;, &#39;5more&#39;, &#39;2&#39;, &#39;big&#39;, &#39;med&#39;], [&#39;vhigh&#39;, &#39;vhigh&#39;, &#39;2&#39;, &#39;2&#39;, &#39;med&#39;, &#39;low&#39;], [&#39;high&#39;, &#39;med&#39;, &#39;4&#39;, &#39;2&#39;, &#39;big&#39;, &#39;low&#39;]]]In&nbsp;[&nbsp;]: len(split_data) Out[&nbsp;]:3split_labels를 사용, index 3에 대해 스플릿한 information gainIn&nbsp;[&nbsp;]: # index 3으로 데이터를 분할하였을 때 정보증가량을 출력weighted_information_gain(car_labels, split_labels) Out[&nbsp;]:0.30666666666666675정보증가량을 찾는 과정을 모든 feature에 대해서 적용In&nbsp;[&nbsp;]: # 데이터에 있는 모든 feature들에 대하여 `split()` 함수와 `information_gain()` 함수를 호출 # 4th feature(persons feature)가 가장 큰 영향을 미친다.for i in range(0,6): split_data, split_labels = split(cars, car_labels, i) print(weighted_information_gain(car_labels, split_labels)) 0.27333333333333340.0400000000000000360.106666666666666660.306666666666666750.150000000000000020.29000000000000004&#49892;&#49845; 4: &#51116;&#44480; &#53944;&#47532; &#47564;&#46308;&#44592; (Recursive Tree Building)&#182;데이터를 분할하였을 때 정보증가량이 가장 높은 feature을 찾을 수 있습니다. 이 방법을 반복하는 재귀 알고리즘을 통하여 트리를 구성할 수 있습니다. 데이터의 모든 instance에서 시작하여 데이터를 분할할 가장 좋은 feature을 찾고, 그 feature에 대해서 데이터를 나눈 후에 생성된 부분집합에 대해서 재귀적으로 위의 순서를 되풀이합니다.정보증가량이 일어나지 않는 feature을 찾을 때까지 재귀를 반복합니다. 다른 말로, 우리는 더이상 불순도가 없는 부분집합을 만드는 분할이 존재하지 않을 때 결정 트리의 끝마디를 생성합니다. 이 끝마디는 전체 데이터에서 분류된 instance의 class을 담고 있습니다.In&nbsp;[&nbsp;]: # 위의 함수들을 종합하여 가장 적합한 분할 feature을 찾는 함수 작성def find_best_split(dataset, labels): best_gain = 0 best_feature = 0 for feature in range(len(dataset[0])): data_subset, label_subset = split(dataset, labels, feature) gain = weighted_information_gain(labels, label_subset) if gain &gt; best_gain: best_gain, best_feature = gain, feature return best_gain, best_feature 위 함수를 cars와 car_labels에 대해 호출In&nbsp;[&nbsp;]: best_gain, best_feature = find_best_split(cars, car_labels) In&nbsp;[&nbsp;]: best_feature Out[&nbsp;]:3In&nbsp;[&nbsp;]: best_gain Out[&nbsp;]:0.30666666666666675data와 labels를 파라미터로 받는 build_tree()라는 함수를 선언이 함수는 재귀적으로 트리를 구성합니다.In&nbsp;[&nbsp;]: def build_tree(data, labels): best_gain, best_feature = find_best_split(data, labels) if best_gain == 0: return Counter(labels) data_subsets, label_subsets = split(data, labels, best_feature) branches = [] for i in range(len(data_subsets)): branch = build_tree(data_subsets[i], label_subsets[i]) branches.append(branch) return branches 만들어진 build_tree 함수 테스트In&nbsp;[&nbsp;]: def print_tree(node, spacing=&quot;&quot;): question_dict = {0: &quot;Buying Price&quot;, 1:&quot;Price of maintenance&quot;, 2:&quot;Number of doors&quot;, 3:&quot;Person Capacity&quot;, 4:&quot;Size of luggage boot&quot;, 5:&quot;Estimated Saftey&quot;} # Base case: 끝노드에 도달함 if isinstance(node, Counter): print (spacing + str(node)) return print (spacing + &quot;Splitting&quot;) # 분할 지점에서 각 브랜치에 대해 재귀적으로 print_tree 함수를 호출 for i in range(len(node)): print (spacing + &#39;--&gt; Branch &#39; + str(i)+&#39;:&#39;) print_tree(node[i], spacing + &quot; &quot;) In&nbsp;[&nbsp;]: # `build_tree` 함수와 `print_tree` 함수를 출력해봅니다.tree = build_tree(cars, car_labels)print_tree(tree) Splitting--&gt; Branch 0: Splitting --&gt; Branch 0: Counter({&#39;acc&#39;: 1}) --&gt; Branch 1: Counter({&#39;acc&#39;: 1}) --&gt; Branch 2: Counter({&#39;vgood&#39;: 1})--&gt; Branch 1: Splitting --&gt; Branch 0: Counter({&#39;unacc&#39;: 1}) --&gt; Branch 1: Counter({&#39;good&#39;: 1}) --&gt; Branch 2: Counter({&#39;acc&#39;: 1})--&gt; Branch 2: Counter({&#39;unacc&#39;: 4})&#49892;&#49845; 5: scikit-learn&#51004;&#47196; &#44396;&#54788;&#54616;&#45716; &#44208;&#51221;&#53944;&#47532;&#182;scikit-learn에서 제공되는 make_moons 데이터를 사용합니다.In&nbsp;[&nbsp;]: import matplotlib.pyplot as pltimport seaborn as snsfrom sklearn.datasets import make_moonsX, y = make_moons(noise=0.32, random_state=42, n_samples=250)sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y, marker=&quot;o&quot;, s=25, edgecolor=&quot;k&quot;, legend=False).set_title(&quot;Moon Data&quot;)plt.show() scikit-learn 패키지로 결정트리 구현DecisionTreeClassifier` 분류기를 사용합니다.[scikit-learn DecisionTreeClassifier documentation] (https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html?highlight=decisiontreeclassifier#sklearn.tree.DecisionTreeClassifier)In&nbsp;[&nbsp;]: from sklearn.tree import DecisionTreeClassifier In&nbsp;[&nbsp;]: dt = DecisionTreeClassifier() .fit() 메소드를 통해 데이터를 Tree에 훈련In&nbsp;[&nbsp;]: dt.fit(X,y) Out[&nbsp;]:DecisionTreeClassifier()정확도 확인In&nbsp;[&nbsp;]: dt.score(X,y) Out[&nbsp;]:1.0완성된 결정트리 시각화In&nbsp;[&nbsp;]: # classifier 결정트리를 시각화from sklearn.tree import export_graphviz # drawing graphs specified in DOT language scriptsfrom six import StringIOfrom IPython.display import Image import pydotplusdot_data = StringIO()export_graphviz(dt, out_file=dot_data)graph = pydotplus.graph_from_dot_data(dot_data.getvalue())graph.write_png(&#39;/tree.png&#39;)Image(graph.create_png()) Out[&nbsp;]:결정 경계 시각화In&nbsp;[&nbsp;]: from mglearn import plot_interactive_treeax = plot_interactive_tree.plot_tree_partition(X, y, dt)ax.set_title(&quot;first decision tree&quot;) Out[&nbsp;]:Text(0.5, 1.0, &#39;first decision tree&#39;) &#49892;&#49845; 6: &#44208;&#51221; &#53944;&#47532; &#44032;&#51648;&#52824;&#44592; (pruning)&#182;가지치기란 최대트리로 형성된 결정트리의 특정 노드 밑의 하부 트리를 제거하여 일반화 성능을 높히는 것을 의미합니다. 모든 끝노드의 불순도가 0인 트리를 full tree라고 하는데, 이 경우에는 분할이 너무 많이 과적합의 위험이 발생합니다. 과적합은 학습 데이터에 과하게 학습하여 실제 데이터에 오차가 증가하는 현상입니다. 이를 방지하기 위해서 적절한 수준에서 끝노드를 결합해주는 기법을 가지치기(pruning)이라고 합니다.scikit-learn DecisionTreeClassifier documentation새로운 결정트리를 생성 (깊이를 지정)In&nbsp;[&nbsp;]: pruned_dt = DecisionTreeClassifier(max_depth = 3)pruned_dt.fit(X, y)print(pruned_dt.score(X,y)) 0.888가지치기된 트리를 시각화이렇게 끝노드의 개수를 지정해주면 트리가 데이터에 더욱 잘 일반화됩니다.In&nbsp;[&nbsp;]: dot_data = StringIO()export_graphviz( pruned_dt, out_file=dot_data)graph = pydotplus.graph_from_dot_data(dot_data.getvalue())graph.write_png(&#39;/tree.png&#39;)Image(graph.create_png()) Out[&nbsp;]:결정 경계 시각화를 통한 비교In&nbsp;[&nbsp;]: ax = plot_interactive_tree.plot_tree_partition(X, y, pruned_dt)ax.set_title(&quot;first decision tree&quot;) Out[&nbsp;]:Text(0.5, 1.0, &#39;first decision tree&#39;) &#44284;&#51228;&#182;Kaggle의 Titanic 데이터를 사용타이타닉 데이터의 feature:Pclass: 승객 등급. 1등급=1, 2등급=2, 3등급=3Sex: 성별Age: 나이SibSp: 함께 탑승한 형제 또는 배우자 수Parch: 함께 탑승한 부모 또는 자녀 수Fare: 여객 운임Label: Survived 생존=1, 죽음=0데이터 파악 및 전처리In&nbsp;[&nbsp;]: import pandas as pddata_url = &quot;https://raw.githubusercontent.com/inikoreaackr/ml_datasets/main/titanic.csv&quot;data = pd.read_csv(data_url)data Out[&nbsp;]: PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked 0 1 0 3 Braund, Mr. Owen Harris male 22.0 1 0 A/5 21171 7.2500 NaN S 1 2 1 1 Cumings, Mrs. John Bradley (Florence Briggs Th... female 38.0 1 0 PC 17599 71.2833 C85 C 2 3 1 3 Heikkinen, Miss. Laina female 26.0 0 0 STON/O2. 3101282 7.9250 NaN S 3 4 1 1 Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35.0 1 0 113803 53.1000 C123 S 4 5 0 3 Allen, Mr. William Henry male 35.0 0 0 373450 8.0500 NaN S ... ... ... ... ... ... ... ... ... ... ... ... ... 886 887 0 2 Montvila, Rev. Juozas male 27.0 0 0 211536 13.0000 NaN S 887 888 1 1 Graham, Miss. Margaret Edith female 19.0 0 0 112053 30.0000 B42 S 888 889 0 3 Johnston, Miss. Catherine Helen \"Carrie\" female NaN 1 2 W./C. 6607 23.4500 NaN S 889 890 1 1 Behr, Mr. Karl Howell male 26.0 0 0 111369 30.0000 C148 C 890 891 0 3 Dooley, Mr. Patrick male 32.0 0 0 370376 7.7500 NaN Q 891 rows × 12 columns &lt;svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\" width=\"24px\"&gt; &lt;/svg&gt; In&nbsp;[&nbsp;]: data.columnsdata = data[[&#39;Survived&#39;, &#39;Pclass&#39;, &#39;Sex&#39;, &#39;Age&#39;, &#39;SibSp&#39;, &#39;Parch&#39;, &#39;Fare&#39;]]data.describe() Out[&nbsp;]: Survived Pclass Age SibSp Parch Fare count 891.000000 891.000000 714.000000 891.000000 891.000000 891.000000 mean 0.383838 2.308642 29.699118 0.523008 0.381594 32.204208 std 0.486592 0.836071 14.526497 1.102743 0.806057 49.693429 min 0.000000 1.000000 0.420000 0.000000 0.000000 0.000000 25% 0.000000 2.000000 20.125000 0.000000 0.000000 7.910400 50% 0.000000 3.000000 28.000000 0.000000 0.000000 14.454200 75% 1.000000 3.000000 38.000000 1.000000 0.000000 31.000000 max 1.000000 3.000000 80.000000 8.000000 6.000000 512.329200 &lt;svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\" width=\"24px\"&gt; &lt;/svg&gt; In&nbsp;[&nbsp;]: data = data.dropna() In&nbsp;[&nbsp;]: data.shape Out[&nbsp;]:(714, 7)In&nbsp;[&nbsp;]: data.dtypes Out[&nbsp;]:Survived int64Pclass int64Sex objectAge float64SibSp int64Parch int64Fare float64dtype: objectIn&nbsp;[&nbsp;]: from sklearn.preprocessing import LabelEncoderle = LabelEncoder()data.Sex = le.fit_transform(data.Sex) /usr/local/lib/python3.7/dist-packages/pandas/core/generic.py:5516: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.Try using .loc[row_indexer,col_indexer] = value insteadSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy self[name] = valueIn&nbsp;[&nbsp;]: data.dtypes Out[&nbsp;]:Survived int64Pclass int64Sex int64Age float64SibSp int64Parch int64Fare float64dtype: objectIn&nbsp;[&nbsp;]: y = data[&#39;Survived&#39;]X = data.drop(columns = [&#39;Survived&#39;]) 기계학습 모델을 훈련시키고 성능을 파악하기 위해서는 데이터를 훈련 데이터와 테스트 데이터로 나누어야 합니다.&lt;img src=https://upload.wikimedia.org/wikipedia/commons/b/bb/ML_dataset_training_validation_test_sets.png width=500px&gt;scikit-learn에서 지원하는 train_test_split 을 사용합니다. scikit-learn train_test_split documentationIn&nbsp;[&nbsp;]: from sklearn.model_selection import train_test_splitX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3) DecisionTreeClassifier 분류기를 사용해 결정트리를 생성In&nbsp;[&nbsp;]: dt = DecisionTreeClassifier() 트레이팅 데이터에 .fit() 메소드를 호출함으로써 트리를 데이터에 훈련, .fit()메소드는 training_points와 training_labels을 파라미터로 받음.In&nbsp;[&nbsp;]: dt.fit(X_train, y_train) Out[&nbsp;]:DecisionTreeClassifier()testing_points와 testing_labels에 대한 결정 트리의 정확도(.score())를 출력In&nbsp;[&nbsp;]: print(dt.score(X_test, y_test)) 0.8046511627906977훈련된 트리 시각화In&nbsp;[&nbsp;]: dot_data = StringIO()export_graphviz(dt, out_file=dot_data)graph = pydotplus.graph_from_dot_data(dot_data.getvalue())graph.write_png(&#39;/tree.png&#39;)Image(graph.create_png()) Out[&nbsp;]:scikit-learn DecisionTreeClassifier documentation과제 1max_leaf_nodes : 트리가 가질 수 있는 최대 leaf node의 수를 지정하는 parameter입니다. 해당 parameter가 지정되지 않은 경우 leaf node 수의 제한을 두지 않습니다.max_depth : 트리의 최대 깊이 parameter가 None일 때는 node가 모든 잎들이 pure해질 때까지 혹은, min_samples_split 이 지정하는 값 이하의 수를 포함하도록 확장됩니다.min_sample_split : node를 분할하기 위해 필요한 최소 샘플 수를 지정하는 parameter로 정수일 때는 최소 숫자로 간주되고 float일 때는 전체 샘플에 대한 min_sample_split의 비율만큼을 최소 샘플로 합니다.min_sample_leaf : leaf nodes에 필요한 최소 샘플의 수르 지정하는 parameter로 양쪽 가지에 필요한 최소 training sample을 의미합니다. 특히, regression에서는 모형을 smoothing하는데 효과가 있습니다.min_impurity_decrease : impurity가 감소하는 경우 node를 분할하도록 합니다.In&nbsp;[&nbsp;]: from sklearn.tree import DecisionTreeClassifierdf = DecisionTreeClassifier() In&nbsp;[&nbsp;]: dt_mxlf = DecisionTreeClassifier(max_leaf_nodes = 1000)dt_mxdth = DecisionTreeClassifier(max_depth = 6)dt_mss = DecisionTreeClassifier(min_samples_split = 8)dt_msl = DecisionTreeClassifier(min_samples_leaf = 32) dt_mid = DecisionTreeClassifier(min_impurity_decrease = 0.02) In&nbsp;[&nbsp;]: # prunig parameter를 조절하지 않은 Decision treedt.fit(X_train, y_train)dt.score(X_test, y_test) Out[&nbsp;]:0.8232558139534883In&nbsp;[&nbsp;]: # `max_leaf_nodes` parameter를 조절한 Decision treedt_mxlf.fit(X_train, y_train)dt_mxlf.score(X_test, y_test) Out[&nbsp;]:0.8186046511627907In&nbsp;[&nbsp;]: # `max_depth` parameter를 조절한 Decision treedt_mxdth.fit(X_train, y_train)dt_mxdth.score(X_test, y_test) Out[&nbsp;]:0.8372093023255814In&nbsp;[&nbsp;]: #`min_sample_split` parameter를 조절한 Decision treedt_mss.fit(X_train, y_train)dt_mss.score(X_test, y_test) Out[&nbsp;]:0.8In&nbsp;[&nbsp;]: #`min_sample_leaf` parameter를 조절한 Decision treedt_msl.fit(X_train, y_train)dt_msl.score(X_test, y_test) Out[&nbsp;]:0.827906976744186In&nbsp;[&nbsp;]: #`min_impurity_decrease` parameter를 조절한 Decision treedt_mid.fit(X_train, y_train)dt_mid.score(X_test, y_test) Out[&nbsp;]:0.8과제3prunig 파라미터를 조정하지 않은 Decision tree의 경우 분류 정확도가 [0.7488372093023256]로 pruning 파라미터를 조정한 Decision tree의 분류 정확도가 각각 [0.7813953488372093, 0.8, 0.7813953488372093, 0.813953488372093, 0.7813953488372093] 인 것과 비교해 볼 때 정확도가 떨어집니다.In&nbsp;[&nbsp;]: # `max_leaf_nodes` parameter를 조절한 Decision treedot_data = StringIO()export_graphviz(dt_mxlf, out_file=dot_data)graph = pydotplus.graph_from_dot_data(dot_data.getvalue())graph.write_png(&#39;/tree.png&#39;)Image(graph.create_png()) Out[&nbsp;]:In&nbsp;[&nbsp;]: # `max_depth` parameter를 조절한 Decision treedot_data = StringIO()export_graphviz(dt_mxdth, out_file=dot_data)graph = pydotplus.graph_from_dot_data(dot_data.getvalue())graph.write_png(&#39;/tree.png&#39;)Image(graph.create_png()) Out[&nbsp;]:In&nbsp;[&nbsp;]: #`min_sample_split` parameter를 조절한 Decision treedot_data = StringIO()export_graphviz(dt_mss, out_file=dot_data)graph = pydotplus.graph_from_dot_data(dot_data.getvalue())graph.write_png(&#39;/tree.png&#39;)Image(graph.create_png()) Out[&nbsp;]:In&nbsp;[&nbsp;]: #`min_sample_leaf` parameter를 조절한 Decision treedot_data = StringIO()export_graphviz(dt_msl, out_file=dot_data)graph = pydotplus.graph_from_dot_data(dot_data.getvalue())graph.write_png(&#39;/tree.png&#39;)Image(graph.create_png()) Out[&nbsp;]:In&nbsp;[&nbsp;]: #`min_impurity_decrease` parameter를 조절한 Decision treedot_data = StringIO()export_graphviz(dt_mid, out_file=dot_data)graph = pydotplus.graph_from_dot_data(dot_data.getvalue())graph.write_png(&#39;/tree.png&#39;)Image(graph.create_png()) Out[&nbsp;]:과제 5번max_leaf_nodes, max_depth, min_samples_splitparameter를 조정한 모델은 모두 트리 모델의 끝마디에서 Gini index는 높으나 node가 가지고 있는 sample의 수가 1 또는 2로 overfitting 되었을 확률이 높은 노드들을 다수 가지고 있으므로 적합하게 학습된 모델이라 할 수 없습니다. 한편 min_impurity_decrease parameter를 조절한 Decision tree 모델의 경우에는 tree 의 끝마디 noder가 가진 gini index가 다소 높게 측정되었기 때문에 적합되었다고 보기 어렵습니다.반면 'min_samples_leaf` parameter를 조절한 Decision tree는 하나의 끝마디에서 gini index가 다소 높게 측정되기는 했지만 각각의 끝마디 gini index가 낮게 나왔으며 각 끝마디의 샘플 수도 적정하기 때문에 다른 parameter를 조절한 Decision tree에 비교해 가장 적합하게, 가장 일반화를 잘 실행한 모델이라 볼 수 있을 것입니다."
    } ,
  
    {
      "title"       : "ML summary - Multiple Variable Linear regression",
      "category"    : "",
      "tags"        : "machine learning, study_theory, Decision Tree",
      "url"         : "./ML-summary-multipleLinearRegression.html",
      "date"        : "2021-03-24 00:00:00 +0900",
      "description" : "ML summary - Multiple Variable Linear regression",
      "content"     : "Multiple Variable Regressionfeature = dimension= attributeNotation\\(n = number of features\\\\x^{i} = input(features) of i_{th} training example \\\\ x_{j}^{i} = value if features j in i_{th} training example\\\\\\)기본적으로 linear Regression 상의 모든 벡터는 comlumn vector로 나타내며 필요한 경우 row vector를 통해 나타내게 된다.Hypothesis: \\(h_{\\theta}(x) = \\theta_{0} + \\theta_{1}x\\)이전의 식으로부터 새로운 가설을 이끌어 낼 수 있다.Hypothesis: \\(h_{\\theta}(x) = \\theta_{0} + \\theta_{1}x_{1} + ... + \\theta_{n}x_{n}\\)위와 같이 모델을 simple하게 확장할 수 있으며 각각의 벡터의 column vector를 통해 좀 더 손쉽게 나타낼 수 있다.\\[x = \\begin{bmatrix} x_{0} \\\\ x_{1} \\\\ ... \\\\ x_{n}\\end{bmatrix} \\in R^{n+1} \\quad \\theta = \\begin{bmatrix} \\theta_{0} \\\\ \\theta_{1} \\\\ ... \\\\ \\theta_{n}\\end{bmatrix} \\in R^{n+1} \\\\ \\theta^{T} x = h_\\theta(x) = \\theta_{0} + \\theta_{1}x_{1} + ... + \\theta_{n}x_{n}\\]Feature ScailingIdea : Make Sure features are on a similar scaleE.g. 집의 가격을 결정하는 요인에 대해서 생각해보자.x1 = size (0~2000 feet^2)x2 = number of bedrooms (1~5)두 feature를 별개의 요소이다. 두 개의 요소가 모두 엇비슷하게 영향을 미친다고 가정한다면 x1은 최대 2000의 값을 가지고 계수 값도 그만큼 클 것이다. 두 features가 가지는 가중치 값이 상이하게 되고, 이를 randomize하게 학습하게 되면 x1의 변화값이 크기 때문에 계수값에 대해서 모델의 예측치의 변화폭이 커지게 된다.x1의 변화폭이 10인 것에 비해 x2의 변화폭이 1000이라 가정한다면, x1의 변화폭이 큰 영향을 미치지 않는 것에 비해 x2의 변화폭은 큰 영향을 미치게 된다. 따라서 가중치(계수값)을 비슷하게 만들어 주기 위해 두 feature의 요소 값의 범위를 비슷하게 만들어주는 것이 필요하다. (nomalization) 학습 시간을 단축시켜주는 장점도 있다.Feature scailing은 최대값만 고려하는 방법도 있으며 최소값을 함께 고려하는 방법이 있다.최소값을 고려하는 방식에서는 실제값에서 최소값을 뺀 값을 최대값과 최소값을 뺀 값으로 나누어 준다.Mean NormalizationReplace xi with xi - mi to make features have approximately zero mean.E.g.\\(x_{1} = {size-1000\\over 2000} \\\\ x_{2} = {\\#bedrooms -2 \\over 5} \\\\\\)\\(-0.5 \\le x_{1} \\le0.5 \\quad\\&amp;\\quad -0.5 \\le x_{2} \\le0.5\\)아래와 같은 예제가 있다고 가정을 하자, 데이터의 범위가 18~55라고 하면, 이를 nomalization을 했을 때, -1에서 1사이의 값에 데이터 값들이 적절하게 분배될 것이다.\\[Mean(\\mu) : 30 \\\\ Variance(\\sigma) : 10^{2}\\]하지만 만약 outlier로 3000의 값을 가지는 데이터가 존재한다고 가정한다면 1의 값에 3000에 해당하는 데이터가 배정되도 나머지 18~55의 값을 가지는 데이터는 한없이 -1에 가까운 위치로만 배정이 될 것이다. max와 min에 가까운 몇 개의 outlier들이 기준이 된다면 이와 같은 문제가 발생한다. 따라서 outlier로 인해 데이터들이 -1에만 치중되는 결과를 피하기 위해 standard nomalization을 할 수 있다. standard nomalization은 몇 개의 기준은 아주 방대한 양의 데이터 일부로 전락하게 하기 때문에 주로 사용된다.Gradient Descent\\(x = \\begin{bmatrix} x_{0} \\\\ x_{1} \\\\ ... \\\\ x_{n}\\end{bmatrix} \\in R^{n+1} \\quad \\theta = \\begin{bmatrix} \\theta_{0} \\\\ \\theta_{1} \\\\ ... \\\\ \\theta_{n}\\end{bmatrix} \\in R^{n+1} \\\\ \\theta^{T} x = h_\\theta(x) = \\theta_{0} + \\theta_{1}x_{1} + ... + \\theta_{n}x_{n}\\)Previous\\[{\\partial \\over \\partial \\theta_{j}} J(\\theta_{0}, \\theta_{1}) = ...\\\\j = 0 : \\quad{\\partial \\over \\partial \\theta_{0}} J(\\theta_{0}, \\theta_{1}) = {1 \\over m}\\Sigma_{i=1}^{m}(h_{\\theta(x^{i}) - y^i}) \\\\j = 1 : \\quad{\\partial \\over \\partial \\theta_{1}} J(\\theta_{0}, \\theta_{1}) = {1 \\over m}\\Sigma_{i=1}^{m}(h_{\\theta(x^{i}) - y^i})* x^i\\]In polynomialx1　= 1　3x2　= 2　-1y　 = 5　7\\[{\\partial \\over \\partial \\theta_{j}} J(\\theta_{0}, \\theta_{1}, ...) = ...\\\\\\theta_{j} := \\theta_{j} - \\alpha{1 \\over m} \\Sigma_{i=1}^{m}(h_{\\theta(x^{i}) - y^i})* x_j^i \\\\j = 0 : \\quad {\\partial \\over \\partial \\theta_{0}} J(\\theta_{0}, \\theta_{1}, \\theta_{2}) =\\theta_0 - \\alpha{1 \\over m}\\Sigma_{i=1}^{m}(h_{\\theta(x^{i}) - y^i})*x_0^i \\\\j = 1 : \\quad{\\partial \\over \\partial \\theta_{1}} J(\\theta_{0}, \\theta_{1}, \\theta_{2}) = \\theta_0 - \\alpha{1 \\over m}\\Sigma_{i=1}^{m}(h_{\\theta(x^{i}) - y^i})* x_1^i \\\\j = 2 : \\quad{\\partial \\over \\partial \\theta_{2}} J(\\theta_{0}, \\theta_{1}, \\theta_{2}) = \\theta_0 - \\alpha{1 \\over m}\\Sigma_{i=1}^{m}(h_{\\theta(x^{i}) - y^i})* x_2^i\\]Polynomial RegressionLinear Regression에서는 완벽히 비례적인 데이터에 대해서만 훌륭하게 작동할 것이다. 집값 예측을 위한 Linear Regression이 존재한다면 집의 면적에 비례적으로 각 평수에 평당 가격 정도를 곱하는 정도의 예측밖에는 하지 못하게 된다. 따라서 선형적인 분류를 따르지 않는 데이터를 분석하기 위해 다른 모델이 필요하다.E.g.\\[\\theta_0 + \\theta_1 x + \\theta_2 {x}^2 \\\\\\theta_0 + \\theta_1 x + \\theta_2 \\sqrt{x} \\\\\\theta_0 + \\theta_1 x + \\theta_2 x^2 + \\theta x^3\\]root 가 들어간 Linear Regression 문제를 풀기 위해서는 x의 root에 해당하는 columns 을 추가한다.\\[x1　= 1　\\quad 2　\\quad 4\\\\ \\sqrt{x}　= 1　\\quad \\sqrt 2　\\quad 2\\\\ y　 = 4　\\quad 7　\\quad 9\\]위에서의 식과 동일하게 x2의 자리에 x1의 root에 해당하는 값을 이용하면 root가 포함된 polynomial Regression의 Gradient Descent를 구할 수 있다.Office hour머신러닝 vs 딥러닝과거에는 컴퓨팅 자원의문제로 딥러닝이 불가능했다. 주로 저장장치, GPU의 한계로 인해 딥러닝에 필요한 자원의 필요를 만족시키는게 어려웠다. Dataset의 미비도 딥러닝이 어려웠던 이유로 알려져 있다. 현재는 많은 개방형 데이터들이 존재하고 이와 같은 많은 데이터와 과거와 비교해 충분히 휼륭해진 컴픁 자원을 통한 연산으로 딥러닝을 수행하는 것이 가능해졌다.머신러닝의 이슈는 주어진 한정된양의 데이터를 통해 수학적인 수식을 통해 원하는 바를 얻어낼 수 있을까에 관란 것이었다. 이에 비해 딥러닝은 엄청난 수의 데이터 학습을 통해 결과를 도출해 내는 것이기 때문에 현재 머신러닝의 학습 효율은 딥러닝만큼 좋게 나타나지는 않고 있다. 그렇다면 머신러닝을 해야하는 이유는 무엇인가. 딥러닝 모델 디자인에서 좋은 학습을 위해서는 variant 조정과 같은 trick들이 필요하다. 이는 결국 머신러닝에서 시작된 수학적 수식에서 비롯된다. 간단한 예로 어떻게 variant를 줄이는가에 대해 loss function을 적용하는 방법이 있을 것이다."
    } ,
  
    {
      "title"       : "ML summary - simple regression",
      "category"    : "",
      "tags"        : "machine learning, study_theory, linear regression",
      "url"         : "./ML-summary-simpleRegression.html",
      "date"        : "2021-03-24 00:00:00 +0900",
      "description" : "ML summary - simple regression",
      "content"     : "Simple Regression AURIWALLLinear Regression직선식 형태의 회귀 방법 기계학습이란 예측을 하기 위해서 주어지는 Data Item (과거 데이터)에 의한 예측(출력)supervised Learning예측 모델에서 어떤 과거의 데이터를 통해 새로운 상황에 대한 예측을 만들어내는 방법* Clustering unsupervised Learning과거의 데이터를 바탕으로 하지 않는 방식Notationm = Number Of training examplesx's = input variable / featuresy's = output variable / target variableTraining Set -&gt; Learning Algorithm -&gt; h (Estimated)Cost functionHypothesis : \\(h_{\\theta}(x) = \\theta_{0} + \\theta_{1}x \\\\ \\theta_{i}'s : parameters\\)Idea : 각각의 theta 를 선정해 h(x)의 값이 training set (x, y)의 y와 가장 근접하도록 하는 것 (MAE, MSE) Squared error functionHypothesis: \\(h_{\\theta}(x) = \\theta_{0} + \\theta_{1}x\\)Parameters : \\(\\theta_{0} , \\theta_{1}\\)Cost function :\\(J(\\theta_{0}, \\theta_{1}) = {1\\over2m}\\Sigma_{i=1}^{m}(h_{\\theta}x^{i} - y^{i})^{2}\\)Goal : \\(minimize _{\\theta_{0}, \\theta_{1}} J(\\theta_{0}, \\theta_{1})\\)이를 단순하게 표현하면Hypothesis: \\(h_{\\theta}(x) = \\theta_{1}x\\)Parameters : \\(\\theta_{1}\\)Cost function :\\(J(\\theta_{1}) = {1\\over2m}\\Sigma_{i=1}^{m}(h_{\\theta}x^{i} - y^{i})^{2}\\)Goal : \\(minimize _{\\theta_{1}} J(\\theta_{1})\\)Gradient descent\\(J(\\theta_{1}, \\theta_{1})\\) 에 대해 우리가 원하는 것은 이의 최솟값을 찾는 것이다.\\(minimize _{\\theta_{0}, \\theta_{1}} J(\\theta_{0}, \\theta_{1})\\)하지만 계속 변화하는 \\(\\theta_{1}, \\theta_{1}\\) 에 대해서 우리는 여러 개의 minimum value들을 가질 수 있고 이를 복수형의 minima로 표현한다.이러한 minima들은 \\(\\theta_{1}, \\theta_{1}\\) 값의 변화에 따라 국소적인 최소값들을 가지며 이를 Local minima라고 한다. 어떤 수식에 대해 J는 기울기 값을 가지는데 이를 수정하기 위해 Learning rate가 사용된다."
    } 
  
]
