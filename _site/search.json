[
  
    {
      "title"       : "Machine Learning - Simple Regression",
      "category"    : "",
      "tags"        : "machine learning, study_model, linear regression",
      "url"         : "./MachineLearning_Simple.html",
      "date"        : "2021-03-24 00:00:00 +0900",
      "description" : "ML Basic",
      "content"     : "Simple Regression &#49892;&#49845; 1&#52264;&#49884; - ZeroR, OneR, Naive Bayes Classifier&#182;&#49892;&#49845; &#45236;&#50857;:&#182;ZeroROneRNaive Bayes classifierIn&nbsp;[&nbsp;]: import numpy as npimport pandas as pd In&nbsp;[&nbsp;]: import sklearnprint(sklearn.__version__) 1.0.2&#49892;&#49845; &#45936;&#51060;&#53552;&#182;In&nbsp;[&nbsp;]: # 데이터 받기url = &quot;https://raw.githubusercontent.com/inikoreaackr/ml_datasets/main/playgolf.csv&quot;df = pd.read_csv(url) In&nbsp;[&nbsp;]: # 데이터 첫 다섯 instance 확인df.head() Out[&nbsp;]: OUTLOOK TEMPERATURE HUMIDITY WINDY PLAY GOLF 0 Rainy Hot High False No 1 Rainy Hot High True No 2 Overcast Hot High False Yes 3 Sunny Mild High False Yes 4 Sunny Cool Normal False Yes &lt;svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\" width=\"24px\"&gt; &lt;/svg&gt; In&nbsp;[&nbsp;]: # 데이터 타입 확인df.dtypes Out[&nbsp;]:OUTLOOK objectTEMPERATURE objectHUMIDITY objectWINDY boolPLAY GOLF objectdtype: objectIn&nbsp;[&nbsp;]: # object 타입을 category로 변경for col in df.columns: df[col] = df[col].astype(&#39;category&#39;) In&nbsp;[&nbsp;]: # 변경이 되었는지 확인df.dtypes Out[&nbsp;]:OUTLOOK categoryTEMPERATURE categoryHUMIDITY categoryWINDY categoryPLAY GOLF categorydtype: object1. ZeroR&#182;ZeroR은 가장 간단한 분류 방법이며, 다른 모든 feature들을 무시하고 label에만 의존합니다.ZeroR 분류기는 단순히 데이터의 class를 다수 카테고리로 예측합니다.ZeroR에는 예측 능력이 없지만, 이것은 표준 성능을 가늠하여 다른 분류 방법 성능의 기준점이 됩니다.In&nbsp;[&nbsp;]: # PLAY GOLF feature 출력df[&#39;PLAY GOLF&#39;] Out[&nbsp;]:0 No1 No2 Yes3 Yes4 Yes5 No6 Yes7 No8 Yes9 Yes10 Yes11 Yes12 Yes13 NoName: PLAY GOLF, dtype: categoryCategories (2, object): [&#39;No&#39;, &#39;Yes&#39;]In&nbsp;[&nbsp;]: # PLAY GOLF는 binary 변수입니다. 각 카테고리의 갯수를 세어봅니다.df[&#39;PLAY GOLF&#39;].value_counts(sort = True) Out[&nbsp;]:Yes 9No 5Name: PLAY GOLF, dtype: int64In&nbsp;[&nbsp;]: # 이 데이터셋에서 &quot;Play Golf = Yes&quot;로 예측하는 ZeroR 모델의 정확도를 계산해봅니다.9 / (9 + 5) Out[&nbsp;]:0.6428571428571429 &lt;img src=https://www.saedsayad.com/images/ZeroR_3.png&gt;위의 데이터셋에서 \"Play Golf = Yes\"로 예측하는 ZeroR모델의 정확도는 0.64가 됩니다.OneR&#182;OneR은 One Rule의 약자이며, 간단하고 정확한 분류 알고리즘입니다.OneR은 데이터의 각 feature 마다 하나의 룰 셋(Rule Set)을 생성합니다. 그리고 생성된 룰 셋 중에서, 전체데이터에 대해 오차가 가장 작은 룰 셋을 One Rule로 결정합니다.각 feature당 룰 셋은 frequency table을 이용하여 만들 수 있습니다.OneR Algorithm각 feature 마다, 각 feature의 value 마다, 룰을 아래와 같이 만듭니다. 그 feature의 value에 해당되는 instance중에 target class가 몇개인지 셉니다. 가장 갯수가 많은 class를 찾습니다. 그 feature의 value가 해당되면 그 갯수가 많은 class로 예측되도록 룰을 하나 만듭니다. 각 feature의 룰들의 전체 에러를 계산합니다. (반대로 정확도를 계산할 수도 있습니다.)가장 작은 에러를 보이는 feature을 선택합니다.아래 그림에서는 outlook과 humidity feature 모두 에러의 갯수가 4이므로 제일 작습니다. 하지만 활동에서는 첫번째 feature인 outlook만 고려할 것입니다.For example:&lt;img src=https://www.saedsayad.com/images/OneR_1.png&gt; In&nbsp;[&nbsp;]: # 수도코드 구현from collections import Countertotal_errors = []for col in df.columns[:-1]: error = 0 for val in df[col].unique(): length = len(df[df[col] == val]) print(f&quot;{col} : {val}, length : {length}&quot;) print(Counter(df[df[col] == val][&#39;PLAY GOLF&#39;]).most_common()) error += (length - Counter(df[df[col] == val][&#39;PLAY GOLF&#39;]).most_common()[0][1]) print(f&quot;\\nerror of {col}: [{error}] \\n&quot;) total_errors.append(error) OUTLOOK : Rainy, length : 5[(&#39;No&#39;, 3), (&#39;Yes&#39;, 2)]OUTLOOK : Overcast, length : 4[(&#39;Yes&#39;, 4)]OUTLOOK : Sunny, length : 5[(&#39;Yes&#39;, 3), (&#39;No&#39;, 2)]error of OUTLOOK: [4] TEMPERATURE : Hot, length : 4[(&#39;No&#39;, 2), (&#39;Yes&#39;, 2)]TEMPERATURE : Mild, length : 6[(&#39;Yes&#39;, 4), (&#39;No&#39;, 2)]TEMPERATURE : Cool, length : 4[(&#39;Yes&#39;, 3), (&#39;No&#39;, 1)]error of TEMPERATURE: [5] HUMIDITY : High, length : 7[(&#39;No&#39;, 4), (&#39;Yes&#39;, 3)]HUMIDITY : Normal, length : 7[(&#39;Yes&#39;, 6), (&#39;No&#39;, 1)]error of HUMIDITY: [4] WINDY : False, length : 8[(&#39;Yes&#39;, 6), (&#39;No&#39;, 2)]WINDY : True, length : 6[(&#39;No&#39;, 3), (&#39;Yes&#39;, 3)]error of WINDY: [5] In&nbsp;[&nbsp;]: # 오류가 가장 작은 feature를 고릅니다.best_feature = df.columns[np.argmin(total_errors)]print(best_feature) OUTLOOKIn&nbsp;[&nbsp;]: # best feature에 대해 룰셋을 생성합니다.oneRules = []for val in df[best_feature].unique(): print(f&quot;{best_feature} : {val}&quot;, &quot;-&gt; &quot;, end = &#39; &#39;) print(Counter(df[df[best_feature] == val][&#39;PLAY GOLF&#39;]).most_common()[0][0]) oneRules.append((best_feature, val, Counter(df[df[best_feature] == val][&#39;PLAY GOLF&#39;]).most_common()[0][0])) OUTLOOK : Rainy -&gt; NoOUTLOOK : Overcast -&gt; YesOUTLOOK : Sunny -&gt; YesThe best feature is:&lt;img src=https://www.saedsayad.com/images/OneR_3.png&gt;Naive Bayes Classifier with scikit-learn&#182;scikit-learn의 Naive Bayes classifier 다큐멘테이션: https://scikit-learn.org/stable/modules/naive_bayes.htmlIn&nbsp;[&nbsp;]: df.head() Out[&nbsp;]: OUTLOOK TEMPERATURE HUMIDITY WINDY PLAY GOLF 0 Rainy Hot High False No 1 Rainy Hot High True No 2 Overcast Hot High False Yes 3 Sunny Mild High False Yes 4 Sunny Cool Normal False Yes &lt;svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\" width=\"24px\"&gt; &lt;/svg&gt; In&nbsp;[&nbsp;]: df.describe() Out[&nbsp;]: OUTLOOK TEMPERATURE HUMIDITY WINDY PLAY GOLF count 14 14 14 14 14 unique 3 3 2 2 2 top Rainy Mild High False Yes freq 5 6 7 8 9 &lt;svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\" width=\"24px\"&gt; &lt;/svg&gt; In&nbsp;[&nbsp;]: # 카테고리 데이터를 정수로 인코딩df_enc = pd.DataFrame()df_enc[&#39;OUTLOOK&#39;] = df[&#39;OUTLOOK&#39;].cat.codesdf_enc[&#39;TEMPERATURE&#39;] = df[&#39;TEMPERATURE&#39;].cat.codesdf_enc[&#39;HUMIDITY&#39;] = df[&#39;HUMIDITY&#39;].cat.codesdf_enc[&#39;WINDY&#39;] = df[&#39;WINDY&#39;].cat.codesdf_enc[&#39;PLAY GOLF&#39;] = df[&#39;PLAY GOLF&#39;].cat.codes In&nbsp;[&nbsp;]: df_enc.head() Out[&nbsp;]: OUTLOOK TEMPERATURE HUMIDITY WINDY PLAY GOLF 0 1 1 0 0 0 1 1 1 0 1 0 2 0 1 0 0 1 3 2 2 0 0 1 4 2 0 1 0 1 &lt;svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\" width=\"24px\"&gt; &lt;/svg&gt; In&nbsp;[&nbsp;]: # 인코딩된 데이터의 타입을 프린트해봅니다.df_enc.dtypes Out[&nbsp;]:OUTLOOK int8TEMPERATURE int8HUMIDITY int8WINDY int8PLAY GOLF int8dtype: objectIn&nbsp;[&nbsp;]: # 분류기에 넣을 feature과 해당 label을 구분합니다.features = df_enc.drop(columns=[&#39;PLAY GOLF&#39;])label = df_enc[&#39;PLAY GOLF&#39;] In&nbsp;[&nbsp;]: from sklearn.naive_bayes import CategoricalNBmodel = CategoricalNB() In&nbsp;[&nbsp;]: model.fit(features.values, label) Out[&nbsp;]:CategoricalNB()In&nbsp;[&nbsp;]: score = model.score(features.values, label)score Out[&nbsp;]:0.9285714285714286In&nbsp;[&nbsp;]: # p(x_i|y_i) 출력from pprint import pprintfeature_log_prior = model.feature_log_prob_for feature_prior in feature_log_prior: pprint(np.exp(feature_prior)) array([[0.125 , 0.5 , 0.375 ], [0.41666667, 0.25 , 0.33333333]])array([[0.25 , 0.375 , 0.375 ], [0.33333333, 0.25 , 0.41666667]])array([[0.71428571, 0.28571429], [0.36363636, 0.63636364]])array([[0.42857143, 0.57142857], [0.63636364, 0.36363636]])In&nbsp;[&nbsp;]: # p(y_j) 출력np.exp(model.class_log_prior_) Out[&nbsp;]:array([0.35714286, 0.64285714])In&nbsp;[&nbsp;]: # instances에 대해서 예측을 해봅니다. # (&quot;Sunny&quot;, &quot;Hot&quot;, &quot;Normal&quot;, False) [2, 1, 1, 0]# (&quot;Rainy&quot;, &quot;Mild&quot;, &quot;High&quot;, False) [1, 2, 0, 0]print(model.predict_proba([[2, 1, 1, 0]]), model.predict([[2, 1, 1, 0]]))print(model.predict_proba([[1, 2, 0, 0]]), model.predict([[1, 2, 0, 0]])) [[0.22086561 0.77913439]] [1][[0.5695011 0.4304989]] [0]1. &#44592;&#44228;&#54617;&#49845; &#44592;&#48376; &#50857;&#50612; &#51221;&#51032;/&#51032;&#48120;&#182;기계학습: 인공지능의 한 분야로 사람의 학습과 같은 능력을 컴퓨터를 통해 실현하고자 하는 기술로, 데이터로부터 모델을 만들어내는 과정을 의미한다.Label (=target): label은 model이 예측하려는 값으로 training을 한 후의 output이다. 데이터를 차별화 할 수 있는 범주이다.class: 데이터를 분류하는 범주Features: feature은 training data의 분석 대상이 되는 속성들로 input set에 있는 column으로 표현된다.Input: input은 일반적으로 model에 전당되는 데이터 집합(X)를 나타낸다. 예를 들어 (X,Y(label)) 형태의 데이터 세트에서 X는 입력이며 레이블인 Y는 대상 또는 출력이 된다.Numerical data (=Quantitative data): Numerical data는 숫자로 표현되는 것을 의미한다.Categorical data (=Qualitative data): Categorical data는 일반적으로 숫자로 표현되지 않는 것을 의미하며 이산적인 형태의 데이터를 표현하기 위해 사용된다.Unlabeled example (=instance): Unlabeled example은 주로 unsupervised learning에 사용되는 데이터로 의미 있는 label이 존재하지 않는다. training data의 한 예시로 예상되는 결과에 대해 정보를 내포하지 않은 데이터이다.Labeled example:labeled example은 주로 supervised learning에 사용되는 데이터로 의미 있는 label이나 class를 가지고 있다.Training (=learning): 주로 성능의 향상을 의미한다. 경험(data)에 따라 예측하려는 값에 대해 배우는 것으로 명시적인 지시가 아닌 경험에 의해 작업의 성능이 향상되는 것을 의미한다.Predict (=inference): prediction이란 과거의 data set에 대한 training을 거쳐 특정 결과의 가능성을 예측하는 model의 출력을 의미한다.Train Set: Train set은 기계 학습 model을 훈련하는데 사용되는 데이터이다.Test Set: Test set은 학습된 모델을 테스트하기 위한 data의 하위 set이다.Regression: feature와 outcome 간의 관계를 이해하기 위한 방법으로 이를 통해 관계가 추정되면 결과를 예측할 수 있게 된다. 기계학습에서는 일반적으로 best fit을 그리는 작업을 의미하며 각 point 사이의 거리를 최소화하는 방법을 통해 best fit을 찾는다.Error (=loss): model의 오류 합계를 나타내는 값으로 model이 얼마나 잘 훈련되었는지를 판단한다. loss가 크면 model이 제대로 작동하지 않는다는 것을 의미한다.Classification: classification은 input의 주어진 data에 대하여 특정 class label을 예측하는 model을 의미한다.Accuracy: Accuracy는 model이 얼마나 잘 예측하고 있는지를 측정한다.2. &#51201;&#50857; &#44284;&#51228;&#182;Data documentation: Car Evaluation, BalloonsCar Evaluation의 feature: buying, maint, doors, persons, lung_boot, satefyCar Evalaution의 label: acceptBalloons의 feature: color, size, act, ageBalloons의 label: inflated2.1 pandas&#182;아래 data url을 통해 각 데이터마다 dataframe을 생성합니다.Car Evalaution의 모든 column name을 출력하시오.Car Evaluation의 buying feature에 어떤 카테고리가 있는지 출력하시오.Car Evaluation의 accept label의 각 class와 해당 class의 instance 개수를 메소드 하나로 출력하시오.Balloons에서 color feature이 yellow인 instance을 모두 출력하시오.In&nbsp;[&nbsp;]: # load datacar_url = &#39;https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data&#39;balloons_url = &#39;https://archive.ics.uci.edu/ml/machine-learning-databases/balloons/adult+stretch.data&#39; In&nbsp;[&nbsp;]: import numpy as npimport pandas as pdimport sklearn In&nbsp;[&nbsp;]: # 데이터 프레임 생성car_df = pd.read_csv(car_url, header = None)car_df.columns = [&quot;buying&quot;, &quot;maint&quot;, &quot;doors&quot;, &quot;persons&quot;, &quot;lung_boot&quot;, &quot;satefy&quot;, &quot;accept&quot;]bal_df = pd.read_csv(balloons_url, header = None)bal_df.columns = [&quot;color&quot;, &quot;size&quot;, &quot;act&quot;, &quot;age&quot;, &quot;inflated&quot;] In&nbsp;[&nbsp;]: # 데이터의 column name 출력print(car_df.columns.to_list()[:-1])print(bal_df.columns.to_list()[:-1]) [&#39;buying&#39;, &#39;maint&#39;, &#39;doors&#39;, &#39;persons&#39;, &#39;lung_boot&#39;, &#39;satefy&#39;][&#39;color&#39;, &#39;size&#39;, &#39;act&#39;, &#39;age&#39;]In&nbsp;[&nbsp;]: # Car evaluation &#39;buying&#39; feature 의 카테고리 출력print(car_df[&#39;buying&#39;].unique()) [&#39;vhigh&#39; &#39;high&#39; &#39;med&#39; &#39;low&#39;]In&nbsp;[&nbsp;]: # Car evaluation &#39;accept&#39; label 의 각 class 와 instance 개수car_df[&#39;accept&#39;].value_counts(sort = True) Out[&nbsp;]:unacc 1210acc 384good 69vgood 65Name: accept, dtype: int64In&nbsp;[&nbsp;]: # Balloons에서 &#39;color&#39; feature이 yellow인 instance 출력bal_df[&#39;color&#39;].value_counts(sort = True)print(bal_df.loc[(bal_df[&#39;color&#39;] == &#39;YELLOW&#39;)]) color size act age inflated0 YELLOW SMALL STRETCH ADULT T1 YELLOW SMALL STRETCH ADULT T2 YELLOW SMALL STRETCH CHILD F3 YELLOW SMALL DIP ADULT F4 YELLOW SMALL DIP CHILD F5 YELLOW LARGE STRETCH ADULT T6 YELLOW LARGE STRETCH ADULT T7 YELLOW LARGE STRETCH CHILD F8 YELLOW LARGE DIP ADULT F9 YELLOW LARGE DIP CHILD F2.2 &#45936;&#51060;&#53552; &#51060;&#54644; &#48143; &#51204;&#52376;&#47532;&#182;데이터 정보를 읽고 각 feature이 무슨 의미인지 파악하여 서술합니다. 실습했던 내용을 바탕으로 Car Evaluation 데이터와 Balloons 데이터를 scikit-learn의 Categorical Naive Bayesian Classifier에 적합하도록, Object 타입을 정수형으로 전처리합니다.2.3 &#47784;&#45944; &#49373;&#49457;, &#54984;&#47144; &#48143; &#44208;&#44284; &#54644;&#49437;&#182;scikit-learn 패키지를 사용하여 car와 balloons 두 데이터에:Categorical Naive Bayesian Classifier을 fit합니다.두 데이터에 대하여 score를 출력합니다.각 class probability와 각 feature probability을 출력합니다.본인이 임의로 만든 두개의 각기 다른 instances에 대하여 예측을 출력합니다. (car 두 개, balloons 두 개, 총 네 개)모델 예측 결과를 데이터의 맥락으로 해설합니다. (ex. 자동차1의 가격이 높고 보수비용이 낮으며... 할 때, 모델은 자동차1의 평가를 매우 좋음으로 예측하였다.)Car Evaluation의 feature: buying, maint, doors, persons, lung_boot, satefybuying :차량의 구매 가격 maint : 차량을 유지보수하기 위한 가격doors : 문의 개수persons : 차량이 운반할 수 있는 사람의 수 (탑승인원)lung_boot : 차량 트렁크의 크기safety : 안전 측정에서 평가된 차량 안전Car Evalaution의 label: accept** accept 차량의 용인 가능성(구매 가능성)Balloons의 feature: color, size, act, agecolor : 풍선의 색상size : 풍선의 크기act : 풍선을 늘어나게 했는지 줄어들게 했는지 여부age : 나이 / 어른 아이 여부 Balloons의 label: inflated** inflated 풍선이 부풀려진 여부In&nbsp;[&nbsp;]: # Object 타입을 정수형으로 전처리car_df_enc = pd.DataFrame()bal_df_enc = pd.DataFrame()# Car evaluationfor col in car_df.columns: car_df[col] = car_df[col].astype(&#39;category&#39;)car_df_enc[&#39;buying&#39;] = car_df[&#39;buying&#39;].cat.codescar_df_enc[&#39;maint&#39;] = car_df[&#39;maint&#39;].cat.codescar_df_enc[&#39;doors&#39;] = car_df[&#39;doors&#39;].cat.codescar_df_enc[&#39;persons&#39;] = car_df[&#39;persons&#39;].cat.codescar_df_enc[&#39;lung_boot&#39;] = car_df[&#39;lung_boot&#39;].cat.codescar_df_enc[&#39;satefy&#39;] = car_df[&#39;satefy&#39;].cat.codescar_df_enc[&#39;accept&#39;] = car_df[&#39;accept&#39;].cat.codes# Balloonsfor col in bal_df.columns: bal_df[col] = bal_df[col].astype(&#39;category&#39;)bal_df_enc[&#39;color&#39;] = bal_df[&#39;color&#39;].cat.codesbal_df_enc[&#39;size&#39;] = bal_df[&#39;size&#39;].cat.codesbal_df_enc[&#39;act&#39;] = bal_df[&#39;act&#39;].cat.codesbal_df_enc[&#39;age&#39;] = bal_df[&#39;age&#39;].cat.codesbal_df_enc[&#39;inflated&#39;] = bal_df[&#39;inflated&#39;].cat.codes In&nbsp;[&nbsp;]: from sklearn.naive_bayes import CategoricalNBcar_model = CategoricalNB()bal_model = CategoricalNB() In&nbsp;[&nbsp;]: # Car evaluation fit &amp; score 출력car_features = car_df_enc.drop(columns=[&#39;accept&#39;])car_label = car_df_enc[&#39;accept&#39;]car_model.fit(car_features.values, car_label)car_score = car_model.score(car_features.values, car_label)car_score Out[&nbsp;]:0.8715277777777778In&nbsp;[&nbsp;]: # Balloons fit &amp; score 출력bal_features = bal_df_enc.drop(columns=[&#39;inflated&#39;])bal_label = bal_df_enc[&#39;inflated&#39;]bal_model.fit(bal_features.values, bal_label)bal_score = bal_model.score(bal_features, bal_label)bal_score /usr/local/lib/python3.8/dist-packages/sklearn/base.py:443: UserWarning: X has feature names, but CategoricalNB was fitted without feature names warnings.warn( Out[&nbsp;]:1.0In&nbsp;[&nbsp;]: # class probability&amp; feature probability 출력# Car evaluationfrom pprint import pprintcar_feature_log_prior = car_model.feature_log_prob_for featue_prior in car_feature_log_prior: pprint(np.exp(featue_prior))print(np.exp(car_model.class_log_prior_)) array([[0.28092784, 0.23195876, 0.29896907, 0.18814433], [0.01369863, 0.64383562, 0.32876712, 0.01369863], [0.26771005, 0.21334432, 0.22158155, 0.29736409], [0.01449275, 0.57971014, 0.39130435, 0.01449275]])array([[0.27319588, 0.23969072, 0.29896907, 0.18814433], [0.01369863, 0.64383562, 0.32876712, 0.01369863], [0.25947282, 0.22158155, 0.22158155, 0.29736409], [0.20289855, 0.39130435, 0.39130435, 0.01449275]])array([[0.21134021, 0.25773196, 0.26546392, 0.26546392], [0.21917808, 0.26027397, 0.26027397, 0.26027397], [0.2693575 , 0.24794069, 0.24135091, 0.24135091], [0.15942029, 0.23188406, 0.30434783, 0.30434783]])array([[0.00258398, 0.51421189, 0.48320413], [0.01388889, 0.51388889, 0.47222222], [0.47568013, 0.25803792, 0.26628195], [0.01470588, 0.45588235, 0.52941176]])array([[0.374677 , 0.35142119, 0.27390181], [0.34722222, 0.34722222, 0.30555556], [0.30420445, 0.32399011, 0.37180544], [0.60294118, 0.38235294, 0.01470588]])array([[0.52971576, 0.00258398, 0.46770026], [0.43055556, 0.01388889, 0.55555556], [0.22918384, 0.47568013, 0.29513603], [0.97058824, 0.01470588, 0.01470588]])[0.22222222 0.03993056 0.70023148 0.03761574]In&nbsp;[&nbsp;]: # Balloonsbal_feature_log_prior = bal_model.feature_log_prob_for featue_prior in bal_feature_log_prior: pprint(np.exp(featue_prior))print(np.exp(bal_model.class_log_prior_)) In&nbsp;[&nbsp;]: # car evaluation instances 예측# (&quot;vhigh&quot;, &quot;vhigh&quot;, 2, 2, &quot;small&quot;, &quot;high&quot;) [3, 3, 0, 0, 2, 0]# (&quot;low&quot;, &quot;low&quot;, &quot;5more&quot;, &quot;more&quot;, &quot;big, &quot;med) [1, 1, 3, 2, 0, 0]print(car_model.predict_proba([[3, 3, 0, 0, 2, 0]]), car_model.predict([[3, 3, 0, 0, 2, 0]]))print(car_model.predict_proba([[1, 1, 3, 2, 0, 0]]), car_model.predict([[1, 1, 3, 2, 0, 0]])) [[9.21115137e-04 4.43484909e-06 9.99074059e-01 3.90721613e-07]] [2][[0.20014669 0.19352277 0.09437552 0.51195502]] [3]In&nbsp;[&nbsp;]: # balloons instances 예측#(&quot;YELLOW&quot;,&quot;LARGE&quot;, &quot;STRETCH&quot;, &quot;ADULT&quot;) [1, 0, 1, 0]#(&quot;PULPLE&quot;, &quot;SMALL&quot;, &quot;DIP&quot;, &quot;ADULT&quot;) [0, 1, 0, 0]print(bal_model.predict_proba([[1, 0, 1, 0]]), bal_model.predict([[1, 0, 1, 0]]))print(bal_model.predict_proba([[0, 1, 0, 0]]), bal_model.predict([[0, 1, 0, 0]])) [[0.19107307 0.80892693]] [1][[0.79281184 0.20718816]] [0]In&nbsp;[&nbsp;]: bal_df_enc.head(15) Out[&nbsp;]: color size act age inflated 0 1 1 1 0 1 1 1 1 1 0 1 2 1 1 1 1 0 3 1 1 0 0 0 4 1 1 0 1 0 5 1 0 1 0 1 6 1 0 1 0 1 7 1 0 1 1 0 8 1 0 0 0 0 9 1 0 0 1 0 10 0 1 1 0 1 11 0 1 1 0 1 12 0 1 1 1 0 13 0 1 0 0 0 14 0 1 0 1 0 &lt;svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\" width=\"24px\"&gt; &lt;/svg&gt; &#47784;&#45944; &#50696;&#52769; &#44208;&#44284; &#54644;&#49444;&#182;car evaluation&#182;자동차 3(index 2)는 가격이 매우 높고 보수 비용도 매우 높으며 차량의 문이 2개이고 탑승 가능 인원이 2명이다. 트렁크 크기는 작고, 안전평가에서 매우 높은 평가를 받았는데 모델은 자동차 3을 수용불가로 예측했다.자동차 1728(index 1727) 가격이 낮고 보수 비용도 매우 낮으며 차량의 문은 5개 이상이고 인원도 이상이며 트렁크 크기가 크고 안전평가에서는 중간 단계의 평가를 받았다. 모델은 자동차 1728을 매우 좋음으로 예측하였다.balloons&#182;풍선 6(index5)는 노란색이고 크기가 크며 풍선을 늘리는 행동을 했고 어른이 불었다. 모델은 풍선6이 부풀려졌을 것으로 예측했다풍선14(index13)은 보라색이고 크기가 작으며 풍선을 줄이는 행위를 했고 어른이 불었다 모델은 풍선14이 부풀려지지 않았을 것으로 예측했다.3. Naive Bayes Classifier &#44396;&#54788;&#182;Naive Bayes Classifier을 코드로 구현하는 것의 문제는 feature dimension이 매우 커질 경우에, 0과 1사이의 확률을 곱하기 때문에 전체 곱이 0와 매우 가까워지며 가끔은 long double으로도 표현하기 어려울정도로 매우 작은 확률이 계산될 수 있습니다.따라서 log probability을 사용하며, 이에 대한 이점은log probability range가 $[-∞, 0]$ 으로 넓어집니다.if $a &lt; b$, then $ log(a) &lt; log(b)$$log(a \\cdot b) = log(a) + log(b)$ 와 같은 규칙을 적용할 수 있습니다.$P(y|x_1, ..., x_n) = argmax_y \\left[ \\prod_{i=1}^{n} P(x_i|y) \\right] P(y)$$P(x_i|y)$: likelihood probability$P(y)$: class prior probability위 식에 로그를 씌우면$\\log(P(y|x_1, ..., x_n)) = argmax_y \\left[ \\sum_{i=1}^{n} \\log(P(x_i|y)) \\right] + log(P(y))$식이 합으로 바뀌어 다룰 수 있는 숫자로 계산됩니다.log likelihood probability 계산 함수 작성log class prior probability 계산 함수 작성log posterior probability을 사용하여 예측하는 Naive Bayes Classifier 함수 작성car data instance, balloon data instance에 대해 예측 출력In&nbsp;[&nbsp;]: import math In&nbsp;[&nbsp;]: import pandas as pd In&nbsp;[&nbsp;]: cdf = pd.read_csv(car_url, header = None)bdf = pd.read_csv(balloons_url, header = None)cdf.columns = [&quot;buying&quot;, &quot;maint&quot;, &quot;doors&quot;, &quot;persons&quot;, &quot;lung_boot&quot;, &quot;satefy&quot;, &quot;accept&quot;]bdf.columns = [&quot;color&quot;, &quot;size&quot;, &quot;act&quot;, &quot;age&quot;, &quot;inflated&quot;] In&nbsp;[&nbsp;]: # log likelihood probabilitydef calculate_likelihood(df): likelihood = dict() y = df[df.columns[-1]] sz = df.size/df.columns.size for feature in df.columns[:-1]: likelihood[feature] ={} for categ in y.unique(): class_count = y.value_counts()[categ] feature_count = df[df.columns[:-1]][feature][y[y == categ].index.values.tolist()].value_counts().to_dict() for feat_cat, feat_count in feature_count.items(): likelihood[feature][feat_cat + &quot;_&quot; + categ] = feat_count/class_count return likelihooddef calc_prior(df): prior={} for feat in df.columns.to_list()[:-1]: values = df[feat].value_counts().to_dict() prior[feat] = {} for value, count in values.items(): prior[feat][value] = count/df[df.columns[:-1]].size return prior In&nbsp;[&nbsp;]: # log class prior probability def calculate_class_prob(y): class_prior = {} for categ in y.unique(): class_prior[categ] = math.log(y.value_counts(normalize = True)[categ]) return class_priorcalculate_class_prob(cdf[cdf.columns[-1]])[&#39;unacc&#39;] Out[&nbsp;]:-0.3563443107732141In&nbsp;[&nbsp;]: def naive_bayes_classifier(df, inst): likelihood = calculate_likelihood(df) prior = calc_prior(df) prob_out = dict() for categ in df[df.columns[-1]].unique(): calculate_class_prob(df[df.columns[-1]]) likesum = 0 for feature, feature_value in zip (df.columns[:-1], inst): if feature_value + &#39;_&#39; + categ not in likelihood[feature]: continue else: likesum += math.log(likelihood[feature][feature_value + &#39;_&#39; + categ]) class_prior = calculate_class_prob(df[df.columns[-1]]) if categ in class_prior: prob_out[categ] = likesum + class_prior[categ] else: continue result = min(prob_out, key = lambda x :prob_out[x]) print(prob_out) In&nbsp;[&nbsp;]: naive_bayes_classifier(cdf, [&quot;vhigh&quot;, &quot;vhigh&quot;, &quot;2&quot;, &quot;2&quot;, &quot;small&quot;, &quot;high&quot;]) {&#39;unacc&#39;: -7.298119948403321, &#39;acc&#39;: -8.33742842300862, &#39;vgood&#39;: -5.152134856369955, &#39;good&#39;: -6.769162938070731}In&nbsp;[&nbsp;]: naive_bayes_classifier(bdf, [&quot;PULPLE&quot;, &quot;SMALL&quot;, &quot;DIP&quot;, &quot;CHILD&quot;]) {&#39;T&#39;: -1.6094379124341003, &#39;F&#39;: -2.014903020542265}참조https://medium.com/@rangavamsi5/na%C3%AFve-bayes-algorithm-implementation-from-scratch-in-python-7b2cc39268b9https://machinelearningmastery.com/naive-bayes-classifier-scratch-python/"
    } ,
  
    {
      "title"       : "Machine Learning - Logistic Regression",
      "category"    : "",
      "tags"        : "machine learning, study_model, logistic regression",
      "url"         : "./MachineLearning_Logistic.html",
      "date"        : "2021-03-24 00:00:00 +0900",
      "description" : "ML Logistic",
      "content"     : "&lt;!DOCTYPE html&gt;2015410052_김태원_3차시 &#54876;&#46041; 1: Simple Linear Regression&#182;가장 간단하고 직관적인 기계학습 모델은 데이터의 경향에 맞게 선을 그어주는 것입니다. 이때 데이터에 대해 가장 잘 맞는 선을 찾아가는 과정을 \"Linear Regression\"이라고 합니다.[Example: Sandra&#8217;s lemonade stand&#8217;s revenue over its first 12 months of being open]&#182;In&nbsp;[&nbsp;]: import matplotlib.pyplot as pltmonths = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]revenue = [52, 74, 79, 95, 115, 110, 129, 126, 147, 146, 156, 184]plt.plot(months, revenue, &quot;o&quot;)plt.title(&quot;Sandra&#39;s Lemonade&quot;)plt.xlabel(&quot;months&quot;)plt.ylabel(&quot;revenue&quot;)plt.show() [Points and Lines]&#182;line은 아래의 수식처럼 slope(=$m$)와 intercept(=$b$)에 의해 결정됩니다. $$y = mx + b$$Linear Regression에서의 목표는 우리가 가지고 있는 data에서 \"가장 좋은\"(=\"최적의\") $m$과 $b$를 찾는 것입니다.위의 데이터에 대해 최적의 $m$과 $b$를 미리 구해 놓았다고 가정하겠습니다. 이때 최적의 $m$과 $b$ 는 각각 10과 53입니다.In&nbsp;[&nbsp;]: m = 10b = 53 [Practice 1]&#182;위에서 주어진 최적의 $m$과 $b$를 이용하여 months에 대한 예측값 y를 생성하고 이를 실제 관측값인 revenue와 그래프를 그려 비교해봅시다.In&nbsp;[&nbsp;]: y = [m*x + b for x in months] In&nbsp;[&nbsp;]: plt.plot(months, revenue, &quot;o&quot;)plt.plot(months, y)plt.show() Loss&#182;최적의 모델 파라미터($m$과 $b$)를 찾기 위해서는 loss, 혹은 cost을 정의해야합니다. 이때 loss은 모델의 예측값이 실제값과 얼마나 차이가 있는지를 수치로 표현한 것입니다.아래 그림처럼 해당 실제값에서 예측값까지의 제곱 거리를 loss라고 정의합니다.결론적으로, 이러한 loss를 기준으로 최적의 모델 파라미터인지 아닌지를 판단합니다. 즉, 주어진 전체 데이터에 대해 loss를 최소로 하는 파라미터(m, b)를 찾는 것이 목표입니다. 이를 식으로 표현하면 아래와 같습니다.$$\\frac{1}{N}\\sum_{i=1}^{N}(y_i-(mx_i+b))^2$$[&#50696;&#49884;]&#182;3개의 점 (1, 5), (2, 1), (3, 3)가 주어지고, $y = x$와 $y = 0.5x +1$ 두가지 선이 주어졌습니다.In&nbsp;[&nbsp;]: #주어진 3개의 pointx = [1, 2, 3]y = [5, 1, 3] In&nbsp;[&nbsp;]: # y = x m1 = 1b1 = 0 In&nbsp;[&nbsp;]: # y = 0.5x + 1m2 = 0.5b2 = 1 주어진 점들에 대해 예측값을 계산해봅니다.In&nbsp;[&nbsp;]: y_pred_1 = [m1*x_val + b1 for x_val in x]y_pred_2 = [m2*x_val + b2 for x_val in x] [ Practice 2 ]&#182;이 두가지 선 중 위의 식을 토대로 loss를 계산하고, 그 중 loss가 더 작은 선을 골라봅시다.In&nbsp;[&nbsp;]: #result = 0#for x_val, y_val in zip(x, y):# value = m1*x_val +b1# value = y_val-value# value *= valuetotal_loss1 = 0total_loss2 = 0N = len(x)for i in range(N): total_loss1 += (y[i] - y_pred_1[i])**2/N total_loss2 += (y[i] - y_pred_2[i])**2/N print(&quot;y = x loss&quot;, total_loss1)print(&quot;y = 0.5x + 1 loss&quot;, total_loss2) y = x loss 5.666666666666666y = 0.5x + 1 loss 4.499999999999999Gradient Descent for Intercept&#182;Gradient Descent은 최적화 알고리즘 중 하나로서 loss function 혹은 cost function의 global 혹은 local minima을 찾을 때 사용됩니다.목표는 데이터의 관계를 잘 표현하는 파라미터 $m$와 $b$을 찾는 것이며, 이것은 gradient descent을 사용하여 loss function을 최소화함을 통해 얻을 수 있습니다.최소의 loss를 찾는 것은 마치 아래의 그림처럼 언덕을 내려가다가 바닥에 도착하면 멈추는 것과 비슷합니다. 즉, 파라미터를 loss가 작아지는 방향으로 조정하다가 최소가되면 멈추게됩니다. 이때 loss가 작아지는 방향은 현재의 경사(=gradient)의 반대 방향을 의미합니다. &lt;img src=https://miro.medium.com/max/640/1*lYpF8xJ3TiDoq461I0AcOQ.jpeg width=500px&gt;먼저 intercept(=$b$)에 대해서 gradient descent를 실행해봅니다. 앞에서 정의한 loss를 b에 대해 미분하여 gradient를 구할 수 있습니다.$$\\frac{2}{N}\\sum_{i=1}^{N}-(y_i-(mx_i+b))$$In&nbsp;[&nbsp;]: #intercept에 대하여 gradient descent를 수행하는 함수를 구현해봅니다.def get_gradient_at_b(x, y, b, m): N = len(x) diff = 0 for i in range(N): x_val = x[i] y_val = y[i] diff += y_val - ((m * x_val) + b) b_gradient = -(2/N) * diff return b_gradient Gradient Descent for Slope&#182;마찬가지로 slope(=$m$)에 대한 gradient descent를 실행해봅니다.이번에는 앞에서 정의한 loss를 $m$에 대해 미분하여 gradient를 구하면 됩니다.결과는 아래와 같습니다.$$\\frac{2}{N}\\sum_{i=1}^{N}-x_i(y_i-(mx_i+b))$$In&nbsp;[&nbsp;]: #intercept에 대하여 gradient descent를 수행하는 함수를 구현해봅니다.def get_gradient_at_m(x, y, b, m): N = len(x) diff = 0 for i in range(N): x_val = x[i] y_val = y[i] diff +=(x_val) *( y_val - ((m * x_val) + b)) m_gradient = -(2/N) * diff return m_gradient Weight Update&#182;$b$와 $m$의 gradient를 이용하여 loss가 감소하는 방향으로 $b$와 $m$을 update합니다. 그리고 loss가 최소가 되면 weight update가 멈추게 되는데, 그때의 $b$와 $m$이 최적의 파라미터가 됩니다.이때 내려가는 보폭을 조절할 수 있는데 이때 사용되는 것이 \"learning rate\"입니다. 즉, learning rate가 크면 큰 보폭으로 언덕을 내려가고 learning rate가 작으면 작은 보폭으로 언덕을 내려갑니다. 즉, \"learning rate\"를 gradient에 곱해주어 보폭을 사용자가 정할 수 있게합니다.그러나 learning rate는 신중하게 정할 필요가 있습니다.learning rate가 너무 작으면 loss의 최솟값에 수렴하는데 시간이 오래 걸립니다.learning rate가 너무 크면 최적의 parameter를 얻지 못할 수 있습니다.learning rate을 사용하여 gradient descent를 수행하여 최적의 파라미터를 찾는 함수를 구현해봅시다.In&nbsp;[&nbsp;]: #step_gradient 함수def step_gradient(b_current, m_current, x, y, learning_rate): b_gradient = get_gradient_at_b(x, y, b_current, m_current) m_gradient = get_gradient_at_m(x, y, b_current, m_current) b = b_current - (learning_rate * b_gradient) m = m_current - (learning_rate * m_gradient) return [b, m] Example: Sandra&#8217;s lemonade stand&#8217;s revenue over its first 12 months of being open&#182;이 함수들을 통해 앞에서 생성했던 data에 대해 한 step 후의 update된 parameter를 구해보겠습니다.In&nbsp;[&nbsp;]: months = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]revenue = [52, 74, 79, 95, 115, 110, 129, 126, 147, 146, 156, 184]b = 0m = 0learning_rate = 0.01[b, m] = step_gradient(b, m, months, revenue, learning_rate)print(&quot;b : &quot;, b)print(&quot;m : &quot;, m) b : 2.355m : 17.78333333333333최적의 parameter를 찾을 위 과정을 반복해봅니다.In&nbsp;[&nbsp;]: def gradient_descent(x, y, learning_rate, num_iter): b = 0. m = 0 for i in range(num_iter): [b, m] = step_gradient(b, m, x, y, learning_rate) return [b, m] 위에서 찾은 최적의 파라미터로 linear regression 모델을 만들어 시각화해봅니다.In&nbsp;[&nbsp;]: [optimal_b, optimal_m] = gradient_descent(months, revenue, 0.01, 1000)print(optimal_b, optimal_m) 49.60215351339813 10.463427732364998In&nbsp;[&nbsp;]: y = [optimal_m * x + optimal_b for x in months]plt.plot(months, revenue, &quot;o&quot;)plt.plot(months, y)plt.show() Scikit-Learn &#46972;&#51060;&#48652;&#47084;&#47532; &#49324;&#50857;&#182;지금까지 linear regression algorithm을 직접 구현했습니다. scikit-learn library를 이용하여 보다 간단하게 linear regression을 사용할 수 있습니다. 다큐멘테이션scikit-learn에 있는 linear_model 모듈을 통해 linear regression을 실습해보겠습니다.(단, 위에서 언급했던 learning_rate와 num_iterations은 scikit-learn의 기본값을 사용합니다.)아래의 temperature/sales 데이터를 scikit-learn을 이용하여 fitting 해보겠습니다.In&nbsp;[&nbsp;]: import matplotlib.pyplot as pltimport numpy as nptemperature = np.array(range(60, 100, 2))temperature = temperature.reshape(-1, 1)sales = [65, 58, 46, 45, 44, 42, 40, 40, 36, 38, 38, 28, 30, 22, 27, 25, 25, 20, 15, 5]plt.plot(temperature, sales, &#39;o&#39;) Out[&nbsp;]:[&lt;matplotlib.lines.Line2D at 0x7f12283b24d0&gt;] In&nbsp;[&nbsp;]: from sklearn.linear_model import LinearRegression In&nbsp;[&nbsp;]: lr = LinearRegression()lr.fit(temperature, sales)sales_predict = lr.predict(temperature)plt.plot(temperature, sales, &quot;o&quot;)plt.plot(temperature, sales_predict,)plt.show() &#44228;&#49688;(Coefficients)&#182;fitting된 모델에서 계수(coefficients)를 출력해봅니다.In&nbsp;[&nbsp;]: lr.coef_ Out[&nbsp;]:array([-1.15225564])In&nbsp;[&nbsp;]: lr.intercept_ Out[&nbsp;]:125.47819548872182&#47784;&#45944; &#54217;&#44032;&#182;R-Squared (Coefficient of Determination)은 0과 1사이의 값으로 linear regression 모델이 데이터에 얼마나 잘 학습되었는지 나타냅니다. R-Squared가 1에 가까울 수록 모델은 종속 변수(dependent variable)를 잘 예측할 수 있습니다.$$R^2 = 1 - \\frac{SS_{RES}}{SS_{TOT}} = 1 - \\frac{\\sum_{i}(y_i - \\hat{y}_i)^2}{\\sum_{i}(y_i - \\bar{y}_i)^2}$$In&nbsp;[&nbsp;]: #scikit-learn에서 제공하는 score함수 사용하여 r-square값 구해보기print(&quot;R-squared:&quot;)print(lr.score(temperature, sales))#lr model sales -&gt; temp 에 대해 아래 확률 만큼 설명 가능하다 R-squared:0.9114088011031334&#54876;&#46041; 2: Logistic Regression&#182;Logistic Regression은 데이터가 어떠한 특정 카테고리에 속할지를 0과 1사이의 연속적인 확률로 예측하는 회귀 알고리즘 중 하나입니다. 그런 다음, 확률에 기반하여 특정 데이터가 어떤 카테고리에 속할지를 결정하게 되고, 궁극적으로 classification문제를 풀게 됩니다.[Linear Regression Approach]&#182;대학교 강의에서 학생들이 기말 시험을 통과할 수 있을지를 예측해보려고 합니다. 각 학생들이 시험을 통과할 확률을 예측함으로써 통과 여부를 예측할 수 있습니다. 여기서 Linear Regression을 활용하면 어떨까요? 한번 해봅시다.우선 기말 시험 데이터를 확인 해보겠습니다.In&nbsp;[&nbsp;]: import numpy as npimport matplotlib.pyplot as pltpassed_exam = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1])passed_exam = passed_exam.reshape(-1, 1) hours_studied = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19])hours_studied = hours_studied.reshape(-1, 1) #시험에 패스/페일 vs 공부한 시간에 대한 산점도 그려보기plt.scatter(hours_studied, passed_exam)plt.show() 각 학생들이 공부한 시간을 num_hours_studied이라 하고 해당 학생이 중간 시험을 통과한 여부를 y (y 는 통과한 경우 1, 그렇지 않은 경우 0) 라고 한다면 linear regression을 통해 다음과 같이 직선을 그릴 수 있습니다.In&nbsp;[&nbsp;]: model = LinearRegression()model.fit(hours_studied, passed_exam) Out[&nbsp;]:LinearRegression()In&nbsp;[&nbsp;]: import numpy as npsample_x = np.linspace(0, 20,100).reshape(-1,1)probability = model.predict(sample_x).ravel() In&nbsp;[&nbsp;]: plt.plot(hours_studied, passed_exam, &quot;o&quot;)plt.plot(sample_x, probability)plt.show() Logistic regression의 예측값은 0과 1사이이므로 위와 같은 linear regression의 한계를 극복할 수 있습니다.Logistic regression은 다음과 같은 과정으로 수행됩니다.모든 coefficients와 intercept(bias)를 0으로 초기화합니다.각각의 feature를 이에 상응하는 coefficient와 곱한 값과 intercept(bias)를 모두 더해 log-odds를 계산합니다.계산한 log-odds 값을 sigmoid 함수에 전달하여 0 과 1 사이의 확률값을 구합니다.계산한 확률값과 실제 label을 비교하여 loss를 계산하고, gradient descent로 최적의 파라미터를 찾습니다.최적의 파라미터를 찾았다면 classification threshold 값을 조절하여 positive class와 negative class를 어떻게 나눌지를 설정합니다.[ Log-Odds ]&#182;Linear regression에서는 각 feature에 상응하는 weight의 곱과 intercept(bias)를 더해 예측을 하였습니다. Logistic regression에서도 마찬가지지만 log-odds를 계산합니다.log-odds는 특정 데이터가 positive class에 속할 확률을 표현합니다. 통계에서 특정 사건의 odds(승산)을 계산하는 공식은 다음과 같습니다.$P(A)$: 특정 사건이 발생할 확률$1-P(A)$: 특정 사건이 발생하지 않는 확률$$Odds = \\frac{P(A)}{1 - P(A)}$$Odds는 특정 사건이 일어나는 횟수가 특정 사건이 일어나지 않는 횟수보다 얼마나 더 많은지를 의미합니다. 만약 특정 학생이 시험에서 pass할 확률이 0.7이라면, pass하지 못 할 확률은 1 - 0.7 = 0.3 이고, 이 경우 odds를 다음과 같이 계산할 수 있습니다.$$\\text{Odds of passing} = \\frac{0.7}{0.3} = 2.33$$Odds는 0과 양의 무한대의 값을 범위로 갖습니다. 그렇기 때문에 제약이 있고, 또, 확률값과 odds 값은 비대칭성을 띕니다.이러한 한계를 극복하기 위하여 odd에 로그를 취하는것을 log-odds라 하고, 음의 무한대부터 양의 무한대까지의 범위를 갖습니다.$$\\text{Log odds of passing} = log(2.33) = 0.847$$Logistic regression 모델에서 아래와 같이 z 값으로 나타내지는 log-odds 값을 계산할 수 있습니다.$$log(\\frac{P(A)}{1-P(A)})=z=b_0+b_1x_1+b_2x_2+\\cdots+b_nx_n$$이로써 특정 데이터의 feature values를 해당 데이터가 positive class에 속할 가능성으로 매핑할 수 있습니다. 이 때 이러한 곱의 합을 dot product(내적) 이라고 합니다. 내적은 numpy의 np.dot() 메서드를 활용하여 쉽게 계산할 수 있습니다.기말 시험 데이터에서 최적의 coefficient와 intercept가 각각 $0.03$, $-0.3$이라고 가정했을 때의 log-odds를 계산 해봅시다.In&nbsp;[&nbsp;]: calculated_coefficients = 0.03intercept = -0.3 In&nbsp;[&nbsp;]: # log_odds 함수를 정의해봅니다.def log_odds(features, coefficient, intercept): return np.dot(features, coefficient) + intercept In&nbsp;[&nbsp;]: # hours_studied 데이터에 대해서 log-odds를 계산해봅니다.calculated_log_odds = log_odds(hours_studied, calculated_coefficients, intercept)calculated_log_odds Out[&nbsp;]:array([[-0.3 ], [-0.27], [-0.24], [-0.21], [-0.18], [-0.15], [-0.12], [-0.09], [-0.06], [-0.03], [ 0. ], [ 0.03], [ 0.06], [ 0.09], [ 0.12], [ 0.15], [ 0.18], [ 0.21], [ 0.24], [ 0.27]])[ Sigmoid Function ]&#182;Sigmoid Function은 log-odds인 $z$ 값을 취해서 아래와 같이 0 과 1 사이의 값을 반환합니다.$$h_θ = σ(z) = \\frac{1}{1+e^{-z}}$$&lt;img src=https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/1200px-Logistic-curve.svg.png width=500px&gt;Logistic Regression이 특정 데이터가 positive class에 속할 확률을 계산합니다.In&nbsp;[&nbsp;]: # sigmoid 함수 정의하기def sigmoid(z): return 1/(1+np.exp(-z)) In&nbsp;[&nbsp;]: # 확률 계산해보기probabilities = sigmoid(calculated_log_odds)probabilities Out[&nbsp;]:array([[0.42555748], [0.4329071 ], [0.44028635], [0.44769209], [0.45512111], [0.46257015], [0.47003595], [0.47751518], [0.4850045 ], [0.49250056], [0.5 ], [0.50749944], [0.5149955 ], [0.52248482], [0.52996405], [0.53742985], [0.54487889], [0.55230791], [0.55971365], [0.5670929 ]])[ Log-Loss ]&#182;이제 최적의 coefficients와 intercept를 구해보겠습니다. 이를 구하기 위해서는 주어진 모델의 예측이 실제 데이터에 얼마나 가까운지 측정하는 기준이 필요합니다. 이를 loss function 혹은 cost function이라고 합니다.모델이 데이터에 ‘fit’ 하단걸 측정하기 위해선 먼저 각 데이터에 대한 loss를 계산한뒤 loss의 평균을 내야합니다. Logistic regression에서의 loss function은 Log Loss라고 불리며, 공식은 다음과 같습니다.$$J(b) = -\\frac{1}{m}\\sum_{i=1}^{m}[y^{(i)}log(a^{(i)}) + (1-y^{(i)})log(1-a^{(i)})]$$m 은 전체 데이터의 개수입니다.$y^{(i)}$는 $i$ 번째 데이터의 class 입니다.$a^{(i)}$는 $i$ 번째 데이터의 log-odds 값에 sigmoid 를 취한 값입니다. 즉 $i$ 번째 데이터가 positive class에 속할 확률을 나타냅니다.만약 $i$ 번째 데이터의 class가 $y=1$ 이라면 해당 데이터에 대한 loss는 다음과 같습니다.$$loss_{i(y=1)} = -log(a^{(i)})$$loss를 최소화 시키려면 $a^{(i)}$ 값이 커야 합니다. 즉, 예측된 확률 값이 원래 class인 1 에 가까울수록 loss는 줄어들게 됩니다.반대로 $i$ 번째 데이터의 class가 $y=0$ 인 경우는 다음과 같습니다..$$loss_{i(y=0)} = -log(1-a^{(i)})$$loss를 최소화 시키려면 $a^{(i)}$값이 작아야 합니다. 즉, 예측된 확률 값이 원래 class이 0에 가까울수록 loss는 줄어들게 됩니다.아래의 그래프는 class가 $y=1$, $y=0$ 일 때 $a$값에 따라 loss가 어떻게 변화하는지를 나타냅니다.그래프를 보면 올바르게 예측할수록 loss가 줄어드는 것을 볼 수 있습니다. 반대로 잘 못 예측하게 되면 loss가 크게 증가하는데, 이는 모델이 잘못 예측할 때 패널티를 강하게 줌으로써 올바른 예측을 할 수 있도록 유도할 수 있습니다.In&nbsp;[&nbsp;]: # log_loss 함수 구현해보기def log_loss(probabilities, actual_class): return np.sum(-(1 / actual_class.shape[0]) * (actual_class * np.log(probabilities) + (1 - actual_class) * np.log(1 - probabilities))) In&nbsp;[&nbsp;]: log_loss(probabilities, passed_exam) Out[&nbsp;]:0.6279073897953891[ Classification Thresholding ]&#182;Logistic Regression은 예측된 확률 값이 임계값을 넘느냐 못 넘느냐에 따라서 class를 분류합니다. 이 임계값을 classification threshold 라고 합니다.Classification threshold의 기본값은 0.5 입니다. 만약 특정 데이터의 예측된 확률 값이 0.5 보다 크거나 같다면 해당 데이터는 positive class로 분류됩니다. 반대로 예측된 확률 값이 0.5 보다 낮다면 negative class로 분류됩니다.만약 더욱 엄격하게하고자 한다면 threshold를 0.6이나 0.7로 조정할 수 있습니다. 즉, 모델이 positive class를 더 적게 예측할 수 있도록 하는 것입니다.이에 대해 예측된 확률값이 임계값을 넘으면 1, 그렇지 않으면 0 을 반환하는 함수를 구현해보겠습니다.In&nbsp;[&nbsp;]: # predict_class함수 구현하기def predict_class(features, coefficients, intercept, threshold): odd = log_odds(features, coefficients, intercept) predicted_probability = sigmoid(odd) result = [] for i in predicted_probability: if i &gt;= threshold: result.append([1]) else: result.append([0]) return result In&nbsp;[&nbsp;]: # threshold=0.5로 최종 예측 해보기predict_class(hours_studied, calculated_coefficients, intercept, 0.5) Out[&nbsp;]:[[0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1]]이번에는 더욱 엄격하게 threshold를 0.55로 설정하여 위의 결과와 비교해봅니다.In&nbsp;[&nbsp;]: #threshold=0.55로 최종 예측 해보기predict_class(hours_studied, calculated_coefficients, intercept, 0.55) Out[&nbsp;]:[[0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [1], [1], [1]][ Scikit-Learn ]&#182;scikit-learn에서 제공하는 메서드를 활용하여 Logistic Regression을 구현해봅니다.In&nbsp;[&nbsp;]: from sklearn.linear_model import LogisticRegressionmodel = LogisticRegression()model.fit(hours_studied, passed_exam) /usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). y = column_or_1d(y, warn=True) Out[&nbsp;]:LogisticRegression()In&nbsp;[&nbsp;]: probability = model.predict_proba(sample_x)[:, 1]plt.plot(hours_studied, passed_exam, &#39;o&#39;)plt.plot(sample_x, probability)plt.xlabel(&#39;hours studied&#39;)plt.show() In&nbsp;[&nbsp;]: predicted_class Out[&nbsp;]:array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])&#44284;&#51228;&#182;이전 실습2 과제의 코드를 이용하여 titanic.csv 데이터의 url를 통해 pandas DataFrame으로 데이터를 가져옵니다.이전 실습2 과제처럼 동일하게1) 전체 데이터에서 Survived, Pclass, Sex, Age, SibSp, Parch, Fare, Embarked feature을 고르고,2) Age feature의 NA값을 drop하고,3) Sex feature을 scikit-learn의 LabelEncoder을 사용하여 정수형으로 변환합니다..head()를 사용하여 데이터의 첫 다섯개 instance를 출력합니다.주어진 onehot() 함수를 읽어보고, 어떤 순서와 방식으로 데이터를 변환하는지 서술합니다. 필요하다면 각 메소드의 디큐멘테이션을 참고합니다.onehot() 함수를 사용하여 Embarked feature을 one-hot encoding 합니다.&lt;img src=https://i.imgur.com/mtimFxh.png width=400px&gt;.head()를 사용하여 one-hot encoding이 올바르게 되었는지 확인합니다.아래 주어진 코드를 이용하여 데이터를 feature 변수 x와 label 변수 y로 분리하고 train_test_split()함수를 이용하여 데이터를 train data과 test data으로 나눕니다. 만약 deterministic한 결과를 원한다면 random_state 파라미터를 지정해줍니다. (지정하지 않아도 과제 점수에는 상관이 없습니다.)train_test_split() 다큐멘테이션# 데이터를 feature X와 label y로 나눕니다.y = df[[&#39;Survived&#39;]].to_numpy().ravel()x = df.drop(columns=[&#39;Survived&#39;])LogisticRegression() 모델을 생성합니다. 이때, 모델 파라미터는 max_iter을 1000으로 지정해줍니다. 이 모델을 train data에 fit 해봅니다.test data에 대하여 score을 계산합니다.훈련된 Logistic regression 모델의 .coef_를 출력해보고, .coef_의 절대값이 큰 feature 2개와 절대값이 작은 feature 2개가 무엇인지, 그리고 각각에 대해 그 값이 몇인지 서술합니다. //의미 서술 XIn&nbsp;[&nbsp;]: import pandas as pddef onehot(data, feature): &#39;&#39;&#39; data의 feature column을 one hot으로 변환해줍니다. data: pandas DataFrame feature: string, 데이터 프레임의 column 이름 &#39;&#39;&#39; return pd.concat([data, pd.get_dummies(data[feature], prefix=feature)], axis=1).drop([feature], axis=1)data_url = &quot;https://raw.githubusercontent.com/inikoreaackr/ml_datasets/main/titanic.csv&quot; In&nbsp;[&nbsp;]: data = pd.read_csv(data_url) In&nbsp;[&nbsp;]: data.columnsdata = data [[&#39;Survived&#39;, &#39;Pclass&#39;, &#39;Sex&#39;, &#39;Age&#39;, &#39;SibSp&#39;, &#39;Parch&#39;, &#39;Fare&#39;, &#39;Embarked&#39;]] In&nbsp;[&nbsp;]: data = data.dropna() In&nbsp;[&nbsp;]: from sklearn.preprocessing import LabelEncoderle = LabelEncoder()data.Sex = le.fit_transform(data.Sex) In&nbsp;[&nbsp;]: data.head() Out[&nbsp;]: Survived Pclass Sex Age SibSp Parch Fare Embarked 0 0 3 1 22.0 1 0 7.2500 S 1 1 1 0 38.0 1 0 71.2833 C 2 1 3 0 26.0 0 0 7.9250 S 3 1 1 0 35.0 1 0 53.1000 S 4 0 3 1 35.0 0 0 8.0500 S &lt;svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\" width=\"24px\"&gt; &lt;/svg&gt; In&nbsp;[&nbsp;]: onehot? In&nbsp;[&nbsp;]: data = onehot(data, &#39;Embarked&#39;) 주어진 onehot() 함수를 읽어보고, 어떤 순서와 방식으로 데이터를 변환하는지 서술합니다. 필요하다면 각 메소드의 디큐멘테이션을 참고합니다.&lt;img src=https://i.imgur.com/mtimFxh.png width=400px&gt;data의 feature column을 0과 1의 값을 가지는 데이터로 구별해주는 인코딩이다.표현하고자 하는 인덱스에는 데이터 1을, 다른 인덱스는 0으로 표현된다.먼저 데이터에 대한 정수 인코딩이 진행된다. 각각의 요소는 다른 정수값을 가지는 데이터로 표현된다.EX 1 : RED 2 : YELLOW 3 : GREEN값의 개수만큼의 행렬 구조가 만들어지고 표현하고자 하는 데이터의 고유한 값을 index로 보아 해당하는 위치에는 1을 다른 값을 지니는 요소의 위치에 0을 부여한다.In&nbsp;[&nbsp;]: y = data[[&#39;Survived&#39;]].to_numpy().ravel()x = data.drop(columns=[&#39;Survived&#39;]) In&nbsp;[&nbsp;]: from sklearn.model_selection import train_test_splitx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = .3) In&nbsp;[&nbsp;]: model_titanic = LogisticRegression(max_iter=1000)model_titanic.fit(x_train, y_train) Out[&nbsp;]:LogisticRegression(max_iter=1000)In&nbsp;[&nbsp;]: print(model_titanic.score(x_test,y_test)) 0.7990654205607477In&nbsp;[&nbsp;]: coef = model_titanic.coef_print(coef, &quot;\\n&quot;)for i in range(len(x.columns)): print(x.columns[i],&quot;:&quot;,coef[0][i]) [[-1.13588865e+00 -2.58221815e+00 -4.56647796e-02 -3.35399948e-01 -4.22408897e-02 2.20349750e-03 4.47231111e-01 -5.75288314e-01 1.18618995e-01]] Pclass : -1.1358886533804295Sex : -2.5822181505918125Age : -0.045664779628737426SibSp : -0.33539994797221Parch : -0.04224088972835762Fare : 0.0022034975000360204Embarked_C : 0.4472311112889102Embarked_Q : -0.5752883138433935Embarked_S : 0.11861899501285339절대값이 큰 featureSex : 2.40791938831739Pclass : 1.174206879583307절대값이 작은 featureFare : 0.00041304172578507105Age : 0.05062299635153086&#44284;&#51228; 2&#182;실습에서는 logistic regression 함수의 cost fuction까지 구현을 해보았습니다. 두번째 과제는 logistic regression cost function의 gradient descent을 함수로 구현하여 최적의 파라미터까지 찾아 최종 모델을 훈련시키는 logistic regression 모델을 scratch부터 구현하는 것 입니다. 데이터는 logistic regression 실습에서 사용한 hours_studied, passed_exam 데이터를 사용합니다.$$z = x \\cdot w + b$$$$h_θ = \\frac{1}{(1+e^{-z})}$$$$cost_{(\\theta, y)} = -y \\cdot log(h_\\theta)-(1-y)\\cdot log(1-h_{\\theta})$$세부 사항:coefficient의 gradient을 계산하는 함수를 작성합니다. ($\\frac{\\delta cost}{\\delta w}$)intercept의 gradient을 계산하는 함수를 작성합니다. ($\\frac{\\delta cost}{\\delta b}$)두 gradient을 받아 step gradient을 실행하는 함수를 작성합니다.위 과정을 반복하여 최적의 파라미터를 찾는 logistic regression 함수를 작성합니다.데이터에 최종 모델을 학습시키고, classification을 잘 수행하는지 결과를 시각화해 확인해봅니다.안내 사항:learning rate, number of iteration은 임의로 적당한 값을 본인이 설정해봅니다.과제 2번은 타 수강생 한 명과 협업이 가능합니다. 협업한 수강생의 이름 및 학번을 개제해주시기 바랍니다.참고한 자료의 출처를 필히 기재해주시기 바랍니다.In&nbsp;[&nbsp;]: # coefficient gradient : SUM(x(a(i) - y))/mdef get_coef_gradient(x, y, y_predict): [[grad]] = (1/len(x)) * np.dot(x.T, y_predict - y) return grad In&nbsp;[&nbsp;]: get_coef_gradient(hours_studied, passed_exam, probabilities) Out[&nbsp;]:-1.5871075590969737In&nbsp;[&nbsp;]: # intercept gradient : SUM(a(i) - y)/mdef get_intercept_gradient(y, y_predict): return (1/len(y)) * np.sum(y_predict - y) In&nbsp;[&nbsp;]: get_intercept_gradient(passed_exam, probabilities) Out[&nbsp;]:0.046277874159417066In&nbsp;[&nbsp;]: def log_loss_step_gradient(weight_current, intercept_current, x, y, y_predicted, learning_rate): coef = get_coef_gradient (x, y, y_predicted) intercept = get_intercept_gradient(y, y_predicted) weight = weight_current - (learning_rate * coef) intercept = intercept_current - (learning_rate * intercept) return [weight, intercept] In&nbsp;[&nbsp;]: def log_loss_gradient_descent(x, y, learning_rate, num_iter): opt_weight = 0 opt_intercept = 0 odd = log_odds(x, weight, intercept) y_predict = sigmoid(odd) print(y_predict) trace = [] trace.append(log_loss(y_predict, y)) for j in range(num_iter): [opt_weight, opt_intercept] = log_loss_step_gradient(opt_weight, opt_intercept, x, y, y_predict, learning_rate) odd = log_odds(x, opt_weight, opt_intercept) y_predict = sigmoid(odd) trace.append(log_loss(y_predict, y)) return [opt_weight, opt_intercept], trace In&nbsp;[&nbsp;]: [weight, intercept], trace = log_loss_gradient_descent(hours_studied, passed_exam, 0.05, 1000)print(&quot;weight : &quot;, weight, &quot;\\n&quot;)print(&quot;intercept : &quot;, intercept, &quot;\\n&quot;) [[0.42555748] [0.51673783] [0.60681714] [0.69017318] [0.76276619] [0.82271945] [0.87010168] [0.90626282] [0.93313104] [0.95269988] [0.96674609] [0.97672306] [0.98375699] [0.98868999] [0.99213681] [0.99453899] [0.99621011] [0.99737121] [0.99817723] [0.99873643]]weight : 0.36710895563627843 intercept : -3.6293894250817496 In&nbsp;[&nbsp;]: def norm_x(x): return x-x.mean()y_prediction = sigmoid(log_odds(hours_studied, weight, intercept))plt.plot(hours_studied, passed_exam, &quot;o&quot;)plt.plot(hours_studied, y_prediction) Out[&nbsp;]:[&lt;matplotlib.lines.Line2D at 0x7f122808e390&gt;] In&nbsp;[&nbsp;]: # model의 log_loss cost변화plt.title(&#39;Log-loss over iterations&#39;)plt.plot(trace)plt.xlabel(&#39;iteration&#39;)plt.ylabel(&#39;Log-loss&#39;)plt.show() 참고한 사이트https://www.kaggle.com/code/paulrohan2020/logistic-regression-implementation-from-scratchhttps://towardsdatascience.com/logistic-regression-from-scratch-in-python-ec66603592e2https://www.kdnuggets.com/2022/04/logistic-regression-classification.html"
    } ,
  
    {
      "title"       : "Machine Learning - Decision Tree",
      "category"    : "",
      "tags"        : "machine learning, study_model, Decision Tree",
      "url"         : "./MachineLearning_DT.html",
      "date"        : "2021-03-24 00:00:00 +0900",
      "description" : "ML Decision Tree",
      "content"     : "Decision_Tree 결정 트리(Decision Tree)In&nbsp;[&nbsp;]: !pip install mglearn!pip install --upgrade joblib==1.1.0 Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/Requirement already satisfied: mglearn in /usr/local/lib/python3.7/dist-packages (0.1.9)Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from mglearn) (1.0.2)Requirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (from mglearn) (2.9.0)Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from mglearn) (1.3.5)Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from mglearn) (7.1.2)Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mglearn) (1.21.6)Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from mglearn) (1.1.0)Requirement already satisfied: cycler in /usr/local/lib/python3.7/dist-packages (from mglearn) (0.11.0)Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from mglearn) (3.2.2)Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;mglearn) (3.0.9)Requirement already satisfied: python-dateutil&gt;=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;mglearn) (2.8.2)Requirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;mglearn) (1.4.4)Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver&gt;=1.0.1-&gt;matplotlib-&gt;mglearn) (4.1.1)Requirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil&gt;=2.1-&gt;matplotlib-&gt;mglearn) (1.15.0)Requirement already satisfied: pytz&gt;=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas-&gt;mglearn) (2022.4)Requirement already satisfied: threadpoolctl&gt;=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn-&gt;mglearn) (3.1.0)Requirement already satisfied: scipy&gt;=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn-&gt;mglearn) (1.7.3)Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/Requirement already satisfied: joblib==1.1.0 in /usr/local/lib/python3.7/dist-packages (1.1.0)&#51648;&#45768; &#48520;&#49692;&#46020; (Gini Impurity)&#182;지니 불순도는 결정 트리의 분할기준 중 하나입니다.&lt;img src=https://s3.amazonaws.com/codecademy-content/programs/data-science-path/decision-trees/comparison_1.svg width=300px&gt;&lt;img src=https://s3.amazonaws.com/codecademy-content/programs/data-science-path/decision-trees/comparison_2.svg width=300px&gt;지니 불순도를 찾기 위해서는 1에서 시작해서 세트의 각 class 비율의 제곱을 빼면 됩니다.$$\\text{Gini Impurity} = 1 - \\text{Gini Index} \\\\ = 1 - \\sum_{i=1}^{K}p_{i}^{2}$$위 식에서 $K$은 class label의 개수이며, $p_i$은 $i$번째 class label의 비율입니다.예를 들어, A class인 instance가 3개 있고 B class인 instance가 1개 있는 데이터의 경우에는 지니 불순도는 아래와 같이 계산됩니다.$$1 - (3/4)^2 - (1/4)^2 = 0.375$$만약 데이터가 하나의 class만 있다면, 지니 불순도는 0이 됩니다. 불순도가 낮으면 낮을수록 결정 트리의 성능은 더 좋아집니다.&#49892;&#49845; 1&#182;위 정리에서 주어진 Tree의 불순도 계산In&nbsp;[&nbsp;]: 1 - (4/6)**2 - (2/6)**2 Out[&nbsp;]:0.4444444444444445sample_labels 리스트의 지니 불순도 계산In&nbsp;[&nbsp;]: sample_labels = [&quot;unacc&quot;, &quot;unacc&quot;, &quot;acc&quot;, &quot;acc&quot;, &quot;good&quot;, &quot;good&quot;]impurity = 1 sample labels에 포함되어있는 class의 개수In&nbsp;[&nbsp;]: from collections import Counterlabel_counts = Counter(sample_labels)print(label_counts) Counter({&#39;unacc&#39;: 2, &#39;acc&#39;: 2, &#39;good&#39;: 2})데이터셋에서 각 label의 확률 계산In&nbsp;[&nbsp;]: for label in label_counts: print(label) prob = label_counts[label]/len(sample_labels) print(prob) unacc0.3333333333333333acc0.3333333333333333good0.3333333333333333확률을 이용하여 sample_labels의 불순도 계산In&nbsp;[&nbsp;]: for label in label_counts: prob = label_counts[label]/len(sample_labels) impurity -= prob ** 2print(impurity) 0.6666666666666665지니 불순도를 계산하는 코드 함수 제작In&nbsp;[&nbsp;]: def gini(dataset): impurity = 1 label_counts =Counter(dataset) for label in label_counts: prob_of_label = label_counts[label] / len(dataset) impurity -= prob_of_label ** 2 return impurity &#51221;&#48372;&#51613;&#44032;&#47049; (Information Gain)&#182;이제 지니 불순도가 낮은 끝마디(leaf node)를 만들기 위해서 어떠한 feature에 따라 데이터를 나누어야하는지 결정해야 합니다.예를 들어, 학생들의 수면 시간 또는 학생들의 공부 시간 둘 중 어느 feature을 기준으로 학생들을 나누어야 더 좋은 tree를 만들 수 있을까요?위 질문에 답하기 위해 어떠한 feature에 대하여 데이터를 나누었을 때의 정보증가량을 계산해야 합니다.정보증가량은 데이터 분할 전과 후의 불순도 차이를 측정합니다.예를 들어, 불순도가 0.5인 데이터를 어떠한 feature에 대해 나누었을 때, 불순도가 각각 0, 0.375, 0 인 끝마디가 생긴다고 가정해봅니다.&lt;img src=https://s3.amazonaws.com/codecademy-content/programs/data-science-path/decision-trees/info.svg width=300px&gt;이 경우에 데이터를 나누는 정보증가량은 0.5 - 0 - 0.375 - 0 = 0.125 입니다.데이터를 나누었을때의 정보 증가량은 양수입니다. 따라서, 위처럼 결정 지점을 나눈 것은 결과적으로 불순도를 낮추었기 때문에 좋은 결정 지점입니다.정보증가량은 크면 클수록 좋습니다.&#49892;&#49845; 2&#182;unsplit_labels라는 임의의 데이터를 두가지 다른 분할 지점으로 나누었습니다. 이는 split_labels_1와 split_labels_2 입니다.각 분할에 대해 information gain 계산In&nbsp;[&nbsp;]: unsplit_labels = [&quot;unacc&quot;, &quot;unacc&quot;, &quot;unacc&quot;, &quot;unacc&quot;, &quot;unacc&quot;, &quot;unacc&quot;, &quot;good&quot;, &quot;good&quot;, &quot;good&quot;, &quot;good&quot;, &quot;vgood&quot;, &quot;vgood&quot;, &quot;vgood&quot;]split_labels_1 = [[&quot;unacc&quot;, &quot;unacc&quot;, &quot;unacc&quot;, &quot;unacc&quot;, &quot;unacc&quot;, &quot;unacc&quot;, &quot;good&quot;, &quot;good&quot;, &quot;vgood&quot;], [ &quot;good&quot;, &quot;good&quot;], [&quot;vgood&quot;, &quot;vgood&quot;]]split_labels_2 = [[&quot;unacc&quot;, &quot;unacc&quot;, &quot;unacc&quot;, &quot;unacc&quot;,&quot;unacc&quot;, &quot;unacc&quot;, &quot;good&quot;, &quot;good&quot;, &quot;good&quot;, &quot;good&quot;], [&quot;vgood&quot;, &quot;vgood&quot;, &quot;vgood&quot;]] In&nbsp;[&nbsp;]: # unsplit_labels의 지니 불순도를 계산해봅니다.info_gain_1 = gini(unsplit_labels)info_gain_1 Out[&nbsp;]:0.6390532544378698split_labels_1의 각 부분집합에 대하여 지니 불순도을 계산하여 정보 증가량 계산In&nbsp;[&nbsp;]: for subset in split_labels_1: info_gain_1 -= gini(subset)print(info_gain_1) 0.14522609394404257split_labels_2에 대해 정보증가량을 계산In&nbsp;[&nbsp;]: info_gain_2 = gini(unsplit_labels)for subset in split_labels_2: info_gain_2 -= gini(subset)print(info_gain_2) 0.15905325443786977정보증가량을 계산하는 함수In&nbsp;[&nbsp;]: def information_gain(starting_labels, split_labels): info_gain = gini(starting_labels) for subset in split_labels: info_gain -= gini(subset) return info_gain &#44032;&#51473; &#51221;&#48372;&#51613;&#44032;&#47049; (Weighted Information Gain)&#182;만약 정보증가량이 0이라면 그 feature에 대해 데이터를 나누는 것은 소용이 없습니다. 때에 따라서 데이터를 나누었을 때 정보증가량이 음수가 될 수 있습니다. 이 문제를 해결하기 위해서 가중 정보증가량 (weighted information gain)을 사용합니다.분할 후에 생성되는 데이터의 부분집합의 크기 또한 중요합니다. 예를 들어서, 아래 이미지에서는 불순도가 같은 두 부분집합이 있습니다.&lt;img src=https://s3.amazonaws.com/codecademy-content/programs/data-science-path/decision-trees/impurity-0.svg width=300px&gt;어느 부분집합을 결정 트리의 끝마디로 정하는게 좋은 결정트리를 만들 수 있을까요?두 부분집합은 모두 불순도가 0으로써 완전하지만, 두 번째 부분집합이 더욱 의미있습니다. 두 번째 부분집합에는 많은 개수의 instance들이 있기 때문에 이 부분집합이 구성된 것이 우연이 아님을 알수 있습니다.그 반대를 생각해보는 것도 도움이 됩니다. 아래 그림에서 같은 값의 불순도를 가지고 있는 두 부분집합이 있습니다.&lt;img src=https://s3.amazonaws.com/codecademy-content/programs/data-science-path/decision-trees/impurity-5.svg width=300px&gt;이 두 부분집합의 불순도는 굉장히 높습니다. 그렇지만 어느 부분집합의 불순도가 더 큰 의미를 가질까요? 왼쪽의 부분집합을 분할하는 것보다는 오른쪽 부분집합을 분할하여 불순도가 없는 집합을 만드는 것이 정보증가량이 더 클 것입니다. 따라서, 집합의 instance 개수를 고려하여 정보증가량을 계산해야 합니다.집합의 크기까지 고려하도록 정보증가량 함수를 수정할 것 입니다. 단순히 불순도를 빼는 것에서 더 나아가 분할된 부분집합의 가중 불순도를 뺄 것입니다. 만약 분할 전의 데이터가 10개의 instance을 가지고 있고 하나의 부분집합이 2개의 instance가 있다면, 그 부분집합의 가중 불순도는 2/10 * impurity가 되어 instance 숫자가 적은 세트의 중요도를 낮춥니다.가중 정보증가량 계산의 예시는 아래와 같습니다.&lt;img src=https://s3.amazonaws.com/codecademy-content/programs/data-science-path/decision-trees/weighted_info.svg&gt;&#49892;&#49845; 3&#182;아래는 데이터셋의 각 feature와 class label에 대한 설명입니다. Car dataset은 class에 해당하는 4가지 label과 각 차량의 특징을 나타내는 6개의 feature을 갖고 있습니다.Label은 4개의 class, unacc(unacceptable), acc(acceptable), good, vgood로 이루어져 있으며, 각 class는 차량 구매시의 만족도(acceptability)를 나타냅니다.각 차량은 6개의 feature을 가지고 있고, 아래와 같습니다.buying (차량의 가격): \"vhigh\",\"high\",\"med\", or \"low\".maint (차량 유지 비용): \"vhigh\",\"high\",\"med\", or \"low\".doors (차의 문 갯수): \"2\",\"3\",\"4\",\"5more\".persons (차량의 최대 탑승 인원): \"2\",\"4\", or \"more\".lug_boot (차량 트렁크의 사이즈): \"small\",\"med\", or \"big\"safety (차량의 안전성 등급): \"low\",\"med\", or \"high\".In&nbsp;[&nbsp;]: # 샘플 데이터cars = [[&#39;med&#39;, &#39;low&#39;, &#39;3&#39;, &#39;4&#39;, &#39;med&#39;, &#39;med&#39;], [&#39;med&#39;, &#39;vhigh&#39;, &#39;4&#39;, &#39;more&#39;, &#39;small&#39;, &#39;high&#39;], [&#39;high&#39;, &#39;med&#39;, &#39;3&#39;, &#39;2&#39;, &#39;med&#39;, &#39;low&#39;], [&#39;med&#39;, &#39;low&#39;, &#39;4&#39;, &#39;4&#39;, &#39;med&#39;, &#39;low&#39;], [&#39;med&#39;, &#39;low&#39;, &#39;5more&#39;, &#39;2&#39;, &#39;big&#39;, &#39;med&#39;], [&#39;med&#39;, &#39;med&#39;, &#39;2&#39;, &#39;more&#39;, &#39;big&#39;, &#39;high&#39;], [&#39;med&#39;, &#39;med&#39;, &#39;2&#39;, &#39;more&#39;, &#39;med&#39;, &#39;med&#39;], [&#39;vhigh&#39;, &#39;vhigh&#39;, &#39;2&#39;, &#39;2&#39;, &#39;med&#39;, &#39;low&#39;], [&#39;high&#39;, &#39;med&#39;, &#39;4&#39;, &#39;2&#39;, &#39;big&#39;, &#39;low&#39;], [&#39;low&#39;, &#39;low&#39;, &#39;2&#39;, &#39;4&#39;, &#39;big&#39;, &#39;med&#39;]]car_labels = [&#39;acc&#39;, &#39;acc&#39;, &#39;unacc&#39;, &#39;unacc&#39;, &#39;unacc&#39;, &#39;vgood&#39;, &#39;acc&#39;, &#39;unacc&#39;, &#39;unacc&#39;, &#39;good&#39;] information_gain 함수를 수정하여 가중 정보증가량을 계산가중치: 부분집합의 label 갯수 `len(subset)` / 분할 전의 집합의 label 갯수 `len(starting_labels)`In&nbsp;[&nbsp;]: def weighted_information_gain(starting_labels, split_labels): info_gain = gini(starting_labels) for subset in split_labels: info_gain -= gini(subset) * (len(subset) / len(starting_labels)) return info_gain 아래 split() 함수를 살펴보겠습니다.In&nbsp;[&nbsp;]: def split(dataset, labels, column): data_subsets = [] label_subsets = [] # empty list counts = list(set([data[column] for data in dataset])) # list 의 중복 항목 제거를 위한 set 변환 for k in counts: # k=counts element [&#39;2&#39;, &#39;4&#39;, &#39;more&#39;] new_data_subset = [] new_label_subset = [] for i in range(len(dataset)): # data set len -&gt; all looping if dataset[i][column] == k: new_data_subset.append(dataset[i]) new_label_subset.append(labels[i]) data_subsets.append(new_data_subset) label_subsets.append(new_label_subset) return data_subsets, label_subsets In&nbsp;[&nbsp;]: # split 함수 호출split_data, split_labels = split(cars, car_labels, 3) In&nbsp;[&nbsp;]: split_data Out[&nbsp;]:[[[&#39;med&#39;, &#39;vhigh&#39;, &#39;4&#39;, &#39;more&#39;, &#39;small&#39;, &#39;high&#39;], [&#39;med&#39;, &#39;med&#39;, &#39;2&#39;, &#39;more&#39;, &#39;big&#39;, &#39;high&#39;], [&#39;med&#39;, &#39;med&#39;, &#39;2&#39;, &#39;more&#39;, &#39;med&#39;, &#39;med&#39;]], [[&#39;med&#39;, &#39;low&#39;, &#39;3&#39;, &#39;4&#39;, &#39;med&#39;, &#39;med&#39;], [&#39;med&#39;, &#39;low&#39;, &#39;4&#39;, &#39;4&#39;, &#39;med&#39;, &#39;low&#39;], [&#39;low&#39;, &#39;low&#39;, &#39;2&#39;, &#39;4&#39;, &#39;big&#39;, &#39;med&#39;]], [[&#39;high&#39;, &#39;med&#39;, &#39;3&#39;, &#39;2&#39;, &#39;med&#39;, &#39;low&#39;], [&#39;med&#39;, &#39;low&#39;, &#39;5more&#39;, &#39;2&#39;, &#39;big&#39;, &#39;med&#39;], [&#39;vhigh&#39;, &#39;vhigh&#39;, &#39;2&#39;, &#39;2&#39;, &#39;med&#39;, &#39;low&#39;], [&#39;high&#39;, &#39;med&#39;, &#39;4&#39;, &#39;2&#39;, &#39;big&#39;, &#39;low&#39;]]]In&nbsp;[&nbsp;]: len(split_data) Out[&nbsp;]:3split_labels를 사용, index 3에 대해 스플릿한 information gainIn&nbsp;[&nbsp;]: # index 3으로 데이터를 분할하였을 때 정보증가량을 출력weighted_information_gain(car_labels, split_labels) Out[&nbsp;]:0.30666666666666675정보증가량을 찾는 과정을 모든 feature에 대해서 적용In&nbsp;[&nbsp;]: # 데이터에 있는 모든 feature들에 대하여 `split()` 함수와 `information_gain()` 함수를 호출 # 4th feature(persons feature)가 가장 큰 영향을 미친다.for i in range(0,6): split_data, split_labels = split(cars, car_labels, i) print(weighted_information_gain(car_labels, split_labels)) 0.27333333333333340.0400000000000000360.106666666666666660.306666666666666750.150000000000000020.29000000000000004&#49892;&#49845; 4: &#51116;&#44480; &#53944;&#47532; &#47564;&#46308;&#44592; (Recursive Tree Building)&#182;데이터를 분할하였을 때 정보증가량이 가장 높은 feature을 찾을 수 있습니다. 이 방법을 반복하는 재귀 알고리즘을 통하여 트리를 구성할 수 있습니다. 데이터의 모든 instance에서 시작하여 데이터를 분할할 가장 좋은 feature을 찾고, 그 feature에 대해서 데이터를 나눈 후에 생성된 부분집합에 대해서 재귀적으로 위의 순서를 되풀이합니다.정보증가량이 일어나지 않는 feature을 찾을 때까지 재귀를 반복합니다. 다른 말로, 우리는 더이상 불순도가 없는 부분집합을 만드는 분할이 존재하지 않을 때 결정 트리의 끝마디를 생성합니다. 이 끝마디는 전체 데이터에서 분류된 instance의 class을 담고 있습니다.In&nbsp;[&nbsp;]: # 위의 함수들을 종합하여 가장 적합한 분할 feature을 찾는 함수 작성def find_best_split(dataset, labels): best_gain = 0 best_feature = 0 for feature in range(len(dataset[0])): data_subset, label_subset = split(dataset, labels, feature) gain = weighted_information_gain(labels, label_subset) if gain &gt; best_gain: best_gain, best_feature = gain, feature return best_gain, best_feature 위 함수를 cars와 car_labels에 대해 호출In&nbsp;[&nbsp;]: best_gain, best_feature = find_best_split(cars, car_labels) In&nbsp;[&nbsp;]: best_feature Out[&nbsp;]:3In&nbsp;[&nbsp;]: best_gain Out[&nbsp;]:0.30666666666666675data와 labels를 파라미터로 받는 build_tree()라는 함수를 선언이 함수는 재귀적으로 트리를 구성합니다.In&nbsp;[&nbsp;]: def build_tree(data, labels): best_gain, best_feature = find_best_split(data, labels) if best_gain == 0: return Counter(labels) data_subsets, label_subsets = split(data, labels, best_feature) branches = [] for i in range(len(data_subsets)): branch = build_tree(data_subsets[i], label_subsets[i]) branches.append(branch) return branches 만들어진 build_tree 함수 테스트In&nbsp;[&nbsp;]: def print_tree(node, spacing=&quot;&quot;): question_dict = {0: &quot;Buying Price&quot;, 1:&quot;Price of maintenance&quot;, 2:&quot;Number of doors&quot;, 3:&quot;Person Capacity&quot;, 4:&quot;Size of luggage boot&quot;, 5:&quot;Estimated Saftey&quot;} # Base case: 끝노드에 도달함 if isinstance(node, Counter): print (spacing + str(node)) return print (spacing + &quot;Splitting&quot;) # 분할 지점에서 각 브랜치에 대해 재귀적으로 print_tree 함수를 호출 for i in range(len(node)): print (spacing + &#39;--&gt; Branch &#39; + str(i)+&#39;:&#39;) print_tree(node[i], spacing + &quot; &quot;) In&nbsp;[&nbsp;]: # `build_tree` 함수와 `print_tree` 함수를 출력해봅니다.tree = build_tree(cars, car_labels)print_tree(tree) Splitting--&gt; Branch 0: Splitting --&gt; Branch 0: Counter({&#39;acc&#39;: 1}) --&gt; Branch 1: Counter({&#39;acc&#39;: 1}) --&gt; Branch 2: Counter({&#39;vgood&#39;: 1})--&gt; Branch 1: Splitting --&gt; Branch 0: Counter({&#39;unacc&#39;: 1}) --&gt; Branch 1: Counter({&#39;good&#39;: 1}) --&gt; Branch 2: Counter({&#39;acc&#39;: 1})--&gt; Branch 2: Counter({&#39;unacc&#39;: 4})&#49892;&#49845; 5: scikit-learn&#51004;&#47196; &#44396;&#54788;&#54616;&#45716; &#44208;&#51221;&#53944;&#47532;&#182;scikit-learn에서 제공되는 make_moons 데이터를 사용합니다.In&nbsp;[&nbsp;]: import matplotlib.pyplot as pltimport seaborn as snsfrom sklearn.datasets import make_moonsX, y = make_moons(noise=0.32, random_state=42, n_samples=250)sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y, marker=&quot;o&quot;, s=25, edgecolor=&quot;k&quot;, legend=False).set_title(&quot;Moon Data&quot;)plt.show() scikit-learn 패키지로 결정트리 구현DecisionTreeClassifier` 분류기를 사용합니다.[scikit-learn DecisionTreeClassifier documentation] (https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html?highlight=decisiontreeclassifier#sklearn.tree.DecisionTreeClassifier)In&nbsp;[&nbsp;]: from sklearn.tree import DecisionTreeClassifier In&nbsp;[&nbsp;]: dt = DecisionTreeClassifier() .fit() 메소드를 통해 데이터를 Tree에 훈련In&nbsp;[&nbsp;]: dt.fit(X,y) Out[&nbsp;]:DecisionTreeClassifier()정확도 확인In&nbsp;[&nbsp;]: dt.score(X,y) Out[&nbsp;]:1.0완성된 결정트리 시각화In&nbsp;[&nbsp;]: # classifier 결정트리를 시각화from sklearn.tree import export_graphviz # drawing graphs specified in DOT language scriptsfrom six import StringIOfrom IPython.display import Image import pydotplusdot_data = StringIO()export_graphviz(dt, out_file=dot_data)graph = pydotplus.graph_from_dot_data(dot_data.getvalue())graph.write_png(&#39;/tree.png&#39;)Image(graph.create_png()) Out[&nbsp;]:결정 경계 시각화In&nbsp;[&nbsp;]: from mglearn import plot_interactive_treeax = plot_interactive_tree.plot_tree_partition(X, y, dt)ax.set_title(&quot;first decision tree&quot;) Out[&nbsp;]:Text(0.5, 1.0, &#39;first decision tree&#39;) &#49892;&#49845; 6: &#44208;&#51221; &#53944;&#47532; &#44032;&#51648;&#52824;&#44592; (pruning)&#182;가지치기란 최대트리로 형성된 결정트리의 특정 노드 밑의 하부 트리를 제거하여 일반화 성능을 높히는 것을 의미합니다. 모든 끝노드의 불순도가 0인 트리를 full tree라고 하는데, 이 경우에는 분할이 너무 많이 과적합의 위험이 발생합니다. 과적합은 학습 데이터에 과하게 학습하여 실제 데이터에 오차가 증가하는 현상입니다. 이를 방지하기 위해서 적절한 수준에서 끝노드를 결합해주는 기법을 가지치기(pruning)이라고 합니다.scikit-learn DecisionTreeClassifier documentation새로운 결정트리를 생성 (깊이를 지정)In&nbsp;[&nbsp;]: pruned_dt = DecisionTreeClassifier(max_depth = 3)pruned_dt.fit(X, y)print(pruned_dt.score(X,y)) 0.888가지치기된 트리를 시각화이렇게 끝노드의 개수를 지정해주면 트리가 데이터에 더욱 잘 일반화됩니다.In&nbsp;[&nbsp;]: dot_data = StringIO()export_graphviz( pruned_dt, out_file=dot_data)graph = pydotplus.graph_from_dot_data(dot_data.getvalue())graph.write_png(&#39;/tree.png&#39;)Image(graph.create_png()) Out[&nbsp;]:결정 경계 시각화를 통한 비교In&nbsp;[&nbsp;]: ax = plot_interactive_tree.plot_tree_partition(X, y, pruned_dt)ax.set_title(&quot;first decision tree&quot;) Out[&nbsp;]:Text(0.5, 1.0, &#39;first decision tree&#39;) &#44284;&#51228;&#182;Kaggle의 Titanic 데이터를 사용타이타닉 데이터의 feature:Pclass: 승객 등급. 1등급=1, 2등급=2, 3등급=3Sex: 성별Age: 나이SibSp: 함께 탑승한 형제 또는 배우자 수Parch: 함께 탑승한 부모 또는 자녀 수Fare: 여객 운임Label: Survived 생존=1, 죽음=0데이터 파악 및 전처리In&nbsp;[&nbsp;]: import pandas as pddata_url = &quot;https://raw.githubusercontent.com/inikoreaackr/ml_datasets/main/titanic.csv&quot;data = pd.read_csv(data_url)data Out[&nbsp;]: PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked 0 1 0 3 Braund, Mr. Owen Harris male 22.0 1 0 A/5 21171 7.2500 NaN S 1 2 1 1 Cumings, Mrs. John Bradley (Florence Briggs Th... female 38.0 1 0 PC 17599 71.2833 C85 C 2 3 1 3 Heikkinen, Miss. Laina female 26.0 0 0 STON/O2. 3101282 7.9250 NaN S 3 4 1 1 Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35.0 1 0 113803 53.1000 C123 S 4 5 0 3 Allen, Mr. William Henry male 35.0 0 0 373450 8.0500 NaN S ... ... ... ... ... ... ... ... ... ... ... ... ... 886 887 0 2 Montvila, Rev. Juozas male 27.0 0 0 211536 13.0000 NaN S 887 888 1 1 Graham, Miss. Margaret Edith female 19.0 0 0 112053 30.0000 B42 S 888 889 0 3 Johnston, Miss. Catherine Helen \"Carrie\" female NaN 1 2 W./C. 6607 23.4500 NaN S 889 890 1 1 Behr, Mr. Karl Howell male 26.0 0 0 111369 30.0000 C148 C 890 891 0 3 Dooley, Mr. Patrick male 32.0 0 0 370376 7.7500 NaN Q 891 rows × 12 columns &lt;svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\" width=\"24px\"&gt; &lt;/svg&gt; In&nbsp;[&nbsp;]: data.columnsdata = data[[&#39;Survived&#39;, &#39;Pclass&#39;, &#39;Sex&#39;, &#39;Age&#39;, &#39;SibSp&#39;, &#39;Parch&#39;, &#39;Fare&#39;]]data.describe() Out[&nbsp;]: Survived Pclass Age SibSp Parch Fare count 891.000000 891.000000 714.000000 891.000000 891.000000 891.000000 mean 0.383838 2.308642 29.699118 0.523008 0.381594 32.204208 std 0.486592 0.836071 14.526497 1.102743 0.806057 49.693429 min 0.000000 1.000000 0.420000 0.000000 0.000000 0.000000 25% 0.000000 2.000000 20.125000 0.000000 0.000000 7.910400 50% 0.000000 3.000000 28.000000 0.000000 0.000000 14.454200 75% 1.000000 3.000000 38.000000 1.000000 0.000000 31.000000 max 1.000000 3.000000 80.000000 8.000000 6.000000 512.329200 &lt;svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\" width=\"24px\"&gt; &lt;/svg&gt; In&nbsp;[&nbsp;]: data = data.dropna() In&nbsp;[&nbsp;]: data.shape Out[&nbsp;]:(714, 7)In&nbsp;[&nbsp;]: data.dtypes Out[&nbsp;]:Survived int64Pclass int64Sex objectAge float64SibSp int64Parch int64Fare float64dtype: objectIn&nbsp;[&nbsp;]: from sklearn.preprocessing import LabelEncoderle = LabelEncoder()data.Sex = le.fit_transform(data.Sex) /usr/local/lib/python3.7/dist-packages/pandas/core/generic.py:5516: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.Try using .loc[row_indexer,col_indexer] = value insteadSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy self[name] = valueIn&nbsp;[&nbsp;]: data.dtypes Out[&nbsp;]:Survived int64Pclass int64Sex int64Age float64SibSp int64Parch int64Fare float64dtype: objectIn&nbsp;[&nbsp;]: y = data[&#39;Survived&#39;]X = data.drop(columns = [&#39;Survived&#39;]) 기계학습 모델을 훈련시키고 성능을 파악하기 위해서는 데이터를 훈련 데이터와 테스트 데이터로 나누어야 합니다.&lt;img src=https://upload.wikimedia.org/wikipedia/commons/b/bb/ML_dataset_training_validation_test_sets.png width=500px&gt;scikit-learn에서 지원하는 train_test_split 을 사용합니다. scikit-learn train_test_split documentationIn&nbsp;[&nbsp;]: from sklearn.model_selection import train_test_splitX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3) DecisionTreeClassifier 분류기를 사용해 결정트리를 생성In&nbsp;[&nbsp;]: dt = DecisionTreeClassifier() 트레이팅 데이터에 .fit() 메소드를 호출함으로써 트리를 데이터에 훈련, .fit()메소드는 training_points와 training_labels을 파라미터로 받음.In&nbsp;[&nbsp;]: dt.fit(X_train, y_train) Out[&nbsp;]:DecisionTreeClassifier()testing_points와 testing_labels에 대한 결정 트리의 정확도(.score())를 출력In&nbsp;[&nbsp;]: print(dt.score(X_test, y_test)) 0.8046511627906977훈련된 트리 시각화In&nbsp;[&nbsp;]: dot_data = StringIO()export_graphviz(dt, out_file=dot_data)graph = pydotplus.graph_from_dot_data(dot_data.getvalue())graph.write_png(&#39;/tree.png&#39;)Image(graph.create_png()) Out[&nbsp;]:scikit-learn DecisionTreeClassifier documentation과제 1max_leaf_nodes : 트리가 가질 수 있는 최대 leaf node의 수를 지정하는 parameter입니다. 해당 parameter가 지정되지 않은 경우 leaf node 수의 제한을 두지 않습니다.max_depth : 트리의 최대 깊이 parameter가 None일 때는 node가 모든 잎들이 pure해질 때까지 혹은, min_samples_split 이 지정하는 값 이하의 수를 포함하도록 확장됩니다.min_sample_split : node를 분할하기 위해 필요한 최소 샘플 수를 지정하는 parameter로 정수일 때는 최소 숫자로 간주되고 float일 때는 전체 샘플에 대한 min_sample_split의 비율만큼을 최소 샘플로 합니다.min_sample_leaf : leaf nodes에 필요한 최소 샘플의 수르 지정하는 parameter로 양쪽 가지에 필요한 최소 training sample을 의미합니다. 특히, regression에서는 모형을 smoothing하는데 효과가 있습니다.min_impurity_decrease : impurity가 감소하는 경우 node를 분할하도록 합니다.In&nbsp;[&nbsp;]: from sklearn.tree import DecisionTreeClassifierdf = DecisionTreeClassifier() In&nbsp;[&nbsp;]: dt_mxlf = DecisionTreeClassifier(max_leaf_nodes = 1000)dt_mxdth = DecisionTreeClassifier(max_depth = 6)dt_mss = DecisionTreeClassifier(min_samples_split = 8)dt_msl = DecisionTreeClassifier(min_samples_leaf = 32) dt_mid = DecisionTreeClassifier(min_impurity_decrease = 0.02) In&nbsp;[&nbsp;]: # prunig parameter를 조절하지 않은 Decision treedt.fit(X_train, y_train)dt.score(X_test, y_test) Out[&nbsp;]:0.8232558139534883In&nbsp;[&nbsp;]: # `max_leaf_nodes` parameter를 조절한 Decision treedt_mxlf.fit(X_train, y_train)dt_mxlf.score(X_test, y_test) Out[&nbsp;]:0.8186046511627907In&nbsp;[&nbsp;]: # `max_depth` parameter를 조절한 Decision treedt_mxdth.fit(X_train, y_train)dt_mxdth.score(X_test, y_test) Out[&nbsp;]:0.8372093023255814In&nbsp;[&nbsp;]: #`min_sample_split` parameter를 조절한 Decision treedt_mss.fit(X_train, y_train)dt_mss.score(X_test, y_test) Out[&nbsp;]:0.8In&nbsp;[&nbsp;]: #`min_sample_leaf` parameter를 조절한 Decision treedt_msl.fit(X_train, y_train)dt_msl.score(X_test, y_test) Out[&nbsp;]:0.827906976744186In&nbsp;[&nbsp;]: #`min_impurity_decrease` parameter를 조절한 Decision treedt_mid.fit(X_train, y_train)dt_mid.score(X_test, y_test) Out[&nbsp;]:0.8과제3prunig 파라미터를 조정하지 않은 Decision tree의 경우 분류 정확도가 [0.7488372093023256]로 pruning 파라미터를 조정한 Decision tree의 분류 정확도가 각각 [0.7813953488372093, 0.8, 0.7813953488372093, 0.813953488372093, 0.7813953488372093] 인 것과 비교해 볼 때 정확도가 떨어집니다.In&nbsp;[&nbsp;]: # `max_leaf_nodes` parameter를 조절한 Decision treedot_data = StringIO()export_graphviz(dt_mxlf, out_file=dot_data)graph = pydotplus.graph_from_dot_data(dot_data.getvalue())graph.write_png(&#39;/tree.png&#39;)Image(graph.create_png()) Out[&nbsp;]:In&nbsp;[&nbsp;]: # `max_depth` parameter를 조절한 Decision treedot_data = StringIO()export_graphviz(dt_mxdth, out_file=dot_data)graph = pydotplus.graph_from_dot_data(dot_data.getvalue())graph.write_png(&#39;/tree.png&#39;)Image(graph.create_png()) Out[&nbsp;]:In&nbsp;[&nbsp;]: #`min_sample_split` parameter를 조절한 Decision treedot_data = StringIO()export_graphviz(dt_mss, out_file=dot_data)graph = pydotplus.graph_from_dot_data(dot_data.getvalue())graph.write_png(&#39;/tree.png&#39;)Image(graph.create_png()) Out[&nbsp;]:In&nbsp;[&nbsp;]: #`min_sample_leaf` parameter를 조절한 Decision treedot_data = StringIO()export_graphviz(dt_msl, out_file=dot_data)graph = pydotplus.graph_from_dot_data(dot_data.getvalue())graph.write_png(&#39;/tree.png&#39;)Image(graph.create_png()) Out[&nbsp;]:In&nbsp;[&nbsp;]: #`min_impurity_decrease` parameter를 조절한 Decision treedot_data = StringIO()export_graphviz(dt_mid, out_file=dot_data)graph = pydotplus.graph_from_dot_data(dot_data.getvalue())graph.write_png(&#39;/tree.png&#39;)Image(graph.create_png()) Out[&nbsp;]:과제 5번max_leaf_nodes, max_depth, min_samples_splitparameter를 조정한 모델은 모두 트리 모델의 끝마디에서 Gini index는 높으나 node가 가지고 있는 sample의 수가 1 또는 2로 overfitting 되었을 확률이 높은 노드들을 다수 가지고 있으므로 적합하게 학습된 모델이라 할 수 없습니다. 한편 min_impurity_decrease parameter를 조절한 Decision tree 모델의 경우에는 tree 의 끝마디 noder가 가진 gini index가 다소 높게 측정되었기 때문에 적합되었다고 보기 어렵습니다.반면 'min_samples_leaf` parameter를 조절한 Decision tree는 하나의 끝마디에서 gini index가 다소 높게 측정되기는 했지만 각각의 끝마디 gini index가 낮게 나왔으며 각 끝마디의 샘플 수도 적정하기 때문에 다른 parameter를 조절한 Decision tree에 비교해 가장 적합하게, 가장 일반화를 잘 실행한 모델이라 볼 수 있을 것입니다."
    } ,
  
    {
      "title"       : "Starting the adventure",
      "category"    : "",
      "tags"        : "general blogging, thoughts, life",
      "url"         : "./starting-the-adventure.html",
      "date"        : "2021-03-24 00:00:00 +0900",
      "description" : "Midlife career change: a disaster or an opportunity?",
      "content"     : "In the midst of a global pandemic caused by the SARS-COV2 coronavirus; I decided to start blogging. I wanted to blog since a long time, I have always enjoyed writing, but many unknowns and having “no time” for it prevented me from taking it up. Things like: “I don’t really know who my target audience is”, “what would my topic or topics be?”, “I don’t think I am a world-class expert in anything”, and many more kept stopping me from setting up my own blog. Now seemed like a good time as any so with those and tons of other questions in my mind I decided it was time to start.Funnily, this is not my first post. The birth of the blog came very natural as a way to “document” my newly established pursuit for getting myself into Machine Learning. This new adventure of mine comprises several things, and if I want to succeed I need to be serious about them all: I want to start coding again! I used to code a long time ago, starting when I was 8 years old in a Tandy Color Computer hooked up to my parent’s TV. Machine Learning is a vast, wide subject, I want to learn the generals, but also to select a few areas to focus on. Setting up a blog to document my journey and share it: Establish a learning and blogging routine. If I don’t do this, I am sure this endeavour will die off soon.As for the focus areas I will start with: Neural Networks fundamentals: history, basic architecture and math behind them Deep Neural Networks Reinforcement Learning Current state of the art: what is at the cutting edge now in terms of Deep Neural Networks and Reinforcement Learning?I selected the above areas to focus on based on my personal interests, I have been fascinated by the developments in reinforcement learning for a long time, in particular Deep Mind’s awesome Go, Chess and Starcraft playing agents. Therefore, I started reading a lot about it and even started a personal project for coding a tic-tac-toe learning agent.With my limited knowledge I have drafted the following learning path: Youtube: Three Blue One Brown’s videos on Neural Networks, Calculus and Linear Algebra. I cannot recommend them enough, they are of sufficient depth and use animation superbly to facilitate the understanding of the subjects. Coursera: Andrew Ng’s Machine Learning course Book: Deep Learning with Python by Francois Chollet Book: Reinforcement Learning: An Introduction, by Richard S. Sutton and Andrew G. BartoAs for practical work I decided to start by coding my first models from scratch (without using libraries such as Tensorflow), to be able to deeply understand the math and logic behind the models, so far it has proven to be priceless.For my next project I think I will start to do the basic hand-written digits recognition, which is the Machine Learning Hello World, for this I think I will start to use Tensorflow already.I will continue to write about my learning road, what I find interesting and relevant, and to document all my practical exercises, as well as news and the state of the art in the world of AI.So far, all I have learned has been so engaging that I am seriously thinking of a career change. I have 17 years of international experience in multinational corporations across various functions, such as Information Services, Sales, Customer Care and New Products Introduction, and sincerely, I am finding more joy in artificial intelligence than anything else I have worked on before. Let’s see where the winds take us.Thanks for reading!P.S. For the geeks like me, here is a snippet on the technical side of the blog.Static Website GeneratorI researched a lot on this, when I started I didn’t even know I needed a static website generator. I was just sure of one thing, I wanted my blog site to look modern, be easy to update and not to have anything extra or additional content or functionality I did not need.There is a myriad of website generators nowadays, after a lengthy search the ones I ended up considering are: wordpress wix squarespace ghost webflow netlify hugo gatsby jekyllI started with the web interfaced generators with included hosting in their offerings:wordpress is the old standard, it is the one CMS I knew from before, and I thought I needed a fully fledged CMS, so I blindly ran towards it. Turns out, it has grown a lot since I remembered, it is now a fully fledged platform for complex websites and ecommerce development, even so I decided to give it a try, I picked a template and created a site. Even with the most simplistic and basic template I could find, there is a lot going on in the site. Setting it up was not as difficult or cumbersome as others claim, it took me about one hour to have it up and running, it looks good, but a bit crowded for my personal taste, and I found out it serves ads in your site for the readers, that is a big no for me.I have tried wix and squarespace before, they are fantastic for quick and easy website generation, but their free offering has ads, so again, a big no for me.I discovered ghost as the platform used by one of the bloggers I follow (Sebastian Ruder), turns out is a fantastic evolution over wordpress. It runs on the latest technologies, its interface is quite modern, and it is focused on one thing only: publishing. They have a paid hosting service, but the software is open sourced, therefore free to use in any hosting.I also tested webflow and even created a mockup there, the learning curve was quite smooth, and its CMS seems quite robust, but a bit too much for the functionalities I required.Next were the generators that don’t have a web interface, but can be easily set up:The first I tried was netlify, I also set up a test site in it. Netlify provides free hosting, and to keep your source files it uses GitHub (a repository keeps the source files where it publishes from). It has its own CMS, Netlify CMS, and you have a choice of site generators: Hugo, Gatsby, MiddleMan, Preact CLI, Next.js, Elevently and Nuxt.js, and once you choose there are some templates for each. I did not find the variety of templates enticing enough, and the set up process was much more cumbersome than with wordpress (at least for my knowledge level). I choose Hugo for my test site.I also tested gatsby with it’s own Gatsby Cloud hosting service, here is my test site. They also use GitHub as a base to host the source files to build the website, so you create a repository, and it is connected to it. I found the free template offerings quite limited for what I was looking for.Finally it came the turn for jekyll, although an older, and slower generator (compared to Hugo and Gatsby), it was created by one of the founders of GitHub, so it’s integration with GitHub Pages is quite natural and painless, so much so, that to use them together you don’t even have to install Jekyll in your machine! You have two choices: keep it all online, by having one repository in Github keep all the source files, modify or add them online, and having Jekyll build and publish your site to the special gh-pages repository everytime you change or add a new file to the source repository. Have a synchronized local copy of the source files for the website, this way you can edit your blog and customize it in your choice of IDE (Integrated Development Environment). Then, when you update any file on your computer, you just “push” the changes to GitHub, and GitHub Pages automatically uses Jekyll to build and publish your site.I chose the second option, specially because I can manipulate files, like images, in my laptop, and everytime I sync my local repository with GitHub, they are updated and published automatically. Quite convenient.After testing with several templates to get the feel for it, I decided to keep Jekyll for my blog for several reasons: the convenience of not having to install anything extra on my computer to build my blog, the integration with GitHub Pages, the ease of use, the future proofing via integration with modern technologies such as react or vue and the vast online community that has produced tons of templates and useful information for issue resolution, customization and added functionality.I picked up a template, just forked the repository and started modifying the files to customize it, it was fast and easy, I even took it upon myself to add some functionality to the template (it served as a coding little project) like: SEO meta tags Dark mode (configurable in _config.yml file) automatic sitemap.xml automatic archive page with infinite scrolling capability new page of posts filtered by a single tag (without needing autopages from paginator V2), also with infinite scrolling click to tweet functionality (just add a &lt;tweet&gt; &lt;/tweet&gt; tag in your markdown. custom and responsive 404 page responsive and automatic Table of Contents (optional per post) read time per post automatically calculated responsive post tags and social share icons (sticky or inline) included linkedin, reddit and bandcamp icons copy link to clipboard sharing option (and icon) view on github link button (optional per post) MathJax support (optional per post) tag cloud in the home page ‘back to top’ button comments ‘courtain’ to mask the disqus interface until the user clicks on it (configurable in _config.yml) CSS variables to make it easy to customize all colors and fonts added several pygments themes for code syntax highlight configurable from the _config.yml file. See the highlighter directory for reference on the options. responsive footer menu and footer logo (if setup in the config file) smoother menu animationsAs a summary, Hugo and Gatsby might be much faster than Jekyll to build the sites, but their complexity I think makes them useful for a big site with plenty of posts. For a small site like mine, Jekyll provides sufficient functionality and power without the hassle.You can use the modified template yourself by forking my repository. Let me know in the comments or feel free to contact me if you are interested in a detailed walkthrough on how to set it all up.HostingSince I decided on Jekyll to generate my site, the choice for hosting was quite obvious, Github Pages is very nicely integrated with it, it is free, and it has no ads! Plus the domain name isn’t too terrible (the-mvm.github.io).Interplanetary File SystemTo contribute to and test IPFS I also set up a mirror in IPFS by using fleek.co. I must confess that it was more troublesome than I imagined, it was definetively not plug and play because of the paths used to fetch resources. The nature of IPFS makes short absolute paths for website resources (like images, css and javascript files) inoperative; the easiest fix for this is to use relative paths, however the same relative path that works for the root directory (i.e. /index.html) does not work for links inside directories (i.e. /tags/), and since the site is static, while generating it, one must make the distinction between the different directory levels for the page to be rendered correctly.At first I tried a simple (but brute force solution):# determine the level of the current file{% assign lvl = page.url | append:'X' | split:'/' | size %}# create the relative base (i.e. \"../\"){% capture relativebase %}{% for i in (3..lvl) %}../{% endfor %}{% endcapture %}{% if relativebase == '' %} {% assign relativebase = './' %}{% endif %}...# Eliminate unecesary double backslashes{% capture post_url %}{{ relativebase }}{{ post.url }}{% endcapture %}{% assign post_url = post_url | replace: \"//\", \"/\" %}This jekyll/liquid code was executed in every page (or include) that needed to reference a resource hosted in the same server.But this fix did not work for the search function, because it relies on a search.json file (also generated programmatically to be served as a static file), therefore when generating this file one either use the relative path for the root directory or for a nested directory, thus the search results will only link correctly the corresponding pages if the page where the user searched for something is in the corresponding scope.So the final solution was to make the whole site flat, meaning to live in a single directory. All pages and posts will live under the root directory, and by doing so, I can control how to address the relative paths for resources."
    } 
  
]
